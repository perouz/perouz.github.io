<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NeurIPS 2025 - PaperAtlas</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background: #f4f6f9;
            color: #2c3e50;
            padding: 0;
            min-height: 100vh;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 0 20px;
        }

        header {
            background: linear-gradient(135deg, #1c3664 0%, #0a1f44 100%);
            padding: 40px 20px 50px;
            margin-bottom: 40px;
            text-align: center;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }

        h1 {
            color: #ffffff;
            font-size: 2.8em;
            margin-bottom: 15px;
            font-weight: 600;
            letter-spacing: -0.5px;
        }

        .subtitle {
            color: #b8c5d6;
            font-size: 1.15em;
            font-weight: 400;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }

        .stat-card {
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08);
            text-align: center;
            transition: all 0.3s ease;
            border: 1px solid #e8ecef;
        }

        .stat-card:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.12);
        }

        .stat-value {
            font-size: 2.8em;
            font-weight: 700;
            color: #1c3664;
            margin: 10px 0;
        }

        .stat-label {
            color: #5d6d7e;
            font-size: 0.85em;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            font-weight: 600;
        }

        .chart-section {
            background: white;
            padding: 35px;
            border-radius: 8px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08);
            margin-bottom: 30px;
            border: 1px solid #e8ecef;
        }

        .chart-section h2 {
            color: #1c3664;
            margin-bottom: 25px;
            font-size: 1.75em;
            font-weight: 600;
        }

        .chart-container {
            position: relative;
            height: 300px;
            margin-bottom: 20px;
        }

        .papers-section {
            background: white;
            padding: 35px;
            border-radius: 8px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.08);
            border: 1px solid #e8ecef;
        }

        .papers-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
            flex-wrap: wrap;
            gap: 15px;
        }

        .papers-header h2 {
            color: #667eea;
            font-size: 1.8em;
        }

        .sort-controls {
            display: flex;
            gap: 10px;
            align-items: center;
        }

        .sort-controls label {
            color: #666;
            font-weight: 500;
        }

        select {
            padding: 10px 15px;
            border: 2px solid #667eea;
            border-radius: 8px;
            background: white;
            color: #333;
            font-size: 1em;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        select:hover {
            background: #f8f9ff;
        }

        #paperSearch:focus {
            outline: none;
            border-color: #667eea;
            box-shadow: 0 0 0 3px rgba(102, 126, 234, 0.15);
        }

        #paperSearch::placeholder {
            color: #999;
        }

        .paper-card {
            background: #ffffff;
            padding: 24px;
            border-radius: 6px;
            margin-bottom: 16px;
            border-left: 4px solid #00c781;
            transition: all 0.2s ease;
            border: 1px solid #e8ecef;
            border-left: 4px solid #00c781;
        }

        .paper-card:hover {
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
            border-left-color: #1c3664;
        }

        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: start;
            margin-bottom: 10px;
            gap: 15px;
        }

        .paper-title {
            font-size: 1.2em;
            font-weight: 600;
            color: #333;
            flex: 1;
        }

        .paper-score {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 8px 16px;
            border-radius: 20px;
            font-weight: bold;
            font-size: 1.1em;
            min-width: 60px;
            text-align: center;
        }

        .paper-authors {
            color: #666;
            margin-bottom: 10px;
            font-size: 0.95em;
        }

        .paper-details {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
            margin-top: 10px;
            font-size: 0.9em;
        }

        .detail-item {
            display: flex;
            align-items: center;
            gap: 5px;
            color: #666;
        }

        .detail-item strong {
            color: #667eea;
        }

        .paper-stats {
            display: flex;
            gap: 15px;
            margin-top: 10px;
        }

        .stat-badge {
            background: white;
            padding: 5px 12px;
            border-radius: 15px;
            font-size: 0.85em;
            display: flex;
            align-items: center;
            gap: 5px;
        }

        .stat-badge strong {
            color: #667eea;
        }

        .paper-link {
            margin-top: 10px;
        }

        .paper-link a {
            color: #667eea;
            text-decoration: none;
            font-weight: 500;
            transition: color 0.3s ease;
        }

        .paper-link a:hover {
            color: #764ba2;
            text-decoration: underline;
        }

        .tabs {
            background: white;
            border-radius: 20px;
            padding: 10px;
            margin-bottom: 30px;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
            display: flex;
            gap: 10px;
        }

        .tab {
            flex: 1;
            padding: 16px 30px;
            background: transparent;
            border: none;
            border-bottom: 3px solid transparent;
            border-radius: 0;
            font-size: 1em;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.2s ease;
            color: #5d6d7e;
        }

        .tab:hover {
            color: #1c3664;
            background: #f8f9fb;
        }

        .tab.active {
            background: transparent;
            color: #1c3664;
            border-bottom: 3px solid #00c781;
        }

        .tab-content {
            display: none;
        }

        .tab-content.active {
            display: block;
        }

        .author-card {
            background: #ffffff;
            padding: 24px;
            border-radius: 6px;
            margin-bottom: 16px;
            border-left: 4px solid #00c781;
            transition: all 0.2s ease;
            border: 1px solid #e8ecef;
        }

        .author-card:hover {
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
            border-left-color: #1c3664;
        }

        .author-header {
            display: flex;
            flex-direction: row;
            gap: 15px;
            margin-bottom: 15px;
            align-items: flex-start;
        }

        .author-photo {
            width: 80px;
            height: 80px;
            border-radius: 50%;
            object-fit: cover;
            border: 3px solid #667eea;
            flex-shrink: 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .author-photo-placeholder {
            width: 80px;
            height: 80px;
            border-radius: 50%;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            flex-shrink: 0;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 2em;
            color: white;
            font-weight: 600;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .author-info {
            flex: 1;
            min-width: 0;
        }

        .author-name {
            font-size: 1.3em;
            font-weight: 600;
            color: #333;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .author-profile-link {
            display: inline-flex;
            align-items: center;
            gap: 5px;
            padding: 4px 10px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            text-decoration: none;
            border-radius: 8px;
            font-size: 0.7em;
            font-weight: 500;
            transition: all 0.2s ease;
        }

        .author-profile-link:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(102, 126, 234, 0.4);
        }

        .author-affiliation {
            display: flex;
            gap: 8px;
            flex-wrap: wrap;
            align-items: center;
        }

        .affiliation-badge {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.85em;
            font-weight: 500;
        }

        .role-badge {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.85em;
            font-weight: 500;
        }

        .author-stats-badges {
            display: flex;
            gap: 10px;
            flex-wrap: wrap;
        }

        .author-badge {
            background: white;
            padding: 6px 12px;
            border-radius: 15px;
            font-size: 0.85em;
            display: flex;
            align-items: center;
            gap: 5px;
            white-space: nowrap;
        }

        .pagination-btn {
            padding: 8px 16px;
            border: 1px solid #ddd;
            background: white;
            border-radius: 8px;
            cursor: pointer;
            font-size: 0.9em;
            transition: all 0.2s ease;
        }

        .pagination-btn:hover:not(:disabled) {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-color: transparent;
        }

        .pagination-btn:disabled {
            opacity: 0.5;
            cursor: not-allowed;
        }

        .pagination-btn.active {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-color: transparent;
            font-weight: 600;
        }

        .pagination-info {
            color: #666;
            font-size: 0.9em;
        }

        .category-pill {
            display: inline-block;
            padding: 6px 14px;
            margin: 4px;
            border-radius: 20px;
            font-size: 0.85em;
            cursor: pointer;
            transition: all 0.2s ease;
            border: 2px solid #ddd;
            background: white;
            color: #555;
        }

        .category-pill:hover {
            border-color: #667eea;
            background: #f0f4ff;
        }

        .category-pill.selected {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-color: transparent;
        }

        .paper-category-badge {
            display: inline-block;
            padding: 4px 10px;
            margin: 2px 4px 2px 0;
            border-radius: 12px;
            font-size: 0.75em;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            font-weight: 500;
        }

        .paper-card {
            cursor: pointer;
        }

        .paper-card.expanded {
            background: #f8f9fa;
        }

        .paper-expandable {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease;
        }

        .paper-card.expanded .paper-expandable {
            max-height: 500px;
            padding-top: 15px;
        }

        .paper-key-info {
            background: white;
            padding: 15px;
            border-radius: 8px;
            margin-bottom: 10px;
        }

        .paper-key-info h4 {
            margin: 0 0 8px 0;
            color: #667eea;
            font-size: 0.9em;
        }

        .paper-key-info p {
            margin: 0;
            color: #555;
            font-size: 0.9em;
            line-height: 1.5;
        }

        .view-details-btn {
            display: inline-block;
            padding: 8px 16px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-radius: 8px;
            border: none;
            cursor: pointer;
            font-size: 0.9em;
            margin-top: 10px;
            transition: transform 0.2s ease;
        }

        .view-details-btn:hover {
            transform: translateY(-2px);
        }

        .modal {
            display: none;
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(0, 0, 0, 0.5);
            z-index: 1000;
            align-items: center;
            justify-content: center;
        }

        .modal.active {
            display: flex;
        }

        .modal-content {
            background: white;
            border-radius: 15px;
            padding: 30px;
            max-width: 800px;
            max-height: 90vh;
            overflow-y: auto;
            position: relative;
            box-shadow: 0 10px 40px rgba(0,0,0,0.3);
        }

        .modal-close {
            position: absolute;
            top: 15px;
            right: 15px;
            font-size: 28px;
            cursor: pointer;
            color: #999;
            background: none;
            border: none;
            padding: 0;
            width: 32px;
            height: 32px;
            line-height: 28px;
            text-align: center;
        }

        .modal-close:hover {
            color: #333;
        }

        .modal-section {
            margin-bottom: 20px;
        }

        .modal-section h3 {
            color: #667eea;
            margin: 0 0 10px 0;
            font-size: 1.1em;
        }

        .modal-section p {
            color: #555;
            line-height: 1.6;
            margin: 0;
        }

        .author-badge strong {
            color: #764ba2;
        }

        .author-papers-list {
            margin-top: 15px;
            padding-top: 15px;
            border-top: 1px solid #ddd;
        }

        .author-paper-item {
            padding: 8px 0;
            display: flex;
            align-items: center;
            gap: 10px;
        }

        .author-paper-score {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 4px 10px;
            border-radius: 12px;
            font-weight: bold;
            font-size: 0.9em;
            min-width: 40px;
            text-align: center;
        }

        .author-paper-title {
            flex: 1;
            color: #333;
            font-size: 0.95em;
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 2em;
            }

            .stats-grid {
                grid-template-columns: 1fr;
            }

            .papers-header {
                flex-direction: column;
                align-items: stretch;
            }

            .sort-controls {
                flex-direction: column;
                width: 100%;
            }

            select {
                width: 100%;
            }

            .tabs {
                flex-direction: column;
            }

            .author-header {
                flex-direction: column;
                align-items: flex-start;
            }
        }

        /* Paper reference tooltips */
        .paper-ref {
            color: #00c781;
            font-weight: 600;
            cursor: help;
            position: relative;
            text-decoration: underline dotted;
            transition: all 0.2s ease;
        }

        .paper-ref:hover {
            color: #1c3664;
        }

        .paper-ref::after {
            content: attr(data-tooltip);
            position: absolute;
            bottom: 125%;
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0, 0, 0, 0.95);
            color: white;
            padding: 12px 16px;
            border-radius: 8px;
            white-space: normal;
            width: 320px;
            font-size: 0.85em;
            font-weight: normal;
            line-height: 1.5;
            box-shadow: 0 4px 12px rgba(0, 0, 0, 0.3);
            z-index: 1000;
            opacity: 0;
            visibility: hidden;
            transition: opacity 0.3s ease, visibility 0.3s ease;
            pointer-events: none;
            text-align: left;
        }

        .paper-ref:hover::after {
            opacity: 1;
            visibility: visible;
        }

        /* Tooltip arrow */
        .paper-ref::before {
            content: '';
            position: absolute;
            bottom: 115%;
            left: 50%;
            transform: translateX(-50%);
            border: 6px solid transparent;
            border-top-color: rgba(0, 0, 0, 0.95);
            opacity: 0;
            visibility: hidden;
            transition: opacity 0.3s ease, visibility 0.3s ease;
        }

        .paper-ref:hover::before {
            opacity: 1;
            visibility: visible;
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>NeurIPS 2025</h1>
            <p class="subtitle">Your personalized conference guide powered by <a href="https://github.com/aldro61/PaperAtlas" target="_blank" style="color:#60a5fa;">PaperAtlas</a></p>
        </header>

        <div class="tabs">
            <button class="tab active" onclick="switchTab('papers')">üìÑ Papers</button>
            <button class="tab" onclick="switchTab('authors')">üë• Authors</button>
            <button class="tab" onclick="switchTab('synthesis')">üî¨ Synthesis</button>
        </div>

        <div id="papersTab" class="tab-content active">
            <div class="stats-grid">
                <div class="stat-card">
                    <div class="stat-label">Total Papers</div>
                    <div class="stat-value" id="totalPapers">-</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Average Score</div>
                    <div class="stat-value" id="avgScore">-</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Top Score</div>
                    <div class="stat-value" id="topScore">-</div>
                </div>
            </div>

            <div class="chart-section">
                <h2>üìä Score Distribution</h2>
                <div class="chart-container">
                    <canvas id="scoreChart"></canvas>
                </div>
            </div>

            <div class="papers-section">
                <div class="papers-header">
                    <h2>üìÑ Your Papers</h2>
                    <div class="sort-controls">
                        <label for="sortBy">Sort by:</label>
                        <select id="sortBy">
                            <option value="score">Score (highest first)</option>
                            <option value="title">Title (A‚ÄìZ)</option>
                        </select>
                    </div>
                </div>

                <div class="search-container" style="margin-bottom: 20px;">
                    <div style="position: relative;">
                        <input type="text" id="paperSearch" placeholder="Search papers by title, author, or keywords..."
                               style="width: 100%; padding: 14px 16px 14px 45px; border: 2px solid #e8ecef; border-radius: 10px;
                                      font-size: 1em; background: white; transition: border-color 0.3s, box-shadow 0.3s;">
                        <span style="position: absolute; left: 16px; top: 50%; transform: translateY(-50%); font-size: 1.2em; opacity: 0.5;">üîç</span>
                        <button id="clearSearch" onclick="clearSearchBox()"
                                style="position: absolute; right: 12px; top: 50%; transform: translateY(-50%);
                                       background: none; border: none; font-size: 1.2em; cursor: pointer; opacity: 0.5; display: none;"
                                title="Clear search">‚úï</button>
                    </div>
                    <div id="searchResultsInfo" style="margin-top: 8px; font-size: 0.9em; color: #666; display: none;"></div>
                </div>

                <div id="categoryFilters" style="margin-bottom: 25px;"></div>

                <div id="papersList"></div>

                <div id="papersPagination" style="display: flex; justify-content: center; align-items: center; gap: 10px; margin-top: 30px;">
                </div>
            </div>
        </div>

        <div id="authorsTab" class="tab-content">
            <div class="papers-section">
                <div class="papers-header">
                    <h2>üë• Key Authors to Meet</h2>
                </div>
                <div style="background: #f8f9fa; padding: 15px; border-radius: 10px; margin-bottom: 20px; font-size: 0.95em; color: #555;">
                    <strong>Ranking by Research Alignment:</strong> Authors are ranked by their number of <strong>highly relevant papers (score ‚â• 85)</strong>.
                    Showing <strong>first, second, and last authors</strong> (primary contributors, key collaborators, and senior researchers) with at least <strong>1 highly relevant paper</strong> ‚Äî these are the must-meet researchers whose work is most aligned with your interests.
                    <span style="opacity: 0.8;">(Focusing on these key positions helps prioritize important contributors)</span>
                </div>

                <div style="background: white; padding: 25px; border-radius: 15px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); margin-bottom: 30px;">
                    <h3 style="margin-top: 0; margin-bottom: 20px; color: #333;">üèõÔ∏è Top Institutions</h3>
                    <canvas id="affiliationChart" style="max-height: 400px;"></canvas>
                </div>

                <div id="authorsList"></div>

                <div id="authorsPagination" style="display: flex; justify-content: center; align-items: center; gap: 10px; margin-top: 30px;">
                </div>
            </div>
        </div>

        <div id="synthesisTab" class="tab-content">
            <div class="papers-section">
                <div class="papers-header">
                    <h2>üî¨ Research Synthesis</h2>
                </div>
                <div style="background: #f8f9fa; padding: 15px; border-radius: 10px; margin-bottom: 20px; font-size: 0.95em; color: #555;">
                    <strong>Critical Analysis:</strong> A synthesized overview of major trends, surprising findings, and impactful work across all papers.
                </div>

                <div id="synthesisContent" style="background: white; padding: 30px; border-radius: 15px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); line-height: 1.8; max-width: 900px; margin: 0 auto;">
                    <div style="font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;"><h1 style="color: #1c3664; font-weight: 600; margin-top: 20px;">NeurIPS 2025: The Convergence of Reasoning, Grounding, and Reinforcement Learning in Multimodal AI</h1>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">Executive Summary</h2>

<p>NeurIPS 2025 marks a pivotal transition in multimodal AI research, characterized by five transformative developments:</p>

<p><strong>The Reasoning Revolution</strong>: Chain-of-thought reasoning has evolved from a prompting technique into a fundamental architectural principle, with reinforcement learning emerging as the dominant training paradigm. Over 40 papers demonstrate that RL-based approaches systematically outperform supervised fine-tuning for complex reasoning tasks, though critical limitations around hallucination, efficiency, and visual grounding remain unresolved.</p>

<p><strong>The Visual Grounding Crisis</strong>: A striking consensus emerges across 30+ papers: current vision-language models fundamentally lack genuine spatial understanding and visual reasoning capabilities. Models achieve high scores on existing benchmarks through textual shortcuts and semantic cues rather than true visual comprehension, exposing a critical gap between benchmark saturation and real-world competence.</p>

<p><strong>Reinforcement Learning's Ascendancy</strong>: GRPO (Group Relative Policy Optimization) and its variants have become the de facto standard for training reasoning models, appearing in 50+ papers. However, the field is fragmenting between those achieving gains through better exploration versus those simply scaling inference-time computation, with limited theoretical understanding of why RL succeeds.</p>

<p><strong>The Prompt Engineering Plateau</strong>: While prompt learning remains ubiquitous (35+ papers), diminishing returns are evident. The frontier has shifted from designing better prompts to learning when and how to bypass prompting entirely, with training-free methods increasingly competitive against heavily engineered approaches.</p>

<p><strong>Embodied AI's Integration Challenge</strong>: Vision-language-action models show promising results in controlled settings, but 15+ papers reveal fundamental failures in cross-task generalization, long-horizon planning, and real-world deployment, suggesting the field may be overfitting to benchmark environments.</p>

<p>---</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">I. The Reasoning Revolution: From Prompting to Policy Optimization</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">The Dominance of Reinforcement Learning</h3>

<p>The most striking trend at NeurIPS 2025 is the wholesale adoption of reinforcement learning for training reasoning capabilities in both language and vision-language models. This represents a fundamental paradigm shift from supervised fine-tuning (SFT) approaches that dominated previous years.</p>

<p><strong>Evidence of RL Superiority</strong>: The evidence for RL's advantages is overwhelming. <span class="paper-ref" data-paper-id="10" data-tooltip="Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning of Vision Language Models">[Paper 10]</span> demonstrates that Reason-RFT achieves state-of-the-art visual reasoning with 85-90% less data than SFT approaches, while <span class="paper-ref" data-paper-id="76" data-tooltip="SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement">[Paper 76]</span> shows ThinkLite-VL reaches 79.7% on MathVista using only 7.5k samples versus 59k-260k for distillation-based methods. <span class="paper-ref" data-paper-id="121" data-tooltip="AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning">[Paper 121]</span> reveals that RL can significantly outperform distillation for small/mid-sized models when using proper training recipes, achieving +14.6% on AIME 2025. <span class="paper-ref" data-paper-id="27" data-tooltip="Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective">[Paper 27]</span> provides systematic evidence that RL's effectiveness is domain-dependent: domains with strong pretraining coverage benefit from cross-domain RL, while underrepresented domains require in-domain training, suggesting RL facilitates genuine skill acquisition rather than just elicitation.</p>

<p><strong>The GRPO Ecosystem</strong>: Group Relative Policy Optimization has emerged as the standard RL algorithm, appearing in <span class="paper-ref" data-paper-id="4" data-tooltip="Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards">[Paper 4]</span>, <span class="paper-ref" data-paper-id="10" data-tooltip="Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning of Vision Language Models">[Paper 10]</span>, <span class="paper-ref" data-paper-id="11" data-tooltip="OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles">[Paper 11]</span>, <span class="paper-ref" data-paper-id="20" data-tooltip="NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation">[Paper 20]</span>, <span class="paper-ref" data-paper-id="25" data-tooltip="SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning">[Paper 25]</span>, <span class="paper-ref" data-paper-id="26" data-tooltip="Janus-Pro-R1: Advancing Collaborative Visual Comprehension and Generation via Reinforcement Learning">[Paper 26]</span>, <span class="paper-ref" data-paper-id="45" data-tooltip="VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning">[Paper 45]</span>, <span class="paper-ref" data-paper-id="63" data-tooltip="Sherlock: Self-Correcting Reasoning in Vision-Language Models">[Paper 63]</span>, <span class="paper-ref" data-paper-id="65" data-tooltip="Towards Unified Multimodal Interleaved Generation via Group Relative Policy Optimization">[Paper 65]</span>, <span class="paper-ref" data-paper-id="71" data-tooltip="Video-R1: Reinforcing Video Reasoning in MLLMs">[Paper 71]</span>, <span class="paper-ref" data-paper-id="72" data-tooltip="MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO">[Paper 72]</span>, <span class="paper-ref" data-paper-id="97" data-tooltip="Point-RFT: Improving Multimodal Reasoning with Visually Grounded Reinforcement Finetuning">[Paper 97]</span>, <span class="paper-ref" data-paper-id="100" data-tooltip="T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT">[Paper 100]</span>, <span class="paper-ref" data-paper-id="102" data-tooltip="Latent Chain-of-Thought for Visual Reasoning">[Paper 102]</span>, <span class="paper-ref" data-paper-id="110" data-tooltip="ChartSketcher: Reasoning with Multimodal Feedback and Reflection for Chart Understanding">[Paper 110]</span>, <span class="paper-ref" data-paper-id="114" data-tooltip="Fast-Slow Thinking GRPO for Large Vision-Language Model Reasoning">[Paper 114]</span>, <span class="paper-ref" data-paper-id="129" data-tooltip="MiCo: Multi-image Contrast for Reinforcement Visual Reasoning">[Paper 129]</span>, <span class="paper-ref" data-paper-id="135" data-tooltip="Multi-step Visual Reasoning with Visual Tokens Scaling and Verification">[Paper 135]</span>, <span class="paper-ref" data-paper-id="136" data-tooltip="Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering">[Paper 136]</span>, <span class="paper-ref" data-paper-id="144" data-tooltip="Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL">[Paper 144]</span>, <span class="paper-ref" data-paper-id="149" data-tooltip="Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs">[Paper 149]</span>, <span class="paper-ref" data-paper-id="151" data-tooltip="Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains">[Paper 151]</span>, <span class="paper-ref" data-paper-id="153" data-tooltip="NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation">[Paper 153]</span>, <span class="paper-ref" data-paper-id="156" data-tooltip="ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning">[Paper 156]</span>, <span class="paper-ref" data-paper-id="157" data-tooltip="VLM-R¬≥: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought">[Paper 157]</span>, <span class="paper-ref" data-paper-id="161" data-tooltip="Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning">[Paper 161]</span>, <span class="paper-ref" data-paper-id="163" data-tooltip="Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning">[Paper 163]</span>, <span class="paper-ref" data-paper-id="165" data-tooltip="Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models">[Paper 165]</span>, <span class="paper-ref" data-paper-id="166" data-tooltip="PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning">[Paper 166]</span>, and <span class="paper-ref" data-paper-id="168" data-tooltip="VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning">[Paper 168]</span>. However, critical variations are emerging. <span class="paper-ref" data-paper-id="20" data-tooltip="NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation">[Paper 20]</span> introduces NoisyGRPO with Bayesian advantage estimation to handle visual noise, <span class="paper-ref" data-paper-id="41" data-tooltip="GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents">[Paper 41]</span> reveals that standard GRPO suffers from vanishing advantages requiring Selective Sample Replay, and <span class="paper-ref" data-paper-id="144" data-tooltip="Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL">[Paper 144]</span> proposes gradient variance minimization for more efficient sampling.</p>

<p><strong>The Two-Stage Paradigm</strong>: A clear architectural pattern has emerged: SFT for cold-start followed by RL for refinement. <span class="paper-ref" data-paper-id="10" data-tooltip="Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning of Vision Language Models">[Paper 10]</span>, <span class="paper-ref" data-paper-id="11" data-tooltip="OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles">[Paper 11]</span>, <span class="paper-ref" data-paper-id="25" data-tooltip="SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning">[Paper 25]</span>, <span class="paper-ref" data-paper-id="42" data-tooltip="EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT">[Paper 42]</span>, <span class="paper-ref" data-paper-id="63" data-tooltip="Sherlock: Self-Correcting Reasoning in Vision-Language Models">[Paper 63]</span>, <span class="paper-ref" data-paper-id="73" data-tooltip="MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning">[Paper 73]</span>, <span class="paper-ref" data-paper-id="97" data-tooltip="Point-RFT: Improving Multimodal Reasoning with Visually Grounded Reinforcement Finetuning">[Paper 97]</span>, <span class="paper-ref" data-paper-id="110" data-tooltip="ChartSketcher: Reasoning with Multimodal Feedback and Reflection for Chart Understanding">[Paper 110]</span>, <span class="paper-ref" data-paper-id="128" data-tooltip="GRIT: Teaching MLLMs to Think with Images">[Paper 128]</span>, <span class="paper-ref" data-paper-id="135" data-tooltip="Multi-step Visual Reasoning with Visual Tokens Scaling and Verification">[Paper 135]</span>, <span class="paper-ref" data-paper-id="156" data-tooltip="ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning">[Paper 156]</span>, <span class="paper-ref" data-paper-id="157" data-tooltip="VLM-R¬≥: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought">[Paper 157]</span>, <span class="paper-ref" data-paper-id="161" data-tooltip="Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning">[Paper 161]</span>, <span class="paper-ref" data-paper-id="163" data-tooltip="Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning">[Paper 163]</span>, and <span class="paper-ref" data-paper-id="168" data-tooltip="VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning">[Paper 168]</span> all adopt this strategy. <span class="paper-ref" data-paper-id="11" data-tooltip="OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles">[Paper 11]</span> provides crucial insight: SFT surfaces latent reasoning actions (triggered by tokens like 'wait', 'check') that narrow the RL search space, while RL refines these behaviors and generates better data for subsequent iterations. This synergistic relationship explains why neither approach alone achieves optimal results.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">The Limits of Reasoning: When Thinking Hurts</h3>

<p>Despite the enthusiasm for reasoning, multiple papers reveal critical failure modes that challenge the "more reasoning is always better" assumption.</p>

<p><strong>Reasoning Collapse in Dynamic Environments</strong>: <span class="paper-ref" data-paper-id="8" data-tooltip="Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation">[Paper 8]</span> demonstrates that test-time reasoning fails catastrophically in vision-language navigation due to error accumulation and distribution shift, with explicit chain-of-thought degrading performance. The paper introduces Test-time Reasoning Collapse (TRC), showing that dynamic, partially observable environments fundamentally differ from static reasoning tasks. Similarly, <span class="paper-ref" data-paper-id="6" data-tooltip="When Thinking Drifts: Evidential Grounding for Robust Video Reasoning">[Paper 6]</span> identifies "visual thinking drift" where CoT reasoning degrades video understanding by generating verbose but misleading reasoning that diverges from actual visual evidence.</p>

<p><strong>The Overthinking Problem</strong>: <span class="paper-ref" data-paper-id="16" data-tooltip="Evaluating the Inductive Abilities of Large Language Models: Why Chain-of-Thought Reasoning Sometimes Hurts More Than Helps">[Paper 16]</span> provides theoretical and empirical evidence that chain-of-thought reasoning can degrade inductive performance by 20-40% on hidden rule inference tasks, identifying three failure modes: incorrect sub-task decomposition, incorrect solving, and incorrect summarization. <span class="paper-ref" data-paper-id="74" data-tooltip="More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models">[Paper 74]</span> reveals that reasoning quality matters more than length for visual tasks, contrary to findings in pure language models. <span class="paper-ref" data-paper-id="119" data-tooltip="CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning">[Paper 119]</span> exposes "reasoning collapse" in Large Reasoning Models where plausible incorrect hints trigger pathological overthinking, consuming 2-3√ó more tokens and causing catastrophic failures.</p>

<p><strong>Instruction-Following Degradation</strong>: <span class="paper-ref" data-paper-id="172" data-tooltip="When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs">[Paper 172]</span> uncovers a surprising phenomenon where explicit CoT reasoning significantly degrades instruction-following accuracy across 20+ models, with models neglecting simple constraints or introducing unnecessary content that violates instructions. This suggests a fundamental tension between reasoning depth and constraint adherence.</p>

<p><strong>The Efficiency-Performance Trade-off</strong>: <span class="paper-ref" data-paper-id="165" data-tooltip="Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models">[Paper 165]</span> demonstrates that models can learn to selectively bypass reasoning for simple questions while engaging detailed reasoning for complex ones, reducing token usage by up to 90% without sacrificing performance. <span class="paper-ref" data-paper-id="82" data-tooltip="VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning">[Paper 82]</span> shows most VQA tasks can be solved with 1/4 resolution images, with only OCR-related tasks requiring high resolution. These findings challenge the assumption that more computation always helps.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">Structured Reasoning Emerges</h3>

<p>Beyond simple chain-of-thought, structured reasoning approaches are gaining traction. <span class="paper-ref" data-paper-id="4" data-tooltip="Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards">[Paper 4]</span> introduces fine-grained step-level reasoning with explicit step decomposition and Process Reward Models, revealing that quality matters more than length for visual reasoning. <span class="paper-ref" data-paper-id="93" data-tooltip="Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought">[Paper 93]</span> unifies Textual-MCoT and Interleaved-MCoT under the "visual thoughts" framework, showing they function as visual information caches enabling deeper reasoning. <span class="paper-ref" data-paper-id="100" data-tooltip="T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT">[Paper 100]</span> demonstrates bi-level CoT reasoning for text-to-image generation, integrating semantic-level and token-level reasoning through BiCoT-GRPO.</p>

<p>---</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">II. The Visual Grounding Crisis: Models That See But Don't Observe</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">Fundamental Failures in Spatial Understanding</h3>

<p>Perhaps the most sobering finding across NeurIPS 2025 is the systematic evidence that vision-language models lack genuine spatial reasoning and visual grounding capabilities, despite high benchmark scores.</p>

<p><strong>The Shortcut Problem</strong>: <span class="paper-ref" data-paper-id="1" data-tooltip="What‚Äôs in Common? Multimodal Models Hallucinate When Reasoning Across Scenes">[Paper 1]</span> reveals that state-of-the-art models achieve only 35% accuracy on cross-scene reasoning and <1% on complex variants, despite saturating existing perception benchmarks at 80-90%. Models hallucinate objects 53% of the time when reasoning across scenes, with hallucinations increasing when similar objects are present. <span class="paper-ref" data-paper-id="9" data-tooltip="Caption This, Reason That: VLMs Caught in the Middle">[Paper 9]</span> demonstrates that VLMs can accurately caption spatial information but fail to utilize it during direct visual reasoning, identifying a "know but cannot tell" phenomenon. <span class="paper-ref" data-paper-id="15" data-tooltip="VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs">[Paper 15]</span> shows leading VLMs fail at basic nonlocal visual reasoning tasks that are trivial for humans, barely exceeding random accuracy, revealing they lack core visual algorithms despite high performance on complex benchmarks.</p>

<p><strong>Benchmark Saturation Without Understanding</strong>: <span class="paper-ref" data-paper-id="18" data-tooltip="ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models">[Paper 18]</span> demonstrates that 74% of ChartQA questions are solvable through text extraction alone, revealing existing benchmarks over-represent textual reasoning. <span class="paper-ref" data-paper-id="64" data-tooltip="Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model">[Paper 64]</span> introduces ComPABench showing that while RL-trained VLMs outperform SFT models in compositional generalization, they still struggle significantly with cross-modal and cross-task reasoning, achieving only 52.8% accuracy on compositional tasks. <span class="paper-ref" data-paper-id="95" data-tooltip="RBench-V: A Primary Assessment for Visual Reasoning Models with Multimodal Outputs">[Paper 95]</span> reveals even state-of-the-art reasoning models achieve only sub-60% accuracy on physics problems requiring visual understanding, with 75% of problems being vision-essential.</p>

<p><strong>The Binding Problem</strong>: <span class="paper-ref" data-paper-id="67" data-tooltip="Visual Structures Help Visual Reasoning:  Addressing the Binding Problem in LVLMs">[Paper 67]</span> demonstrates that VLMs exhibit "tunnel vision," selectively engaging visual processing and struggling with continuous operations that cannot be easily decomposed into natural language steps. <span class="paper-ref" data-paper-id="105" data-tooltip="Object-centric binding in Contrastive Language-Image Pretraining">[Paper 105]</span> shows CLIP's compositional failures stem from representational limitations rather than data insufficiency, with models exhibiting bag-of-words behavior. <span class="paper-ref" data-paper-id="127" data-tooltip="BLINK-Twice: You see, but do you observe?  A Reasoning Benchmark on Visual Perception">[Paper 127]</span> reveals that current MLLMs struggle with vision-centric reasoning, with even GPT-4o and Gemini-2.5 Pro achieving suboptimal performance on metrics requiring genuine visual understanding.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">New Benchmarks Expose Hidden Weaknesses</h3>

<p>Multiple papers introduce benchmarks specifically designed to isolate visual reasoning from textual shortcuts, revealing systematic failures.</p>

<p><strong>Spatial Reasoning Benchmarks</strong>: <span class="paper-ref" data-paper-id="14" data-tooltip="Surprise3D: A Dataset for Spatial Understanding and Reasoning in Complex 3D Scenes">[Paper 14]</span> introduces SURPRISE3D with 200K+ query-mask pairs that deliberately exclude object names from queries, preventing semantic shortcuts. State-of-the-art 3D vision-language models perform significantly worse when deprived of explicit object references. <span class="paper-ref" data-paper-id="35" data-tooltip="Spatial Understanding from Videos: Structured Prompts Meet Simulation Data">[Paper 35]</span> demonstrates that VLMs better interpret textual spatial descriptions than structured formats like 3D maps or grids, achieving up to 8.5% improvement through structured prompting. <span class="paper-ref" data-paper-id="86" data-tooltip="SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning">[Paper 86]</span> introduces SpatialReasoner showing that most errors in 3D spatial reasoning stem from failures in 3D perception rather than computation.</p>

<p><strong>Cross-Scene and Multi-Image Reasoning</strong>: <span class="paper-ref" data-paper-id="148" data-tooltip="Robust Cross-modal Alignment Learning for Cross-Scene Spatial Reasoning and Grounding">[Paper 148]</span> introduces Cross-Scene Spatial Reasoning and Grounding (CSSRG), revealing that existing 3DVG methods struggle when scene-text correspondence is not predetermined, with 250√ó computational overhead. <span class="paper-ref" data-paper-id="129" data-tooltip="MiCo: Multi-image Contrast for Reinforcement Visual Reasoning">[Paper 129]</span> demonstrates that models show performance improvements even with non-essential diagrams and exhibit distinct failure patterns in visual misinterpretation, indicating weak visual reasoning capabilities.</p>

<p><strong>Video and Temporal Understanding</strong>: <span class="paper-ref" data-paper-id="170" data-tooltip="Two Causally Related Needles in a Video Haystack">[Paper 170]</span> introduces CAUSAL2NEEDLES showing that current VLMs struggle significantly with causal two-needle questions, with ChatGPT-4o achieving only 11.4% accuracy on questions requiring joint understanding of causally related events. <span class="paper-ref" data-paper-id="146" data-tooltip="OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding">[Paper 146]</span> reveals that performance degrades substantially as exploration horizons extend and memory grows, particularly for tasks requiring multi-step spatial connections.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">Visual Grounding Solutions</h3>

<p>Despite these challenges, several papers propose promising solutions for improving visual grounding.</p>

<p><strong>Explicit Visual Reasoning</strong>: <span class="paper-ref" data-paper-id="97" data-tooltip="Point-RFT: Improving Multimodal Reasoning with Visually Grounded Reinforcement Finetuning">[Paper 97]</span> introduces Point-RFT demonstrating that reinforcement learning with visually grounded CoT (where reasoning steps are explicitly linked to visual points) achieves 90.04% accuracy on ChartQA versus 70.88% baseline. <span class="paper-ref" data-paper-id="98" data-tooltip="Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing">[Paper 98]</span> enables models to perform spatial reasoning through iterative visual drawing operations, achieving 98.2% accuracy on maze navigation. <span class="paper-ref" data-paper-id="128" data-tooltip="GRIT: Teaching MLLMs to Think with Images">[Paper 128]</span> introduces GRIT showing that grounded reasoning chains interleaving text and bounding boxes can be learned via GRPO with only 20 training samples.</p>

<p><strong>Structured Visual Representations</strong>: <span class="paper-ref" data-paper-id="67" data-tooltip="Visual Structures Help Visual Reasoning:  Addressing the Binding Problem in LVLMs">[Paper 67]</span> demonstrates that adding simple visual structures (horizontal lines) combined with sequential scanning prompts substantially improves LVLM performance on visual reasoning tasks, with 25.0% improvement on visual search. <span class="paper-ref" data-paper-id="99" data-tooltip="SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning">[Paper 99]</span> introduces SSR showing that converting depth data into structured textual rationales achieves up to 22.5% improvement on spatial tasks. <span class="paper-ref" data-paper-id="183" data-tooltip="Struct2D: A Perception-Guided Framework for Spatial Reasoning in MLLMs">[Paper 183]</span> demonstrates that MLLMs can perform robust 3D spatial reasoning using only structured 2D inputs (BEV images with object marks) without requiring explicit 3D representations.</p>

<p>---</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">III. Reinforcement Learning: Mechanisms, Limitations, and Frontiers</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">Understanding Why RL Works</h3>

<p>While RL's empirical success is clear, several papers provide crucial insights into the mechanisms underlying its effectiveness.</p>

<p><strong>Exploration vs. Elicitation</strong>: <span class="paper-ref" data-paper-id="27" data-tooltip="Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective">[Paper 27]</span> provides systematic evidence that RL's role is domain-dependent: it primarily elicits existing knowledge in well-represented domains (Math, Code, Science) but facilitates genuine skill acquisition in underrepresented domains (Logic, Simulation, Tabular). <span class="paper-ref" data-paper-id="121" data-tooltip="AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning">[Paper 121]</span> reveals that RL both elicits foundational reasoning capabilities and pushes models to solve previously unsolvable problems, with math-only RL improving code reasoning performance (+6.8%/+5.8% on LiveCodeBench).</p>

<p><strong>Reward Design Matters</strong>: <span class="paper-ref" data-paper-id="4" data-tooltip="Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards">[Paper 4]</span> demonstrates that fine-grained process rewards significantly outperform outcome-only rewards for visual reasoning. <span class="paper-ref" data-paper-id="6" data-tooltip="When Thinking Drifts: Evidential Grounding for Robust Video Reasoning">[Paper 6]</span> introduces Visual Evidence Reward showing that explicitly rewarding reasoning grounded in visual evidence achieves +4.0% average improvement. <span class="paper-ref" data-paper-id="19" data-tooltip="Reasoning as an Adaptive Defense for Safety">[Paper 19]</span> reveals that both RL and reasoning are essential for safety, with RL without reasoning or reasoning via SFT alone achieving inferior safety-refusal trade-offs.</p>

<p><strong>The Curriculum Effect</strong>: <span class="paper-ref" data-paper-id="114" data-tooltip="Fast-Slow Thinking GRPO for Large Vision-Language Model Reasoning">[Paper 114]</span> introduces difficulty-aware length rewards and dynamic KL regularization, achieving state-of-the-art accuracy with 32.7-67.3% token reduction. <span class="paper-ref" data-paper-id="30" data-tooltip="VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents">[Paper 30]</span> demonstrates that Bi-Level GAE enables turn-aware credit assignment in multi-turn reinforcement learning, achieving 3√ó performance improvement. <span class="paper-ref" data-paper-id="139" data-tooltip="Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics">[Paper 139]</span> shows that reformulating continuous next-state prediction as multiple-choice QA enables efficient RL training for embodied reasoning.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">Critical Limitations and Failure Modes</h3>

<p>Despite successes, multiple papers identify fundamental limitations in current RL approaches.</p>

<p><strong>Reward Hacking and Degenerate Solutions</strong>: <span class="paper-ref" data-paper-id="41" data-tooltip="GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents">[Paper 41]</span> identifies opposing reward hacking behaviors in GUI grounding (smaller vs. larger boxes) and training biases (length and difficulty) specific to visual tasks. <span class="paper-ref" data-paper-id="19" data-tooltip="Reasoning as an Adaptive Defense for Safety">[Paper 19]</span> shows that na√Øve RL on safety leads to reward hacking and degenerate refusal strategies, requiring strategic prompt mixing and separate reward functions.</p>

<p><strong>Vanishing Advantages and Training Instability</strong>: <span class="paper-ref" data-paper-id="45" data-tooltip="VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning">[Paper 45]</span> introduces Selective Sample Replay to mitigate vanishing advantages in GRPO by maintaining a replay buffer of high-value experiences. <span class="paper-ref" data-paper-id="144" data-tooltip="Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL">[Paper 144]</span> proposes gradient variance minimization showing that static, uniform sampling strategies create inefficiency, with dynamic allocation achieving 2-4√ó speedup.</p>

<p><strong>The Hallucination Amplification Problem</strong>: <span class="paper-ref" data-paper-id="74" data-tooltip="More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models">[Paper 74]</span> reveals that longer reasoning chains systematically reduce visual attention allocation, causing models to drift toward language priors. Attention analysis shows that extended reasoning increases hallucination rates compared to non-reasoning counterparts. <span class="paper-ref" data-paper-id="137" data-tooltip="MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM">[Paper 137]</span> demonstrates that model scale, data scale, and training stages significantly affect logical, fabrication, and factual hallucinations, but show limited improvement on spatial hallucinations.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">Novel RL Approaches</h3>

<p>Several papers introduce innovative RL methods addressing specific challenges.</p>

<p><strong>Latent Reasoning</strong>: <span class="paper-ref" data-paper-id="151" data-tooltip="Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains">[Paper 151]</span> introduces CoLaR enabling dynamic-speed latent reasoning through auto-regressive prediction of compressed embeddings, achieving 14.1% higher accuracy than latent-based baselines while reducing reasoning chain length by 53.3%. <span class="paper-ref" data-paper-id="102" data-tooltip="Latent Chain-of-Thought for Visual Reasoning">[Paper 102]</span> introduces LaCoT treating visual reasoning as latent variable inference, enabling diverse chain-of-thought sampling through GFlowNets-based amortized variational inference.</p>

<p><strong>Multi-Stage and Hierarchical RL</strong>: <span class="paper-ref" data-paper-id="156" data-tooltip="ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning">[Paper 156]</span> introduces ThinkAct using action-aligned visual rewards (goal completion and trajectory alignment) to incentivize embodied reasoning, achieving 84.4% on LIBERO. <span class="paper-ref" data-paper-id="167" data-tooltip="PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments">[Paper 167]</span> introduces PhysVLM-AVR formalizing Active Visual Reasoning as a task requiring sequential actions for information gathering in partially observable environments.</p>

<p><strong>Cross-Modal RL</strong>: <span class="paper-ref" data-paper-id="72" data-tooltip="MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO">[Paper 72]</span> introduces RGPO (Reasoning Generation Policy Optimization) extending RL to multimodal generation by incorporating dual KL divergence regularizers for text and visual rollouts. <span class="paper-ref" data-paper-id="168" data-tooltip="VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning">[Paper 168]</span> introduces VideoRFT with semantic-consistency reward that selectively targets video-describing portions of reasoning traces for visual alignment.</p>

<p>---</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">IV. The Prompt Engineering Plateau and Beyond</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">Diminishing Returns from Prompt Design</h3>

<p>While prompt learning remains widespread, evidence of diminishing returns is mounting across multiple fronts.</p>

<p><strong>Training-Free Methods Competitive</strong>: <span class="paper-ref" data-paper-id="32" data-tooltip="DualCnst: Enhancing Zero-Shot Out-of-Distribution Detection via Text-Image Consistency in Vision-Language Models">[Paper 32]</span> introduces DualCnst achieving state-of-the-art zero-shot OOD detection without training by combining text-based semantic similarity with visual similarity to synthetic images. <span class="paper-ref" data-paper-id="54" data-tooltip="Towards Self-Refinement of Vision-Language Models with Triangular Consistency">[Paper 54]</span> demonstrates that VLMs possess inherent self-refinement capabilities through Triangular Consistency, achieving improvements without external supervision. <span class="paper-ref" data-paper-id="68" data-tooltip="CoFFT: Chain of Foresight-Focus Thought for Visual Language Models">[Paper 68]</span> introduces CoFFT achieving 3.1-5.8% improvement across visual reasoning benchmarks without requiring model retraining.</p>

<p><strong>Prompt Optimization Challenges</strong>: <span class="paper-ref" data-paper-id="33" data-tooltip="Understanding Prompt Tuning and In-Context Learning via Meta-Learning">[Paper 33]</span> provides theoretical analysis showing that optimal prompting is only possible when the target task has support in the pretraining distribution and is a single task, not a mixture. <span class="paper-ref" data-paper-id="75" data-tooltip="How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation">[Paper 75]</span> demonstrates that dynamic prompt-specific budget allocation based on acceptance rates achieves 2-4√ó speedup over uniform strategies. <span class="paper-ref" data-paper-id="108" data-tooltip="Towards Reliable and Holistic Visual In-Context Learning Prompt Selection">[Paper 108]</span> shows that the similarity-priority assumption in Visual In-Context Learning is statistically weak and unreliable.</p>

<p><strong>When Prompting Fails</strong>: <span class="paper-ref" data-paper-id="55" data-tooltip="Towards Visualization-of-Thought Jailbreak Attack against Large Visual Language Models">[Paper 55]</span> introduces VoTA demonstrating that VLMs struggle to maintain safety guarantees when processing complex multimodal inputs requiring intricate reasoning, achieving 26.71% improvement in attack success rate. <span class="paper-ref" data-paper-id="172" data-tooltip="When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs">[Paper 172]</span> reveals that explicit CoT reasoning can significantly degrade instruction-following accuracy, with models neglecting simple constraints.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">Adaptive and Learned Prompting</h3>

<p>Several papers move beyond static prompting toward adaptive, learned approaches.</p>

<p><strong>Meta-Learning for Prompts</strong>: <span class="paper-ref" data-paper-id="7" data-tooltip="AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking">[Paper 7]</span> introduces AdaReasoner using factorized RL policy with Boltzmann exploration to jointly optimize reasoning configurations (instruction format, temperature, reasoning steps) on a per-question basis. <span class="paper-ref" data-paper-id="69" data-tooltip="VaMP: Variational Multi-Modal Prompt Learning for Vision-Language Models">[Paper 69]</span> introduces VaMP using variational inference to generate instance-conditioned prompts from learned posterior distributions, improving generalization to novel classes by 1.51%.</p>

<p><strong>Selective and Conditional Prompting</strong>: <span class="paper-ref" data-paper-id="165" data-tooltip="Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models">[Paper 165]</span> demonstrates that models must be explicitly trained to skip reasoning through format-following, as prompting alone is insufficient. <span class="paper-ref" data-paper-id="82" data-tooltip="VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning">[Paper 82]</span> introduces VisionThink showing that most VQA tasks can be solved with 1/4 resolution images, with models learning to autonomously decide resolution requirements.</p>

<p><strong>Prompt Efficiency</strong>: <span class="paper-ref" data-paper-id="43" data-tooltip="All You Need is One: Capsule Prompt Tuning with a Single Vector">[Paper 43]</span> introduces CaPT using only one single learnable vector per layer, achieving state-of-the-art results while using only 0.003-0.004% of model parameters. <span class="paper-ref" data-paper-id="111" data-tooltip="REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning">[Paper 111]</span> introduces REP achieving up to 51% reduction in training time and 41% reduction in memory usage for prompt-based continual learning.</p>

<p>---</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">V. Embodied AI: The Generalization Challenge</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">Promising Results in Controlled Settings</h3>

<p>Vision-language-action models show impressive performance in benchmark environments, but critical limitations emerge when evaluating generalization.</p>

<p><strong>Strong Benchmark Performance</strong>: <span class="paper-ref" data-paper-id="91" data-tooltip="CogVLA: Cognition-Aligned Vision-Language-Action Models via Instruction-Driven Routing & Sparsification">[Paper 91]</span> introduces CogVLA achieving 97.4% success rate on LIBERO and 70.0% on real-world tasks while reducing training costs by 2.5√ó and inference latency by 2.8√ó. <span class="paper-ref" data-paper-id="156" data-tooltip="ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning">[Paper 156]</span> demonstrates ThinkAct achieving state-of-the-art results on LIBERO (84.4%) through action-aligned visual rewards. <span class="paper-ref" data-paper-id="173" data-tooltip="ChatVLA-2: Vision-Language-Action Model with Open-World Reasoning">[Paper 173]</span> shows ChatVLA-2 achieving 82.7% success on novel math equations and 81.4% on unseen object placement tasks.</p>

<p><strong>Efficient Training</strong>: <span class="paper-ref" data-paper-id="8" data-tooltip="Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation">[Paper 8]</span> demonstrates that using CoT as auxiliary supervision during training but performing direct action prediction at test time achieves state-of-the-art success rates while using significantly less training data (1.6M vs 5.9M samples). <span class="paper-ref" data-paper-id="28" data-tooltip="ESCA: Contextualizing Embodied Agents via Scene-Graph Generation">[Paper 28]</span> shows that augmenting MLLMs with structured scene graphs reduces perception errors from 69% to 30%.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">The Cross-Task Generalization Failure</h3>

<p>Multiple papers reveal that current VLA models fail to generalize beyond their training task distributions.</p>

<p><strong>Zero-Shot Failures</strong>: <span class="paper-ref" data-paper-id="122" data-tooltip="Exploring the Limits of Vision-Language-Action Manipulation in Cross-task Generalization">[Paper 122]</span> introduces AGNOSTOS showing that current VLA models struggle significantly with zero-shot cross-task generalization, with even state-of-the-art models failing on many unseen tasks involving novel object-action combinations. The proposed X-ICM method achieves only 6.0% improvement over œÄ0, highlighting the difficulty of the problem.</p>

<p><strong>Long-Horizon Planning Limitations</strong>: <span class="paper-ref" data-paper-id="94" data-tooltip="ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks">[Paper 94]</span> introduces ROVER demonstrating that VLMs hallucinate 50-70% more during non-expert trajectory moments, with standard approaches causing inefficiency and hallucinations when reasoning over more than 10 frames. <span class="paper-ref" data-paper-id="109" data-tooltip="World-aware Planning Narratives Enhance Large Vision-Language Model Planner">[Paper 109]</span> shows that open-source VLMs can achieve 60.7 absolute improvement in task success rates using only raw visual observations, but still struggle with commonsense reasoning and long-horizon planning.</p>

<p><strong>World Model Limitations</strong>: <span class="paper-ref" data-paper-id="50" data-tooltip="SAMPO: Scale-wise Autoregression with Motion Prompt for Generative World Models">[Paper 50]</span> introduces SAMPO achieving 4.4√ó faster inference for world models but shows that methods trained on simpler knots fail to generalize to more complex configurations. <span class="paper-ref" data-paper-id="89" data-tooltip="Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization">[Paper 89]</span> demonstrates that enhancing long-context capacity before reasoning fine-tuning yields significant performance gains, suggesting current models lack sufficient context modeling.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">Promising Directions</h3>

<p>Despite challenges, several papers propose solutions for improving embodied AI.</p>

<p><strong>Active Perception and Reasoning</strong>: <span class="paper-ref" data-paper-id="167" data-tooltip="PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments">[Paper 167]</span> introduces PhysVLM-AVR formalizing Active Visual Reasoning, achieving state-of-the-art performance on CLEVR-AVR by enabling models to actively gather information through sequential actions. <span class="paper-ref" data-paper-id="30" data-tooltip="VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents">[Paper 30]</span> demonstrates that explicit world model reasoning through StateEstimation and TransitionModeling achieves 3√ó performance improvement.</p>

<p><strong>Structured Scene Understanding</strong>: <span class="paper-ref" data-paper-id="28" data-tooltip="ESCA: Contextualizing Embodied Agents via Scene-Graph Generation">[Paper 28]</span> shows that structured scene graphs capturing entities, attributes, and spatial-temporal relations reduce perception errors from 69% to 30%. <span class="paper-ref" data-paper-id="187" data-tooltip="AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models">[Paper 187]</span> introduces AffordBot achieving state-of-the-art performance on fine-grained 3D embodied reasoning by unifying affordance grounding and motion estimation.</p>

<p><strong>Curriculum and Meta-Learning</strong>: <span class="paper-ref" data-paper-id="89" data-tooltip="Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization">[Paper 89]</span> demonstrates that DALI achieves up to 96.4% performance gains through dynamics-aligned context encoding with theoretical guarantees. <span class="paper-ref" data-paper-id="141" data-tooltip="ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents">[Paper 141]</span> introduces ReCAP achieving 32% gain on synchronous Robotouille through structured context management and recursive decomposition.</p>

<p>---</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">VI. Benchmarking and Evaluation: The Contamination Crisis</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">The Benchmark Saturation Problem</h3>

<p>A recurring theme across NeurIPS 2025 is that existing benchmarks fail to measure genuine capabilities, with models achieving high scores through shortcuts and memorization.</p>

<p><strong>Contamination and Shortcuts</strong>: <span class="paper-ref" data-paper-id="1" data-tooltip="What‚Äôs in Common? Multimodal Models Hallucinate When Reasoning Across Scenes">[Paper 1]</span> addresses contamination by creating entirely new real and synthetic images, revealing that models rely on object co-occurrence patterns rather than true reasoning. <span class="paper-ref" data-paper-id="18" data-tooltip="ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models">[Paper 18]</span> shows that 74% of ChartQA questions are solvable through text extraction alone. <span class="paper-ref" data-paper-id="62" data-tooltip="Face-Human-Bench: A Comprehensive Benchmark of Face and Human Understanding for Multi-modal Assistants">[Paper 62]</span> reveals that even Face-Human-Bench, designed for face understanding, shows the best open-source model outperforming the best closed-source model, suggesting benchmark-specific optimization.</p>

<p><strong>The Need for Fresh Benchmarks</strong>: <span class="paper-ref" data-paper-id="147" data-tooltip="Seeking and Updating with Live Visual Knowledge">[Paper 147]</span> introduces LIVEVQA with 107,143 questions from April 2024-May 2025, revealing that current MLLMs struggle significantly with recent visual content beyond their training cutoff. <span class="paper-ref" data-paper-id="39" data-tooltip="NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions">[Paper 39]</span> introduces NaturalReasoning with 2.8M diverse, challenging questions grounded in pretraining corpora, demonstrating superior sample efficiency.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">Novel Evaluation Paradigms</h3>

<p>Several papers introduce innovative evaluation approaches addressing benchmark limitations.</p>

<p><strong>Process-Level Evaluation</strong>: <span class="paper-ref" data-paper-id="4" data-tooltip="Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards">[Paper 4]</span> introduces Process Reward Models for evaluating intermediate reasoning quality, revealing that quality matters more than length for visual reasoning. <span class="paper-ref" data-paper-id="22" data-tooltip="MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models">[Paper 22]</span> introduces process-reward evaluation tracing multi-hop reasoning paths rather than just final answers. <span class="paper-ref" data-paper-id="74" data-tooltip="More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models">[Paper 74]</span> introduces RH-AUC capturing the dynamic balance between reasoning performance and hallucination across varying reasoning lengths.</p>

<p><strong>Multi-Dimensional Assessment</strong>: <span class="paper-ref" data-paper-id="17" data-tooltip="CAPability: A Comprehensive Visual Caption Benchmark for Evaluating Both Correctness and Thoroughness">[Paper 17]</span> introduces CAPability evaluating both correctness (precision) and thoroughness (hit) across 12 dimensions, revealing significant performance gaps and the "know but cannot tell" phenomenon. <span class="paper-ref" data-paper-id="96" data-tooltip="FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges">[Paper 96]</span> introduces FineGRAIN with 27 fine-grained failure modes, revealing that VQAScore has only 57.7% agreement with human judgments versus 67.4% for fine-grained evaluation.</p>

<p><strong>Contamination-Free Evaluation</strong>: <span class="paper-ref" data-paper-id="21" data-tooltip="Counterfactual Evolution of Multimodal Datasets via Visual Programming">[Paper 21]</span> introduces SCOPE using executable programs as symbolic representations to enable verifiable transformations and controllable difficulty enhancement. <span class="paper-ref" data-paper-id="34" data-tooltip="Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing">[Paper 34]</span> introduces RISEBench for reasoning-informed visual editing, revealing that GPT-4o-Image achieves only 28.9% accuracy.</p>

<p>---</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">VII. Critical Gaps and Future Directions</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">Unresolved Fundamental Problems</h3>

<p>Despite progress, several critical problems remain largely unsolved.</p>

<p><strong>The Reasoning-Grounding Dilemma</strong>: <span class="paper-ref" data-paper-id="74" data-tooltip="More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models">[Paper 74]</span> reveals a fundamental tension: longer reasoning chains reduce visual attention allocation, causing drift toward language priors. <span class="paper-ref" data-paper-id="9" data-tooltip="Caption This, Reason That: VLMs Caught in the Middle">[Paper 9]</span> shows models can caption spatial information but fail to utilize it during reasoning. This suggests current architectures cannot effectively integrate visual grounding with extended reasoning.</p>

<p><strong>Cross-Modal Alignment</strong>: <span class="paper-ref" data-paper-id="80" data-tooltip="Quantifying Cross-Modality Memorization in Vision-Language Models">[Paper 80]</span> reveals significant asymmetric cross-modality memorization gaps, where knowledge learned in one modality transfers imperfectly to the other. <span class="paper-ref" data-paper-id="142" data-tooltip="Cross-modal Associations in Vision and Language Models: Revisiting the Bouba-Kiki Effect">[Paper 142]</span> demonstrates that approximately 70% of failures in multimodal symbolic reasoning stem from logical misalignment between visual and textual modalities.</p>

<p><strong>Compositional Generalization</strong>: <span class="paper-ref" data-paper-id="64" data-tooltip="Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model">[Paper 64]</span> shows that even RL-trained VLMs struggle significantly with cross-modal and cross-task reasoning, achieving only 52.8% accuracy on compositional tasks. <span class="paper-ref" data-paper-id="106" data-tooltip="Counterfactual reasoning: an analysis of in-context emergence">[Paper 106]</span> demonstrates that counterfactual reasoning requires noise abduction capabilities not needed in standard in-context learning.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">Overhyped Approaches</h3>

<p>Several popular approaches show limited evidence of genuine progress.</p>

<p><strong>Scaling Alone Is Insufficient</strong>: <span class="paper-ref" data-paper-id="95" data-tooltip="RBench-V: A Primary Assessment for Visual Reasoning Models with Multimodal Outputs">[Paper 95]</span> reveals that scaling model size, omni-modal capabilities, and long text-only chain-of-thought approaches are insufficient for solving vision-indispensable reasoning tasks. <span class="paper-ref" data-paper-id="131" data-tooltip="Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning">[Paper 131]</span> shows that long-context capability is fundamentally important for reasoning, but models still struggle with effective context handling.</p>

<p><strong>Multimodal Fusion Challenges</strong>: <span class="paper-ref" data-paper-id="48" data-tooltip="OmniBench: Towards The Future of  Universal Omni-Language Models">[Paper 48]</span> reveals that current MLLM training paradigms overlook the ability to construct consistent contexts from multiple modalities. <span class="paper-ref" data-paper-id="124" data-tooltip="Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation">[Paper 124]</span> identifies attention bias toward visual tokens over contextual tokens and conflicts between parametric and contextual knowledge as critical limiting factors.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">Promising Research Directions</h3>

<p>Several emerging directions show particular promise for future work.</p>

<p><strong>Causal and Counterfactual Reasoning</strong>: <span class="paper-ref" data-paper-id="77" data-tooltip="CF-VLMÔºöCounterFactual Vision-Language Fine-tuning">[Paper 77]</span> introduces CF-VLM achieving state-of-the-art results through counterfactual fine-tuning that teaches models to identify causal decision points. <span class="paper-ref" data-paper-id="87" data-tooltip="Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition">[Paper 87]</span> demonstrates that representation-level counterfactual calibration achieves substantial improvements in zero-shot recognition. <span class="paper-ref" data-paper-id="159" data-tooltip="Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning">[Paper 159]</span> introduces causal sufficiency and necessity analysis for optimizing CoT reasoning.</p>

<p><strong>Active and Iterative Perception</strong>: <span class="paper-ref" data-paper-id="130" data-tooltip="Pixel Reasoner: Incentivizing Pixel Space Reasoning via Curiosity-Driven Reinforcement Learning">[Paper 130]</span> introduces pixel-space reasoning enabling VLMs to perform reasoning operations directly on visual inputs through zoom-in and frame selection. <span class="paper-ref" data-paper-id="135" data-tooltip="Multi-step Visual Reasoning with Visual Tokens Scaling and Verification">[Paper 135]</span> introduces VTS-V enabling dynamic visual token scaling through an MDP formulation with formal termination guarantees. <span class="paper-ref" data-paper-id="164" data-tooltip="VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception">[Paper 164]</span> introduces Visual Test-Time Scaling achieving over 5% average improvement by enabling iterative perception refinement.</p>

<p><strong>Unified Multimodal Architectures</strong>: <span class="paper-ref" data-paper-id="44" data-tooltip="UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning">[Paper 44]</span> introduces UniPixel unifying object referring and segmentation through an object memory bank, achieving state-of-the-art performance on 10 benchmarks. <span class="paper-ref" data-paper-id="72" data-tooltip="MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO">[Paper 72]</span> demonstrates that unified vision-language models can generate high-quality multimodal interleaved outputs using minimal training data through GRPO.</p>

<p>---</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">VIII. Practical Takeaways for Practitioners</h2>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">What Actually Works</h3>

<p>Based on the evidence from NeurIPS 2025, practitioners should focus on:</p>

<p><strong>1. Adopt Two-Stage Training (SFT + RL)</strong>: The evidence is overwhelming that combining supervised fine-tuning for cold-start with reinforcement learning for refinement achieves superior results [Paper 10, 11, 25, 42, 63, 73, 97, 110, 128, 135, 156, 157, 161, 163, 168]. Use SFT to surface latent reasoning behaviors, then RL to refine and scale them.</p>

<p><strong>2. Prioritize Visual Grounding</strong>: For visual reasoning tasks, explicit grounding through bounding boxes [Paper 97, 128], visual drawing operations <span class="paper-ref" data-paper-id="98" data-tooltip="Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing">[Paper 98]</span>, or structured visual representations [Paper 67, 99, 183] consistently outperforms text-only reasoning. Don't assume models can reason visually without explicit spatial anchoring.</p>

<p><strong>3. Use Process-Level Rewards</strong>: Fine-grained, step-level rewards [Paper 4, 6, 42] significantly outperform outcome-only rewards for complex reasoning tasks. Invest in reward design that evaluates intermediate reasoning quality, not just final answers.</p>

<p><strong>4. Be Selective About Reasoning</strong>: Not all tasks benefit from extended reasoning [Paper 165, 82]. Train models to selectively engage reasoning based on task difficulty, potentially reducing token usage by 90% without sacrificing performance.</p>

<p><strong>5. Address Data Quality Over Quantity</strong>: <span class="paper-ref" data-paper-id="76" data-tooltip="SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement">[Paper 76]</span> achieves state-of-the-art performance with only 7.5k samples through difficulty-curated selection. <span class="paper-ref" data-paper-id="107" data-tooltip="Teaching Language Models to Reason with Tools">[Paper 107]</span> shows 30 high-quality manually annotated samples can match models trained on hundreds of automatically generated ones. Focus on sample quality and difficulty rather than scale.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">What to Avoid</h3>

<p><strong>1. Don't Rely on Existing Benchmarks Alone</strong>: [Paper 1, 15, 18, 64, 95] demonstrate that benchmark saturation doesn't indicate genuine capability. Evaluate on contamination-free, shortcut-resistant benchmarks that isolate the capabilities you care about.</p>

<p><strong>2. Don't Assume More Reasoning Is Better</strong>: [Paper 8, 16, 74, 119, 172] show that extended reasoning can degrade performance in dynamic environments, instruction-following, and inductive tasks. Understand when reasoning helps versus hurts.</p>

<p><strong>3. Don't Ignore Visual Grounding Failures</strong>: [Paper 1, 9, 15, 64] reveal that high benchmark scores mask fundamental failures in spatial understanding and visual reasoning. Test explicitly for visual grounding capabilities before deploying.</p>

<p><strong>4. Don't Expect Cross-Task Generalization</strong>: [Paper 122, 139] show that VLA models fail dramatically on zero-shot cross-task generalization. Plan for task-specific fine-tuning rather than assuming broad generalization.</p>

<h3 style="color: #1c3664; font-weight: 600; margin-top: 25px;">Emerging Best Practices</h3>

<p><strong>For Visual Reasoning</strong>: Combine structured visual representations [Paper 67, 99, 183] with explicit grounding [Paper 97, 98, 128] and process-level RL training [Paper 4, 42, 157]. Use difficulty-aware curriculum learning <span class="paper-ref" data-paper-id="114" data-tooltip="Fast-Slow Thinking GRPO for Large Vision-Language Model Reasoning">[Paper 114]</span> and visual evidence rewards <span class="paper-ref" data-paper-id="6" data-tooltip="When Thinking Drifts: Evidential Grounding for Robust Video Reasoning">[Paper 6]</span>.</p>

<p><strong>For Embodied AI</strong>: Integrate world model reasoning <span class="paper-ref" data-paper-id="30" data-tooltip="VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents">[Paper 30]</span> with structured scene understanding [Paper 28, 187]. Use active perception <span class="paper-ref" data-paper-id="167" data-tooltip="PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments">[Paper 167]</span> and curriculum learning <span class="paper-ref" data-paper-id="89" data-tooltip="Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization">[Paper 89]</span>. Don't expect zero-shot cross-task generalization <span class="paper-ref" data-paper-id="122" data-tooltip="Exploring the Limits of Vision-Language-Action Manipulation in Cross-task Generalization">[Paper 122]</span>.</p>

<p><strong>For Multimodal Understanding</strong>: Address attention bias <span class="paper-ref" data-paper-id="124" data-tooltip="Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation">[Paper 124]</span> and cross-modal alignment [Paper 80, 142] explicitly. Use counterfactual learning [Paper 77, 87] to improve compositional understanding. Leverage visual test-time scaling [Paper 135, 164] for complex tasks.</p>

<p><strong>For Efficient Training</strong>: Use dynamic sampling strategies <span class="paper-ref" data-paper-id="144" data-tooltip="Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL">[Paper 144]</span>, selective sample replay <span class="paper-ref" data-paper-id="45" data-tooltip="VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning">[Paper 45]</span>, and difficulty-based curriculum <span class="paper-ref" data-paper-id="114" data-tooltip="Fast-Slow Thinking GRPO for Large Vision-Language Model Reasoning">[Paper 114]</span>. Prioritize sample quality over quantity [Paper 76, 107]. Consider training-free methods [Paper 32, 54, 68] for simpler tasks.</p>

<p>---</p>

<h2 style="color: #1c3664; font-weight: 600; margin-top: 30px;">Conclusion: A Field in Transition</h2>

<p>NeurIPS 2025 reveals a field at a critical juncture. The wholesale adoption of reinforcement learning for reasoning represents genuine progress, with consistent evidence of superior performance across diverse tasks. However, fundamental limitations in visual grounding, cross-modal alignment, and compositional generalization remain largely unsolved.</p>

<p>The most important insight is the growing recognition that benchmark saturation does not equal genuine capability. Papers like [Paper 1, 15, 18, 64, 95, 127] systematically demonstrate that models achieve high scores through shortcuts, textual cues, and memorization rather than true understanding. This has profound implications for how we evaluate and deploy multimodal AI systems.</p>

<p>The field is moving toward more structured, grounded, and verifiable reasoning approaches. The convergence of explicit visual grounding [Paper 97, 98, 128], process-level supervision [Paper 4, 42, 157], and reinforcement learning [Paper 10, 11, 161] represents a promising direction. However, critical challenges around hallucination [Paper 74, 137], efficiency [Paper 165, 82], and generalization [Paper 122, 139] require continued attention.</p>

<p>For practitioners, the message is clear: adopt two-stage training with SFT and RL, prioritize visual grounding for visual tasks, use process-level rewards, be selective about when to apply reasoning, and focus on data quality over quantity. Most importantly, don't trust benchmark scores alone‚Äîevaluate explicitly for the capabilities you need in deployment.</p>

<p>The next frontier lies in solving the reasoning-grounding dilemma, achieving genuine compositional generalization, and developing models that can actively perceive and reason in dynamic, open-world environments. The papers at NeurIPS 2025 provide a roadmap, but the journey is far from complete.</p></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Modal for paper details -->
    <div id="paperModal" class="modal">
        <div class="modal-content">
            <button class="modal-close" onclick="closePaperModal()">‚úï</button>
            <div id="modalContent"></div>
        </div>
    </div>

    <script>
        // Embedded paper data
        const papers = [{"paper_id": 4453333, "title": "What‚Äôs in Common? Multimodal Models Hallucinate When Reasoning Across Scenes", "authors": "Candace Ross, Florian Bordes, Adina Williams, Polina Kirichenko, Mark Ibrahim", "pdf_url": "https://arxiv.org/pdf/2511.03768", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52654, "poster_number": 4713, "tag": "SD-1-4713 | Exhibit Hall C,D,E ", "relevance_score": 93.57, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "State-of-the-art multimodal models achieve only 35% accuracy on Common-O Bench and <1% on Common-O Complex, despite saturating existing perception benchmarks at 80-90%. Models hallucinate objects significantly more when reasoning across scenes (53% of the time) compared to single-image perception, with hallucinations increasing when similar objects are present. Multi-image training provides 3√ó improvement over single-image trained models, but even the best models struggle dramatically with cross-scene reasoning.", "description": "This paper introduces Common-O Bench, a novel benchmark with 10.5k examples using exclusively new real and synthetic images to evaluate multimodal models' ability to reason across scenes by identifying common objects. Inspired by cognitive tests for humans, the benchmark tests whether models can identify 'what's in common' across multiple images containing 3-7 objects in diverse configurations, going beyond simple perception to probe multi-scene reasoning capabilities.", "key_contribution": "The paper establishes that reasoning across scenes remains an open challenge for multimodal models, introducing Common-O Bench as the first large-scale benchmark using contamination-free images to specifically test cross-scene reasoning rather than just perception, revealing a critical gap between benchmark saturation and real-world reasoning capabilities.", "novelty": "Unlike existing benchmarks that either use web-scraped images (risking contamination) or focus on single-image perception, this work creates entirely new real and synthetic images to test multi-image reasoning in a controlled setting. The benchmark reveals that models rely on object co-occurrence patterns from training rather than true reasoning, as evidenced by increased hallucinations when similar objects appear together. The work uniquely separates perception from reasoning by comparing single-image and multi-image performance on identical scenes.", "ai_categories": ["Hallucination Detection and Model Robustness", "Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought"], "score": 93.57, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4439926, "title": "SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens", "authors": "Yinhan He, Wendy Zheng, Yaochen Zhu, Zaiyi Zheng, Lin Su, Sriram Vasudevan, Qi Guo, Liangjie Hong, Jundong Li", "pdf_url": "https://openreview.net/pdf/39a70c107ed5819e3435a94da518615c9e3db25c.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52482, "poster_number": 1509, "tag": "SD-1-1509 | Exhibit Hall C,D,E ", "relevance_score": 71.56, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "SemCoT achieves superior performance in both efficiency and effectiveness compared to state-of-the-art implicit Chain-of-Thought methods by jointly optimizing token-level generation speed and semantic alignment. The framework demonstrates that using a lightweight language model to generate implicit reasoning tokens, guided by a customized sentence transformer, can maintain reasoning quality while significantly reducing computational costs. Experiments show SemCoT achieves the highest answer accuracy across multiple datasets while maintaining near-optimal inference speed.", "description": "This paper proposes SemCoT, a framework that accelerates Chain-of-Thought reasoning in Large Language Models by using semantically-aligned implicit tokens instead of explicit reasoning text. The approach employs a contrastively-trained sentence transformer to ensure semantic alignment between implicit and ground-truth reasoning, and uses a lightweight language model (distilled from the original LLM) to efficiently generate implicit reasoning tokens. The framework addresses two key challenges: preserving semantic alignment with ground-truth reasoning and reducing the computational cost of generating each reasoning token.", "key_contribution": "The main contribution is a novel implicit CoT framework that simultaneously optimizes for both semantic preservation and token-level generation efficiency through two key innovations: (1) a customized sentence transformer that enforces semantic alignment between implicit and explicit reasoning via contrastive learning, and (2) an efficient implicit reasoning generator based on a lightweight language model that dramatically reduces per-token generation costs.", "novelty": "Unlike existing implicit CoT methods that either discard ground-truth reasoning entirely, match only keywords, or optimize solely for answer correctness, SemCoT explicitly enforces semantic alignment between implicit reasoning embeddings and ground-truth reasoning through a specialized sentence transformer. The work is the first to address both the semantic alignment problem and the token-level generation efficiency problem jointly, recognizing that reducing reasoning length alone is insufficient when each token generation is computationally expensive. The use of a lightweight distilled model for generating implicit reasoning, guided by semantic alignment constraints, represents a novel approach to balancing efficiency and effectiveness.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization"], "score": 71.56, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4230311, "title": "RelationAdapter: Learning and Transferring Visual Relation with Diffusion Transformers", "authors": "Yan Gong, Yiren Song, Yicheng Li, Chenglin Li, Yin Zhang", "pdf_url": "https://openreview.net/pdf/2c9877508b7505c4c6e730f2b3f8e054edbe33ea.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52107, "poster_number": 4313, "tag": "SD-1-4313 | Exhibit Hall C,D,E ", "relevance_score": 70.68, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "RelationAdapter introduces a lightweight DiT-based adapter that learns visual transformations from paired images, achieving superior performance over baselines with 30.8% faster inference and reduced memory usage. The method demonstrates strong generalization to unseen editing tasks through a decoupled attention mechanism that prevents feature contamination. The paper also contributes Relation252K, a comprehensive dataset with 218 diverse editing tasks spanning 251,580 training instances.", "description": "This paper presents RelationAdapter, a framework for visual prompt-based image editing that learns transformations from source-target image pairs and applies them to new query images. The approach combines a dual-branch adapter module that extracts visual relationships with an In-Context Editor that performs consistency-aware editing through positional encoding cloning and noise-free conditioning. The method is trained on a large-scale dataset covering low-level processing, style transfer, semantic editing, and customized generation tasks.", "key_contribution": "The main contribution is RelationAdapter, the first DiT-based adapter module that decouples visual prompt extraction from image generation through a dedicated attention fusion mechanism, enabling efficient and accurate transfer of editing intent from minimal examples without requiring full bidirectional attention over concatenated inputs.", "novelty": "Unlike existing methods that concatenate image pairs with the target image (causing high memory consumption and degraded text prompt performance), RelationAdapter uses a decoupled attention injection strategy with separate key-value projections for visual prompts. The approach addresses limitations of single-reference methods that struggle with non-rigid transformations by explicitly modeling pairwise visual relationships. The positional encoding cloning and noise-free paradigm for conditional features further improve spatial alignment and detail preservation compared to conventional in-context learning approaches.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "In-Context Learning and Few-Shot Adaptation", "Prompt Learning and Optimization"], "score": 70.68, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4389810, "title": "Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards", "authors": "Honghao Chen, Xingzhou Lou, Xiaokun Feng, Kaiqi Huang, Xinlong Wang", "pdf_url": "https://openreview.net/pdf/b75967c26b83c8cc4921f37f32977c5e087b1f44.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52604, "poster_number": 4803, "tag": "SD-1-4803 | Exhibit Hall C,D,E ", "relevance_score": 83.09, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that fine-grained, step-level reasoning with structured outputs significantly outperforms coarse-grained chain-of-thought approaches in vision-language models. The work reveals that for visual reasoning, ensuring high-quality reasoning steps is more important than simply increasing reasoning length, contrary to findings in pure language models. The proposed Process Reward Model (PRM) enables accurate evaluation of intermediate reasoning steps, leading to effective reinforcement learning and inference-time scaling.", "description": "The paper introduces Chain-of-Step (CoS) reasoning for vision-language models, which structures reasoning into discrete steps with three components: Name, Thought, and Reflection. The framework includes ShareGPT-Step-300K (a 300K structured reasoning dataset), a Process Reward Model for step-level evaluation, and an iterative Direct Preference Optimization approach. The method enables fine-grained reward signals for both training and inference-time scaling through step-level beam search.", "key_contribution": "The main contribution is a complete framework for fine-grained step-level reasoning in VLMs, including: (1) a structured reasoning template with special tokens for consistent step decomposition, (2) a large-scale step-annotated dataset, (3) a Process Reward Model trained to evaluate intermediate reasoning quality, and (4) an RL training pipeline that leverages fine-grained rewards to achieve consistent improvements across multiple benchmarks.", "novelty": "Unlike existing VLM reasoning approaches that use coarse-grained, unstructured chain-of-thought outputs, this work introduces fine-grained structured reasoning with explicit step decomposition and evaluation. It addresses the limitation that current VLMs cannot accurately assess intermediate reasoning quality by training a PRM on process-annotated data using Monte Carlo estimation and LLM-as-Judge methods. The work also reveals unique properties of visual reasoning‚Äîthat quality matters more than length‚Äîwhich differs from findings in pure language model reasoning.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Multimodal Benchmarks and Evaluation"], "score": 83.09, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4279454, "title": "Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames", "authors": "Anurag Arnab, Ahmet Iscen, Mathilde Caron, Alireza Fathi, Cordelia Schmid", "pdf_url": "https://openreview.net/pdf/ecef04e6004300bdfe10dc8ca9560dcffe5ecf5d.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52454, "poster_number": 4801, "tag": "SD-1-4801 | Exhibit Hall C,D,E ", "relevance_score": 87.42, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Temporal Chain of Thought (TCoT) achieves state-of-the-art results on 4 video QA datasets by using a VLM to iteratively select relevant frames before answering, rather than processing all frames. On hour-long videos (LVBench), TCoT with a 32K context window outperforms standard inference with a 700K context window by 2.8 points, demonstrating that curated context is more effective than simply increasing context length. The method shows that inference-time computation scaling through intelligent frame selection is more effective than just scaling model parameters or context windows.", "description": "This paper presents Temporal Chain of Thought, an inference strategy for long-video question answering that addresses the problem of VLMs being overwhelmed by irrelevant content in long contexts. The approach uses the same VLM to first identify and extract the most relevant frames from a video based on the question, then uses only these selected frames for answering. The method partitions long videos into segments, processes each independently to extract relevant frames, and aggregates them for final reasoning.", "key_contribution": "The main contribution is a novel VLM inference strategy that decomposes video QA into two stages: context aggregation (selecting relevant frames) and answering, both performed by the same VLM without external models. This approach enables effective processing of videos exceeding the model's native context window while improving accuracy by removing distractors.", "novelty": "Unlike prior works that use multiple specialized models (captioners, retrievers, LLMs) or rely on intermediate text representations, TCoT uses a single VLM for both frame selection and answering, operating directly on video frames. The method addresses limitations of long-context VLMs that struggle with irrelevant distractors by adaptively selecting relevant context based on question type. It introduces a partitioned segment approach that decouples video length from context limits, enabling processing of hour-long videos while maintaining or improving accuracy compared to much larger context windows.", "ai_categories": ["Video Understanding and Temporal Reasoning", "Vision-Language Reasoning and Chain-of-Thought"], "score": 87.42, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4409948, "title": "When Thinking Drifts: Evidential Grounding for Robust Video Reasoning", "authors": "Mi Luo, Zihui Xue, Alex Dimakis, Kristen Grauman", "pdf_url": "https://openreview.net/pdf/013556e3887ae4788bffd0dd1845278e3ce6ff47.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52843, "poster_number": 5005, "tag": "SD-1-5005 | Exhibit Hall C,D,E ", "relevance_score": 72.02, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper reveals that Chain-of-Thought (CoT) reasoning often degrades performance in video understanding tasks, causing a phenomenon called 'visual thinking drift' where models generate verbose but misleading reasoning that diverges from actual visual evidence. The proposed Visual Evidence Reward (VER) framework addresses this by explicitly rewarding reasoning traces grounded in visual evidence, achieving consistent improvements (+4.0% average, up to +9.0%) across 10 video benchmarks.", "description": "The paper presents a systematic analysis of CoT reasoning failures in video understanding, demonstrating that models often hallucinate visual details and override correct intuitions. To counteract this, the authors introduce Visual Evidence Reward (VER), a reinforcement learning framework using GRPO that rewards reasoning traces verifiably grounded in question-specific visual evidence generated through inverted prompting, training models to 'see while thinking' rather than just 'think before answering'.", "key_contribution": "The main contribution is the identification of 'visual thinking drift' in video reasoning and the introduction of Visual Evidence Reward (VER), a novel RL-based reward mechanism that explicitly encourages MLLMs to ground their chain-of-thought reasoning in observable visual evidence, leading to more accurate and reliable video understanding.", "novelty": "Unlike prior work that applies CoT prompting directly to video tasks or uses generic hallucination mitigation, this paper identifies the unique failure mode of visual thinking drift specific to video reasoning and addresses it through question-conditioned visual evidence generation via inverted prompting. The approach differs from existing methods by using a binary reward signal that verifies whether reasoning references actual visual content, rather than relying on rule-based rewards or generic video captions.", "ai_categories": ["Video Understanding and Temporal Reasoning", "Hallucination Detection and Model Robustness", "Reinforcement Learning for Multimodal Models"], "score": 72.02, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4212865, "title": "AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking", "authors": "Xiangqi Wang, Yue Huang, Yanbo Wang, Xiaonan Luo, Kehan Guo, Yujun Zhou, Xiangliang Zhang", "pdf_url": "https://openreview.net/pdf/dd1128b101a8e529cfa219b76424da7861923e32.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52890, "poster_number": 5418, "tag": "SD-1-5418 | Exhibit Hall C,D,E ", "relevance_score": 77.46, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "AdaReasoner achieves consistent performance improvements across six different LLMs and diverse reasoning tasks by adaptively configuring three key hyperparameters: reasoning instruction format, generation temperature, and number of reasoning steps. The method demonstrates fast convergence with only 50-100 training examples (few-shot setting) and maintains strong out-of-distribution generalization. Theoretical analysis provides sublinear regret bounds O(‚àö(K|A|log|A|)) and convergence guarantees, while empirical results show average accuracy improvements of 5-12% over standard Chain-of-Thought approaches.", "description": "This paper introduces AdaReasoner, an LLM-agnostic plugin that automatically adapts reasoning configurations for different types of questions requiring diverse cognitive processes. The system uses a reinforcement learning framework with a factorized action space covering reasoning instructions, temperature settings, and reasoning steps, trained using Boltzmann exploration and a pretrained reward model. AdaReasoner operates as a decision-making agent that selects optimal configurations for each input question, enabling LLMs to dynamically adjust their reasoning strategies based on task-specific demands.", "key_contribution": "The main contribution is a theoretically-grounded, data-efficient RL framework that automates the selection of reasoning configurations (instruction format, temperature, reasoning steps) on a per-question basis, achieving superior performance across multiple LLMs with only few-shot training while providing convergence guarantees and sublinear regret bounds.", "novelty": "Unlike existing prompting methods that use fixed, general-purpose configurations (CoT, ToT) or focus only on prompt templates (Auto-CoT), AdaReasoner jointly optimizes multiple reasoning hyperparameters adaptively for each question. It addresses the limitation that different tasks require different reasoning strategies by using a factorized RL policy with Boltzmann exploration, enabling efficient few-shot learning. The approach is unique in providing both theoretical guarantees (convergence bounds, regret analysis) and practical effectiveness across diverse reasoning types, from logical to creative tasks.", "ai_categories": ["Prompt Learning and Optimization", "Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models"], "score": 77.46, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4204124, "title": "Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation", "authors": "Shuo Wang, Yongcai Wang, Wanting Li, Xudong Cai, Yucheng Wang, Maiyue Chen, kaihui.wang, Zhizhong Su, Deying Li, Zhaoxin Fan", "pdf_url": "https://openreview.net/pdf/4eb2cec937108c8a5e1785cf53b814a0999b4eb2.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52359, "poster_number": 2308, "tag": "SD-1-2308 | Exhibit Hall C,D,E ", "relevance_score": 82.12, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper reveals a critical phenomenon called Test-time Reasoning Collapse (TRC), where explicit Chain-of-Thought reasoning during testing consistently degrades navigation performance in Vision-Language Navigation tasks. The proposed Aux-Think framework achieves state-of-the-art success rates while using significantly less training data (1.6M vs 5.9M samples) by using CoT as auxiliary supervision during training but performing direct action prediction at test time. The work introduces R2R-CoT-320k, the first large-scale CoT-annotated dataset specifically designed for VLN tasks.", "description": "The paper conducts the first systematic investigation of reasoning strategies (No-Think, Pre-Think, Post-Think) for Vision-Language Navigation in continuous environments. It proposes Aux-Think, a novel training paradigm that decouples CoT reasoning and action prediction during training through auxiliary tasks (CoT-based reasoning, instruction-based reasoning, and receding-horizon action planning), while maintaining efficient No-Think testing. The approach is validated on R2R-CE and RxR-CE benchmarks using monocular RGB observations.", "key_contribution": "The main contribution is the Aux-Think framework that leverages Chain-of-Thought reasoning as auxiliary supervision during training to internalize structured reasoning patterns, while avoiding explicit reasoning at test time to prevent reasoning collapse. This is accompanied by R2R-CoT-320k, the first CoT dataset for VLN containing over 320,000 reasoning traces.", "novelty": "Unlike prior VLN methods that either ignore reasoning or apply CoT at test time, this work demonstrates that test-time reasoning fails in dynamic, partially observable navigation environments due to error accumulation and distribution shift. The novel insight is using CoT supervision only during training (inspired by dual-process theory) to improve data efficiency and generalization, while maintaining fast, reliable action prediction at test time. This addresses the fundamental limitation of applying static-task reasoning strategies to long-horizon embodied navigation.", "ai_categories": ["Embodied AI and Vision-Language-Action Models", "Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models"], "score": 82.12, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4219301, "title": "Caption This, Reason That: VLMs Caught in the Middle", "authors": "Zihan Weng, Lucas Gomez, Taylor Whittington Webb, Pouya Bashivan", "pdf_url": "https://openreview.net/pdf/756a05a421ed180f3aedfcaa7a55a73f50e83d30.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52393, "poster_number": 2112, "tag": "SD-1-2112 | Exhibit Hall C,D,E ", "relevance_score": 83.22, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "VLMs exhibit distinct cognitive profiles with strong category perception but significant weaknesses in spatial reasoning and selective attention. A simple vision-text decoupling method (self-captioning) dramatically improves performance, revealing that the bottleneck lies not in visual perception or language reasoning alone, but in integrating visual features into the reasoning process. Fine-tuning on composite visual reasoning tasks improves core cognitive abilities, with strong correlations between PAM/CVR performance and established benchmarks like MMMU-Pro and VQAv2.", "description": "This paper systematically evaluates Vision-Language Models through a cognitive science lens, analyzing performance along three core axes: Perception, Attention, and Memory (PAM). Using procedurally-generated tasks from the iWISDM environment, the authors assess state-of-the-art VLMs including GPT-4o on both isolated cognitive abilities and composite visual reasoning tasks, investigating failure modes and improvement methods through vision-text decoupling and targeted fine-tuning.", "key_contribution": "The paper introduces the PAM dataset for systematic cognitive evaluation of VLMs and demonstrates that a simple self-captioning approach (having models reason over their own generated text descriptions rather than directly from images) significantly improves visual reasoning performance, revealing a critical Chain-of-Thought bottleneck in current VLMs even for single-image spatial reasoning tasks.", "novelty": "Unlike existing VLM benchmarks that focus on high-level reasoning or abstract tasks, this work adopts a cognitive science framework to isolate and measure fundamental abilities (Perception, Attention, Memory) and their interactions. The vision-text decoupling analysis reveals that VLMs can accurately caption spatial information but fail to utilize it during direct visual reasoning, identifying a specific CoT training deficiency rather than a fundamental perceptual or reasoning limitation. This diagnosis differs from prior work by pinpointing the integration bottleneck between vision and language modalities.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Multimodal Benchmarks and Evaluation", "Visual Grounding and Spatial Reasoning"], "score": 83.22, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4128900, "title": "Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning of Vision Language Models", "authors": "Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Xiansheng Chen, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang", "pdf_url": "https://openreview.net/pdf/65dc0346cca4c6d3ceb0f9fcbf6938b5bb15a527.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52373, "poster_number": 1508, "tag": "SD-1-1508 | Exhibit Hall C,D,E ", "relevance_score": 92.8, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Reason-RFT is the first two-stage reinforcement fine-tuning framework for visual reasoning that combines supervised fine-tuning with GRPO-based reinforcement learning, achieving state-of-the-art performance while demonstrating superior generalization under domain shift. The framework achieves over 90% of full-dataset SFT performance using less than 5% of the data, and significantly outperforms both SFT-only and RL-only baselines across visual counting, structure perception, and spatial transformation tasks. Notably, the 2B Reason-RFT model surpasses GPT-4o by 31.7% on spatial transformation under domain shift.", "description": "This paper introduces Reason-RFT, a novel training paradigm for vision-language models that addresses the limitations of traditional supervised fine-tuning approaches in visual reasoning tasks. The framework employs a two-stage process: first, supervised fine-tuning with curated Chain-of-Thought data activates reasoning potential, followed by Group Relative Policy Optimization (GRPO) reinforcement learning that generates multiple reasoning-response pairs to enhance generalization. The authors construct a comprehensive benchmark dataset covering visual counting, structure perception, and spatial transformation to systematically evaluate visual reasoning capabilities.", "key_contribution": "The main contribution is the first two-stage reinforcement fine-tuning framework specifically designed for visual reasoning in VLMs, combining the complementary strengths of SFT-based reasoning activation and RL-based enhancement to achieve superior performance, generalization, and data efficiency compared to existing paradigms.", "novelty": "Unlike traditional SFT-based methods that suffer from overfitting and cognitive rigidity, Reason-RFT introduces reinforcement learning to visual reasoning tasks, enabling dynamic exploration and feedback-driven optimization that significantly improves robustness under domain shift. The work adapts DeepSeek-R1 methodologies to multimodal settings for the first time, providing systematic analysis comparing SFT-based and RL-based paradigms in visual reasoning. The framework addresses the transient adaptation gap and reasoning redundancy phenomena through careful reward design and two-stage training, achieving remarkable data efficiency while maintaining strong generalization.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Visual Grounding and Spatial Reasoning"], "score": 92.8, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4121768, "title": "OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles", "authors": "Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, Kai-Wei Chang", "pdf_url": "https://openreview.net/pdf/62ceb097c643e0416c764c187ebf4f4d6d1ba9c3.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52817, "poster_number": 5514, "tag": "SD-1-5514 | Exhibit Hall C,D,E ", "relevance_score": 70.66, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "OpenVLThinker demonstrates that iteratively alternating between supervised fine-tuning (SFT) and reinforcement learning (RL) enables sophisticated chain-of-thought reasoning in vision-language models, achieving 3.8% improvement on MathVista and competitive performance with GPT-4o using only 12K training examples. The key insight is that SFT surfaces latent reasoning actions (triggered by tokens like 'wait', 'check') that narrow the RL search space, while RL refines these behaviors and generates better data for subsequent iterations.", "description": "This paper introduces OpenVLThinker-7B, an open-source large vision-language model trained through iterative cycles of SFT and RL to achieve complex reasoning capabilities. The approach addresses the challenge that naive distillation from text-based reasoning models (like DeepSeek-R1) to vision-language models fails due to imprecise visual grounding, while pure RL struggles with the large search space. The iterative framework uses lightweight SFT (3K examples) to highlight reasoning actions, followed by curriculum RL with GRPO to refine and generalize these capabilities.", "key_contribution": "The main contribution is a simple yet effective iterative SFT-RL training framework that successfully brings R1-style reasoning to multimodal contexts, demonstrating that SFT acts as an inductive prior highlighting reasoning actions while RL provides exploration and generalization, with each iteration producing progressively better training data for self-improvement.", "novelty": "Unlike prior work that either distills from text-only reasoning models (leading to performance drops) or relies solely on RL (which struggles to surface reflective behaviors in smaller models), this work discovers that alternating between SFT and RL creates a synergistic effect. The paper provides novel analysis showing SFT surfaces specific reasoning tokens that guide RL exploration, and introduces a curriculum RL approach based on data source difficulty. This achieves superior results with 1/10th the data compared to concurrent single-iteration approaches.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models"], "score": 70.66, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4251426, "title": "Eliciting Reasoning in Language Models with Cognitive Tools", "authors": "Brown Ebouky, Andrea Bartezzaghi, Mattia Rigotti", "pdf_url": "https://openreview.net/pdf/ea576c6461243d27a8102c9dfa554db950d66757.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52741, "poster_number": 302, "tag": "SD-1-302 | Exhibit Hall C,D,E ", "relevance_score": 68.01, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that implementing cognitive operations as modular 'cognitive tools' within an agentic framework significantly improves LLM reasoning performance without requiring reinforcement learning. GPT-4.1 with cognitive tools achieves 53% accuracy on AIME2024, surpassing o1-preview's 44.6%, suggesting that structured modular prompting can elicit latent reasoning capabilities from base models. The approach consistently outperforms both baseline models and monolithic cognitive prompting across multiple mathematical reasoning benchmarks.", "description": "The paper introduces a cognitive tools framework that encapsulates specific reasoning operations (understand question, recall related, examine answer, backtracking) as modular, prompt-driven functions within a tool-calling architecture. Unlike traditional agentic tools that interface with external APIs, these cognitive tools modularize the LLM's internal reasoning processes, with each tool executed by the LLM itself in isolation. The approach is evaluated on mathematical reasoning benchmarks (AIME2024, MATH500, AMC) using both open-weight models (Qwen, Llama) and closed models (GPT-4.1).", "key_contribution": "The main contribution is demonstrating that modular cognitive operations, implemented as discrete tools rather than monolithic prompts, can effectively elicit reasoning capabilities in LLMs without reinforcement learning post-training. This provides an alternative mechanism to RL for surfacing latent reasoning abilities already present in base models.", "novelty": "Unlike cognitive prompting which uses predetermined monolithic prompts, this work implements cognitive operations as modular, compartmentalized tools that reduce interference between reasoning steps and allow flexible orchestration. The approach addresses limitations of flat prompting by enabling the LLM to autonomously select which cognitive operations to invoke and when, rather than following a fixed sequence. This bridges insights from cognitive architectures with modern agentic AI frameworks, treating internal reasoning operations as discrete tools rather than external APIs.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation", "Compositional and Counterfactual Reasoning"], "score": 68.01, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4442508, "title": "Improve Temporal Reasoning in Multimodal Large Language Models via Video Contrastive Decoding", "authors": "Daiqing Qi, Dongliang Guo, Hanzhang Yuan, Handong Zhao, Mengxuan Hu, Lehan Yang, Sheng Li", "pdf_url": "https://openreview.net/pdf/1494d847179a71c38929ca5d87774036b701f164.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52407, "poster_number": 4705, "tag": "SD-1-4705 | Exhibit Hall C,D,E ", "relevance_score": 61.8, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper identifies that Video LLMs struggle with temporal reasoning due to language priors (from pre-training) and novel 'image' priors (static visual features in individual frames that mislead temporal perception). The proposed Video Contrastive Decoding with Temporal Distortion achieves consistent improvements across temporal-specific benchmarks (TempCompass, EventHallusion) and general video understanding benchmarks (Video-MME, MLVU) without requiring any additional training.", "description": "The paper proposes a training-free method to improve temporal reasoning in Video Large Language Models by introducing Video Temporal Distortion during contrastive decoding. The approach adaptively distorts key frames to remove temporal information while preserving misleading static visual context, generating time-insensitive wrong responses that serve as contrastive signals to guide the model away from such errors during final output generation.", "key_contribution": "A novel plug-and-play Video Contrastive Decoding framework with an adaptive Temporal Distortion Unit that improves temporal reasoning in Video LLMs at inference time without any additional training, by explicitly avoiding temporally insensitive responses through contrastive decoding guided by attention-based frame importance estimation.", "novelty": "Unlike existing works that require expensive model modifications, training data, or architecture changes to improve temporal reasoning, this work introduces a post-hoc, model-agnostic solution that operates purely at the decoding stage. It uniquely identifies and leverages both language priors and 'image' priors as sources of temporal reasoning failures, and proposes an adaptive distortion strategy that balances removing temporal cues while maintaining confounding visual context to consistently induce useful negative responses for contrastive decoding.", "ai_categories": ["Video Understanding and Temporal Reasoning", "Vision-Language Reasoning and Chain-of-Thought", "Hallucination Detection and Model Robustness"], "score": 61.8, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4291822, "title": "Surprise3D: A Dataset for Spatial Understanding and Reasoning in Complex 3D Scenes", "authors": "Jiaxin Huang, Ziwen Li, Hanlue Zhang, Runnan Chen, Zhengqing Gao, Xiao He, Yandong Guo, Wenping Wang, Tongliang Liu, Mingming Gong", "pdf_url": "https://arxiv.org/pdf/2507.07781", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52742, "poster_number": 5200, "tag": "SD-1-5200 | Exhibit Hall C,D,E ", "relevance_score": 60.01, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "SURPRISE3D introduces the first large-scale benchmark for spatial reasoning segmentation in 3D scenes with 200K+ vision-language pairs across 900+ indoor scenes, deliberately excluding object names to prevent semantic shortcuts. Experiments reveal that state-of-the-art 3D vision-language models perform significantly worse when deprived of explicit object references, demonstrating their reliance on semantic cues rather than genuine spatial understanding. The dataset includes 89K+ human-annotated spatial queries covering narrative perspective, parametric perspective, relative position, and absolute distance reasoning.", "description": "This paper presents SURPRISE3D, a dataset and benchmark for evaluating 3D Spatial Reasoning Segmentation (3D-SRS) in complex indoor scenes. The dataset addresses limitations in existing 3D vision-language benchmarks by providing queries that require genuine spatial understanding without relying on object names or semantic shortcuts. It combines human-annotated spatial reasoning queries with LLM-generated common sense and human intention queries, all validated through rigorous quality control processes.", "key_contribution": "The main contribution is the SURPRISE3D dataset with 200K+ query-mask pairs and the 3D-SRS benchmark that evaluates spatial reasoning capabilities independent of semantic recognition. The dataset uniquely features shortcut-free queries that force models to rely on spatial understanding rather than object category matching, revealing significant performance gaps in current state-of-the-art models.", "novelty": "Unlike existing datasets that mix semantic cues with spatial context, SURPRISE3D deliberately excludes object names from queries, preventing models from exploiting shortcuts through category detection. The paper introduces fine-grained spatial reasoning categories (narrative perspective, parametric perspective, relative position, absolute distance) that have not been systematically evaluated before. The human-in-the-loop annotation process ensures high-quality spatial queries that LLMs alone cannot generate with sufficient fidelity.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Multimodal Benchmarks and Evaluation", "Compositional and Counterfactual Reasoning"], "score": 60.01, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4303553, "title": "VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs", "authors": "Shmuel Berman, Jia Deng", "pdf_url": "https://openreview.net/pdf/ca79a743ac55c7fce5e100be3b2d695bf84a71ba.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52243, "poster_number": 4911, "tag": "SD-1-4911 | Exhibit Hall C,D,E ", "relevance_score": 69.82, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Leading VLMs (GPT-5, Gemini 2.5 Pro, Claude Sonnet 4) fail at basic nonlocal visual reasoning tasks that are trivial for humans, barely exceeding random accuracy on some variants. The paper reveals that despite improvements in visual acuity benchmarks, current models lack core visual reasoning capabilities including comparative perception, saccadic search, and smooth visual search. Models selectively engage their perceptual abilities and struggle most with tasks requiring continuous contour tracing that resist natural language reformulation.", "description": "This paper presents a systematic evaluation of Vision-Language Models' capacity for nonlocal visual reasoning‚Äîreasoning that requires chaining evidence from multiple, possibly distant regions of an image. The authors introduce three procedurally-generated task categories (Object Re-Identification, Visual Scavenger Hunt, and Circuit Connections) designed to isolate and test three distinct forms of nonlocal vision: comparative perception, saccadic search, and smooth visual search. Through controlled variants of each task, the study determines when and how VLMs engage their visual processing capabilities.", "key_contribution": "The paper introduces a novel evaluation suite with three procedural generators that systematically isolate and test core nonlocal visual reasoning capabilities in VLMs. This structured approach enables precise diagnosis of whether failures stem from perceptual limitations versus reasoning deficits, revealing that models lack fundamental visual algorithms despite high performance on complex benchmarks.", "novelty": "Unlike existing benchmarks that test either high-level abstract reasoning or adversarial perception, this work isolates primitive visual algorithms by creating tasks that are trivial for humans but require minimal prior knowledge. The evaluation distinguishes between perceptual failures and reasoning failures through carefully designed variants, revealing that VLMs selectively engage visual processing and struggle with continuous operations that cannot be easily decomposed into natural language steps. This addresses the gap between VLMs' strong performance on complex benchmarks and their inability to perform basic visual reasoning.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Visual Grounding and Spatial Reasoning", "Hallucination Detection and Model Robustness"], "score": 69.82, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4441657, "title": "Evaluating the Inductive Abilities of Large Language Models: Why Chain-of-Thought Reasoning Sometimes Hurts More Than Helps", "authors": "Haibo Jin, Peiyan Zhang, Man Luo, Haohan Wang", "pdf_url": "https://openreview.net/pdf/df72ac5f5d065191323bb112c1280014c74a4ab1.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52470, "poster_number": 1414, "tag": "SD-1-1414 | Exhibit Hall C,D,E ", "relevance_score": 59.96, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Chain-of-thought (CoT) reasoning in Large Reasoning Models (LRMs) can actually degrade inductive performance compared to non-reasoning LLMs, with LRMs often underperforming by 20-40% on hidden rule inference tasks. The paper identifies three failure modes‚Äîincorrect sub-task decomposition, incorrect sub-task solving, and incorrect final answer summarization‚Äîthat explain why deeper reasoning can amplify errors rather than reduce them. Structured interventions targeting these failure modes improve inductive accuracy by up to 49% without model retraining.", "description": "This paper evaluates the inductive reasoning abilities of LLMs and LRMs using four controlled game-based tasks (chess, Texas Hold'em, dice games, blackjack) with hidden human-defined rules. The authors develop a theoretical framework modeling CoT reasoning as a belief update process with error propagation, revealing how reasoning steps can amplify errors through misaligned decomposition, noisy solving, and suboptimal stopping. They propose targeted interventions‚Äîstructured decomposition templates, guided solving with examples, and token budget constraints‚Äîthat address each failure mode.", "key_contribution": "The paper provides both theoretical and empirical evidence that chain-of-thought reasoning can hurt inductive performance, introduces a formal framework explaining error propagation through three distinct failure modes, and demonstrates that structured interventions can significantly improve reasoning accuracy without retraining.", "novelty": "Unlike prior work that assumes CoT reasoning is universally beneficial, this paper demonstrates when and why it fails, particularly for inductive tasks with hidden rules. The work goes beyond empirical observation to provide a theoretical model of error propagation in multi-step reasoning, decomposing failures into alignment errors, solving noise, and summarization errors. The proposed interventions are theory-guided and address specific failure modes rather than applying generic prompting strategies.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "In-Context Learning and Few-Shot Adaptation", "Prompt Learning and Optimization"], "score": 59.96, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4070984, "title": "CAPability: A Comprehensive Visual Caption Benchmark for Evaluating Both Correctness and Thoroughness", "authors": "Zhihang Liu, Chen-Wei Xie, Bin Wen, Feiwu Yu, JixuanChen, Pandeng Li, Boqiang Zhang, Nianzu Yang, YingluLi, ..., Hongtao Xie", "pdf_url": "https://arxiv.org/pdf/2502.14914", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52463, "poster_number": 4708, "tag": "SD-1-4708 | Exhibit Hall C,D,E ", "relevance_score": 62.91, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "CAPability introduces the first comprehensive visual captioning benchmark evaluating both correctness (precision) and thoroughness (hit) across 12 dimensions and 6 views. The benchmark reveals significant performance gaps between models, with GPT-4o achieving the best precision (79.2%) and Gemini-1.5-pro the best hit (60.4%), and identifies a 'know but cannot tell' (KÃÑT) phenomenon where models can answer questions correctly but fail to include the same information in captions automatically. The evaluation shows models struggle particularly with object counting, camera angles, character identification, and action recognition.", "description": "This paper presents CAPability, a comprehensive visual captioning benchmark with nearly 11K human-annotated images and videos evaluated across 12 dimensions (9 static, 4 dynamic) spanning object-related, global-related, text-related, camera-related, temporal-related, and knowledge-related views. Unlike traditional benchmarks that compare caption sentences directly, CAPability annotates specific visual elements and evaluates both correctness through precision metrics and thoroughness through hit metrics, while also converting annotations to QA format to measure the gap between QA and captioning capabilities.", "key_contribution": "The main contribution is a multi-view benchmark that evaluates visual captioning comprehensively across 12 dimensions with dual metrics (precision and hit) for assessing both correctness and thoroughness, along with the introduction of the KÃÑT metric that reveals the performance gap between question-answering and automatic captioning capabilities in MLLMs.", "novelty": "Unlike previous benchmarks that rely on vague sentence-level comparisons or focus on single aspects like objects, CAPability provides the first comprehensive multi-view evaluation framework with element-level annotations across 12 dimensions. It addresses limitations of existing work by evaluating both correctness and thoroughness rather than just accuracy, and introduces the 'One Represents All' annotation strategy to ensure unbiased statistical coverage. The benchmark also uniquely identifies the 'know but cannot tell' phenomenon, showing that models often possess knowledge but fail to express it automatically in captions.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought"], "score": 62.91, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4205669, "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "authors": "Liyan Tang, Grace Kim, Xinyu Zhao, Thom Lake, Wenxuan Ding, Fangcong Yin, Prasann Singhal, Manya Wadhwa, Zeyu Liu, ..., Greg Durrett", "pdf_url": "https://arxiv.org/pdf/2505.13444", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52957, "poster_number": 4709, "tag": "SD-1-4709 | Exhibit Hall C,D,E ", "relevance_score": 63.72, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Current large vision-language models (LVLMs) exhibit severe limitations in visual reasoning for chart understanding, with the best model (Gemini-2.5-Pro) achieving only 63% accuracy compared to 93% human performance. Models show a 35-55% performance drop on visual reasoning questions compared to text-heavy questions, revealing a fundamental imbalance between textual and visual reasoning capabilities. Existing chart understanding benchmarks over-represent textual reasoning, with 74% of ChartQA questions solvable through text extraction alone.", "description": "This paper introduces ChartMuseum, a comprehensive chart question-answering benchmark containing 1,162 expert-annotated questions from 928 real-world charts across 184 sources. The benchmark is specifically designed to evaluate complex visual and textual reasoning capabilities of LVLMs, with questions classified into four reasoning types: textual, visual, visual/textual, and synthesis. The authors conduct extensive evaluation of 21 state-of-the-art models and provide detailed error analysis identifying specific visual reasoning task categories where models struggle.", "key_contribution": "ChartMuseum provides the first chart understanding benchmark that effectively differentiates model capabilities and exposes substantial gaps between model and human performance, specifically targeting visual reasoning skills that existing benchmarks fail to adequately test. The benchmark includes a taxonomy of visual reasoning tasks and demonstrates that current LVLMs over-rely on textual reasoning strategies even when visual reasoning would be more appropriate.", "novelty": "Unlike prior chart QA benchmarks that use model-generated questions or allow approximate answers, ChartMuseum features entirely human-written questions with objectively verifiable answers and requires non-trivial visual reasoning. The paper provides novel analysis showing existing benchmarks can be largely solved through text extraction (74% accuracy on ChartQA), and introduces a synthetic dataset demonstrating that model visual reasoning degrades with complexity while human performance remains robust. The work establishes a clear taxonomy distinguishing visual reasoning (symbol selection, visual comparison, trajectory tracking, X/Y value identification) from textual reasoning in chart understanding.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought", "Visual Grounding and Spatial Reasoning"], "score": 63.72, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4276950, "title": "Reasoning as an Adaptive Defense for Safety", "authors": "Taeyoun Kim, Fahim Tajwar, Aditi Raghunathan, Aviral Kumar", "pdf_url": "https://openreview.net/pdf/c036ebd40ea82ab94b9badc0028faf7e94456ca1.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52246, "poster_number": 405, "tag": "SD-1-405 | Exhibit Hall C,D,E ", "relevance_score": 61.12, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "TARS (Training Adaptive Reasoners for Safety) demonstrates that training LLMs to reason about safety via reinforcement learning achieves superior safety-refusal trade-offs compared to existing methods, including circuit breakers and supervised fine-tuning approaches. Models trained with TARS adaptively allocate more test-time compute to ambiguous prompts, exhibit better internal separation between harmful and harmless prompts, and show greater robustness to both white-box (GCG) and black-box (PAIR) attacks. The approach achieves state-of-the-art performance with 6.6√ó fewer parameters than comparable 8B-sized models.", "description": "This paper presents TARS, a systematic recipe for training large language models to reason adaptively about safety on a per-prompt basis using reinforcement learning with long chain-of-thought traces. The approach involves three key stages: lightweight supervised fine-tuning on exploratory reasoning traces, mixing harmful, harmless, and ambiguous prompts during RL training, and using separate reward functions for safety and task completion to prevent degenerate refusal strategies. The method enables models to spend more compute on complex or ambiguous queries while maintaining both safety and helpfulness.", "key_contribution": "The main contribution is a complete training recipe (TARS) that combines lightweight SFT warmup, strategic prompt mixing (harmful, harmless, and ambiguous), and separate reward functions to train reasoning models that adaptively defend against safety vulnerabilities while avoiding over-refusal. This represents the first systematic open study of best practices for training LLMs to reason about safety through RL.", "novelty": "Unlike prior work that uses supervised fine-tuning on curated reasoning traces or proprietary RL methods with undisclosed details, TARS provides an open, systematic recipe identifying three critical design choices for effective safety reasoning. The work demonstrates that both RL and reasoning are essential‚ÄîRL without reasoning or reasoning via SFT alone achieve inferior safety-refusal trade-offs. TARS addresses the key challenge that na√Øve RL on safety leads to reward hacking and degenerate refusal strategies by mixing prompt types and splitting reward functions, enabling adaptive behavior that previous approaches could not achieve.", "ai_categories": ["Reinforcement Learning for Multimodal Models", "Hallucination Detection and Model Robustness"], "score": 61.12, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4433932, "title": "NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation", "authors": "Longtian Qiu, Shan Ning, Jiaxuan Sun, Xuming He", "pdf_url": "https://openreview.net/pdf/96f8ac8e6e7af57f7d1898ceced0a9d165735c5e.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52728, "poster_number": 5007, "tag": "SD-1-5007 | Exhibit Hall C,D,E ", "relevance_score": 59.92, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "NoisyGRPO introduces a systematic multimodal RL framework that significantly improves Chain-of-Thought reasoning in MLLMs through noise injection and Bayesian advantage estimation. The method achieves +4.4 improvement on MME-CoT and +3.7 on MMStar with small-scale models (3B parameters), demonstrating superior generalization beyond training distribution compared to vanilla GRPO. The framework maintains computational efficiency while producing more concise and accurate reasoning paths.", "description": "This paper presents NoisyGRPO, a reinforcement learning framework for enhancing multimodal Chain-of-Thought reasoning in vision-language models. The approach injects controllable Gaussian noise into visual inputs during rollout generation to encourage diverse exploration, then uses Bayesian inference to estimate trajectory advantages by combining noise level priors with observed rewards. The method addresses GRPO's limitations in generalizing beyond training data by promoting thorough exploration while ensuring robust policy optimization through principled advantage estimation.", "key_contribution": "The main contribution is a unified RL framework that combines (1) a noise-injected exploration policy that perturbs visual inputs to diversify rollouts and implicitly favor visually grounded reasoning, and (2) a Bayesian advantage estimation method that treats injected noise as a prior and trajectory reward as likelihood to compute robust posterior advantage estimates, enabling accurate policy updates despite distribution shift.", "novelty": "Unlike vanilla GRPO which relies solely on temperature sampling and suffers from limited exploration and missing process supervision, NoisyGRPO systematically addresses these issues through structured visual noise injection. The key innovation is formulating advantage estimation as Bayesian inference that adaptively fuses noise magnitude and reward signals, with prior uncertainty modeled as a function of reward variance within trajectory groups. This principled approach enables the model to handle off-policy scenarios and learn visually grounded reasoning without additional computational overhead.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Hallucination Detection and Model Robustness"], "score": 59.92, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4442752, "title": "Counterfactual Evolution of Multimodal Datasets via Visual Programming", "authors": "Minghe Gao, Zhongqi Yue, Wenjie Yan, Yihao Hu, Wei Ji, Siliang Tang, Jun Xiao, Tat-Seng Chua, Yueting Zhuang, Juncheng Li", "pdf_url": "https://openreview.net/pdf/1c64e34216940d5fb89d4399163b323815db7475.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52727, "poster_number": 4910, "tag": "SD-1-4910 | Exhibit Hall C,D,E ", "relevance_score": 58.77, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "SCOPE introduces a novel framework that uses symbolic Visual Programming to enable verifiable, controllable, and scalable evolution of multimodal datasets through counterfactual reasoning. The framework systematically expands datasets along three dimensions‚Äîreasoning path, visual context, and cross-instance composition‚Äîwhile maintaining explicit traceability. Experiments demonstrate that SCOPE-trained models show improved reasoning performance, better generalization from easy-to-hard tasks, and enhanced visual dialog capabilities across multiple model scales.", "description": "This paper presents SCOPE (Scalable COunterfactual Program Evolution), a framework that addresses the bottleneck of dataset evolution in multimodal learning by converting vision-language tasks into executable Python programs. The framework performs counterfactual inference through three stages: abduction (generating verifiable programs), action (intervening on program structure), and prediction (categorizing evolved instances by difficulty, structure, and input multiplicity). The authors construct SCOPE-Train and SCOPE-Test benchmarks and propose MAP, a curriculum learning strategy that aligns model training with structured difficulty progression.", "key_contribution": "The main contribution is a program-centric dataset evolution framework that enables verifiable transformations, controllable difficulty enhancement, and diverse sample generation through symbolic Visual Programming, along with continuously evolving benchmarks (SCOPE-Train/Test) and a curriculum learning strategy (MAP) that aligns training with structured sample difficulty.", "novelty": "Unlike existing methods that rely on prompt-based transformations (model-dependent, unverifiable), template-based augmentation (limited diversity, no difficulty increase), or decomposition-based extraction (loss of global context), SCOPE uses executable programs as symbolic representations to enable explicit reasoning chain construction and systematic counterfactual interventions. This approach provides verifiability through code execution, controllability through structured program manipulation across three axes, and scalability through iterative evolution‚Äîaddressing fundamental limitations of static datasets and heuristic augmentation methods.", "ai_categories": ["Compositional and Counterfactual Reasoning", "Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought"], "score": 58.77, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4444932, "title": "MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models", "authors": "Zimeng Huang, Jinxin Ke, Xiaoxuan Fan, Yufeng Yang, Yang Liu, Liu Zhonghan, Zedi Wang, Junteng Dai, Haoyi Jiang, ..., Ziliang Chen", "pdf_url": "https://arxiv.org/pdf/2510.26937", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52404, "poster_number": 4612, "tag": "SD-1-4612 | Exhibit Hall C,D,E ", "relevance_score": 55.26, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Current state-of-the-art LVLMs fall significantly short of human performance in open-ended association reasoning, with top models achieving only 19.86% HR-4 on RIA tasks compared to humans' 22.84%. Models demonstrate a critical 'distinctiveness gap' where they can establish plausible connections but struggle with clear conceptualization and deep knowledge integration, often resorting to overgeneralization rather than precise associative reasoning.", "description": "This paper introduces MM-OPERA, a comprehensive benchmark with 11,497 instances designed to evaluate association reasoning in Large Vision-Language Models through two open-ended tasks: Remote-Item Association (RIA) and In-Context Association (ICA). The benchmark employs tailored LLM-as-a-Judge strategies including process-reward evaluation to assess both response quality and reasoning paths across 13 ability dimensions, multiple cultures, languages, and domains.", "key_contribution": "The paper presents the first systematic open-ended benchmark for evaluating associative reasoning in LVLMs, introducing novel process-reward LLM-as-a-Judge methods that assess step-by-step reasoning quality through Reasonableness, Distinctiveness, and Knowledgeability metrics, moving beyond traditional outcome-focused evaluation.", "novelty": "Unlike existing closed-ended benchmarks like Labyrinth of Links that use predefined options, MM-OPERA evaluates open-ended association reasoning without constraints, enabling assessment of both convergent and divergent thinking. It introduces process-reward evaluation that traces multi-hop reasoning paths rather than just final answers, and provides comprehensive analysis across cultural, linguistic, and domain boundaries that previous benchmarks lack.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Multimodal Benchmarks and Evaluation", "Compositional and Counterfactual Reasoning"], "score": 55.26, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4210468, "title": "Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering", "authors": "Kuicai Dong, CHANG YUJING, Shijie Huang, Yasheng Wang, Ruiming Tang, Yong Liu", "pdf_url": "https://arxiv.org/pdf/2505.16470", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52467, "poster_number": 4718, "tag": "SD-1-4718 | Exhibit Hall C,D,E ", "relevance_score": 58.33, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "MMDocRAG reveals that even state-of-the-art models struggle with multimodal document question answering, with GPT-4.1 achieving only 70.2% F1 on quote selection and 4.14/5 on answer quality. Advanced proprietary VLMs show moderate advantages using multimodal inputs over text-only inputs, while open-source models exhibit significant performance degradation with multimodal inputs. Fine-tuning on the development set yields substantial improvements across all model sizes.", "description": "This paper introduces MMDocRAG, a comprehensive benchmark for evaluating retrieval-augmented multimodal generation in document question answering. The benchmark contains 4,055 expert-annotated QA pairs with multi-page, cross-modal evidence chains and supports answers that interleave text with visual elements. Through experiments with 60 VLM/LLM models and 14 retrieval systems, the work identifies persistent challenges in multimodal evidence retrieval, selection, and integration.", "key_contribution": "The main contribution is a rigorous benchmark that evaluates both multimodal quote selection (with hard negatives) and interleaved text-image answer generation, introducing novel metrics for assessing evidence selection accuracy and multimodal integration quality. This enables comprehensive evaluation of the complete RAG pipeline from retrieval to generation in document understanding tasks.", "novelty": "Unlike existing DocVQA benchmarks that focus on text-only answers or end-to-end generation, MMDocRAG uniquely evaluates models' ability to select relevant multimodal evidence from noisy contexts and generate answers with properly cited and integrated visual content. It addresses the limitation of current benchmarks by providing explicit annotations for multimodal evidence chains, hard negative quotes for robust evaluation, and metrics specifically designed for assessing citation quality and text-image coherence in RAG settings.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought", "Hallucination Detection and Model Robustness"], "score": 58.33, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4442401, "title": "Reliable Lifelong Multimodal Editing: Conflict-Aware Retrieval Meets Multi-Level Guidance", "authors": "Qiang Zhang, Fanrui Zhang, Jiawei Liu, Ming Hu, Junjun He, Zheng-Jun Zha", "pdf_url": "https://openreview.net/pdf/e9c3d5a5e890cfc52d58bb0f8c853abe590119fe.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52338, "poster_number": 4902, "tag": "SD-1-4902 | Exhibit Hall C,D,E ", "relevance_score": 54.65, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "CARML achieves near-perfect performance (97-100% accuracy) in lifelong multimodal knowledge editing even after 1000 sequential edits, significantly outperforming existing methods. The framework introduces a conflict-aware dynamic retrieval mechanism that quantifies intra-modal uncertainty and inter-modal conflicts to precisely locate relevant knowledge from an expanding knowledge base. The multi-level guidance strategy combining implicit prompts and explicit logits enhancement enables reliable editing while preserving model locality.", "description": "This paper presents CARML, a retrieval-augmented framework for reliable lifelong multimodal knowledge editing in MLLMs. The approach constructs multi-type knowledge bases and uses conflict-aware dynamic retrieval to accurately match edit samples with relevant knowledge across visual, textual, and multimodal channels. An edit scope classifier determines whether edits should be applied, while multi-level guidance (static/dynamic prompts and logits enhancement) steers the model to generate desired outputs without modifying core parameters.", "key_contribution": "The main contribution is a novel retrieval-augmented editing framework that integrates conflict-aware dynamic retrieval with multi-level knowledge guidance (implicit continuous prompts and explicit logits enhancement) to achieve reliable, generalizable, and localized lifelong multimodal knowledge editing without catastrophic forgetting.", "novelty": "Unlike existing methods that struggle with accurate knowledge retrieval as the knowledge base grows and lack multi-level guidance, CARML quantifies both intra-modal uncertainty and inter-modal conflicts to dynamically fuse multi-channel retrieval information. It introduces context-aware dynamic prompts that capture fine-grained cross-modal associations at the token level, and combines implicit guidance with explicit output logits enhancement. The edit scope classifier in a learned semantic space ensures locality by filtering out-of-scope samples, addressing the key limitations of catastrophic forgetting and imprecise knowledge matching in lifelong editing scenarios.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Hallucination Detection and Model Robustness", "In-Context Learning and Few-Shot Adaptation"], "score": 54.65, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4227753, "title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning", "authors": "Zhongwei Wan, Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, ..., Shen Yan", "pdf_url": "https://openreview.net/pdf/514851b938f3c4ab888ed4499926b37fdbb1b89c.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52615, "poster_number": 4903, "tag": "SD-1-4903 | Exhibit Hall C,D,E ", "relevance_score": 55.55, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "SRPO demonstrates that explicitly training multimodal LLMs to self-reflect and self-correct through both supervised fine-tuning and reinforcement learning significantly improves reasoning performance, achieving state-of-the-art results on multiple benchmarks (e.g., 75.8% on MathVista for 7B model). The work shows that reflection-aware rewards during RL training are critical for teaching models to generate meaningful, concise reflections that genuinely improve reasoning rather than superficial verbose outputs. The approach successfully transfers reflective capabilities from large teacher models to smaller policy models, enabling them to surpass limitations established during pre-training.", "description": "This paper introduces SRPO, a two-stage reflection-aware reinforcement learning framework for enhancing multimodal reasoning in vision-language models. The first stage constructs high-quality reflection-focused training data using advanced MLLMs to generate reflections on initial responses, followed by supervised fine-tuning. The second stage employs Group Relative Policy Optimization (GRPO) with a novel reward function that explicitly incentivizes effective, concise reflection while penalizing redundancy and rewarding genuine reasoning improvements.", "key_contribution": "The main contribution is a novel two-stage training framework that combines reflection-oriented supervised fine-tuning with reflection-aware reinforcement learning, featuring a specially designed reward mechanism that evaluates reflection effectiveness (whether it corrects errors or maintains correct answers) and encourages appropriate response length to prevent verbosity while promoting meaningful self-correction.", "novelty": "Unlike prior work that focuses solely on enhancing reasoning through extended chain-of-thought supervision or applies RL without explicit reflection mechanisms, SRPO explicitly teaches models to self-reflect and self-correct during both SFT and RL phases. The work addresses the limitation that existing RL methods struggle with redundant or incorrect reasoning steps by introducing reflection-specific rewards that distinguish between helpful reflections (correcting wrong answers, refining correct ones) and harmful ones (introducing errors or unnecessary verbosity). This enables models to surpass cognitive boundaries established during pre-training through external knowledge injection via reflection.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models"], "score": 55.55, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4227522, "title": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and Generation via Reinforcement Learning", "authors": "Kaihang Pan, Yang Wu, Wendong Bu, Kai Shen, Juncheng Li, Yingting Wang, liyunfei, Siliang Tang, Jun Xiao, ..., Yueting Zhuang", "pdf_url": "https://openreview.net/pdf/4aad34acb1fe54ce3d709604ec7d81ff8ce11b11.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52697, "poster_number": 5401, "tag": "SD-1-5401 | Exhibit Hall C,D,E ", "relevance_score": 54.7, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that visual comprehension and generation capabilities in MLLMs can be synergistically combined through a two-stage training approach (SFT + RL) to create genuine chain-of-thought reasoning for image generation. The model achieves state-of-the-art performance on text-to-image benchmarks, unlocks 'Aha moments' where it can self-correct generated images, and extends to unified image generation tasks including image editing. Notably, the trained model also emerges as a superior image semantic evaluator, showing that RL enables true collaborative co-evolution of comprehension and generation.", "description": "The paper introduces Janus-Pro-R1, a multimodal large language model that advances visual generation from simple text-to-image tasks to an iterative introspective process. Using supervised fine-tuning to teach foundational CoT abilities and reinforcement learning (GRPO) to unlock full potential through exploration-exploitation trade-off, the model learns to generate images, self-evaluate them, and regenerate when needed. The approach uses a bi-level QA-based reward function that incentivizes both generation quality and comprehension accuracy.", "key_contribution": "The main contribution is enabling genuine collaboration between visual comprehension and generation in MLLMs through a two-stage training paradigm that produces self-driven chain-of-thought reasoning for image generation, unlocking Aha moments where the model can introspectively correct its outputs and extending capabilities to unified image generation tasks.", "novelty": "Unlike existing MLLMs where comprehension and generation remain independent, this work creates true synergy between these capabilities where they mutually enhance each other. Previous CoT approaches for visual generation relied on forced textual planning, while this work enables spontaneous, self-driven reasoning chains that emerge from the model's deep thinking. The RL stage is critical for moving from imitation (SFT) to genuine reasoning and generalization, particularly for counterfactual generation.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Multimodal Benchmarks and Evaluation"], "score": 54.7, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4256007, "title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective", "authors": "Jorge (Zhoujun) Cheng, Shibo Hao, Tianyang Liu, Fan Zhou, Yutao Xie, Feng Yao, Yuexin Bian, Nilabjo Dey, Yonghao Zhuang, ..., Zhiting Hu", "pdf_url": "https://arxiv.org/pdf/2506.14965", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52530, "poster_number": 107, "tag": "SD-1-107 | Exhibit Hall C,D,E ", "relevance_score": 53.16, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "RL's effectiveness for reasoning is highly domain-dependent: domains with strong pretraining coverage (Math, Code, Science) benefit from cross-domain RL training, while underrepresented domains (Logic, Simulation, Tabular) require in-domain training, suggesting RL facilitates genuine skill acquisition in less familiar domains. The paper introduces GURU, a 92K curated multi-domain RL dataset spanning six reasoning domains, and demonstrates that GURU-7B/32B models achieve state-of-the-art performance among open models trained with publicly available RL data, outperforming baselines by 7.9% and 6.7% respectively.", "description": "This paper systematically investigates reinforcement learning for LLM reasoning across six diverse domains (Math, Code, Science, Logic, Simulation, Tabular) rather than focusing solely on math and code. The authors curate GURU, a high-quality RL corpus with domain-specific reward designs and filtering, and conduct extensive cross-domain transfer experiments to understand how RL mechanisms vary across domains. They train GURU-7B and GURU-32B models on mixed-domain data and evaluate on 17 benchmarks.", "key_contribution": "The main contribution is GURU, a curated 92K multi-domain RL reasoning dataset with verifiable rewards across six domains, along with systematic empirical findings showing that RL's role (elicitation vs. skill acquisition) is domain-dependent based on pretraining exposure. The paper also releases state-of-the-art general reasoning models (GURU-7B/32B) trained exclusively on open data.", "novelty": "Unlike prior work focusing narrowly on math and code domains, this paper provides the first systematic cross-domain analysis of RL for reasoning, revealing that established findings about RL primarily eliciting existing knowledge do not generalize across all domains. It addresses the limitation of domain-specific RL training by demonstrating that underrepresented domains require in-domain data for meaningful gains, challenging the view that RL only amplifies pretrained capabilities. The work introduces careful domain-specific reward design and difficulty filtering strategies for diverse reasoning tasks.", "ai_categories": ["Reinforcement Learning for Multimodal Models", "Vision-Language Reasoning and Chain-of-Thought", "Multimodal Benchmarks and Evaluation"], "score": 53.16, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4426614, "title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation", "authors": "Jiani Huang, Amish Sethi, Matthew Kuo, Mayank Keoliya, Neelay Velingker, JungHo Jung, Ser-Nam Lim, Ziyang Li, Mayur Naik", "pdf_url": "https://openreview.net/pdf/32e384d1b7bff0817b05af22b3999df7789e5b8d.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52529, "poster_number": 4908, "tag": "SD-1-4908 | Exhibit Hall C,D,E ", "relevance_score": 54.25, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "ESCA demonstrates that augmenting MLLMs with structured scene graphs significantly reduces perception errors in embodied agents (from 69% to 30%), enabling open-source models to surpass proprietary baselines. SGClip, trained on 87K+ videos using model-driven self-supervision, achieves state-of-the-art performance on scene graph generation and action recognition benchmarks while requiring no human annotations. The framework achieves consistent improvements across four diverse embodied environments with both open-source and commercial MLLMs.", "description": "This paper introduces ESCA, a framework that enhances MLLM-based embodied agents by grounding their perception in spatial-temporal scene graphs. At its core is SGClip, a CLIP-based foundation model trained using a neurosymbolic pipeline on the ESCA-Video-87K dataset, which aligns automatically generated captions with scene graphs. The framework uses a transfer protocol with selective concept extraction to contextualize agents with relevant scene information for improved perception, reasoning, and planning.", "key_contribution": "The main contribution is ESCA, a general framework for contextualizing embodied agents through selective scene graph generation, along with SGClip‚Äîa promptable, open-domain scene graph model trained via neurosymbolic learning without human annotations. The work also introduces ESCA-Video-87K, an MLLM-annotated dataset with object traces and programmatic specifications.", "novelty": "Unlike prior work that relies on object detection models (YOLO, Grounding DINO) which miss semantic attributes and relationships, ESCA provides structured scene graphs capturing entities, attributes, and spatial-temporal relations. The neurosymbolic training approach eliminates the need for costly human annotations by using model-driven self-supervision with programmatic specifications. The selective grounding mechanism allows MLLMs to focus on task-relevant concepts rather than injecting full scene graphs, which can degrade performance.", "ai_categories": ["Embodied AI and Vision-Language-Action Models", "Visual Grounding and Spatial Reasoning", "Vision-Language Reasoning and Chain-of-Thought"], "score": 54.25, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4246547, "title": "MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning", "authors": "Yuxuan Luo, Ryan Yuan, Junwen Chen, Haonan Cai, Ziyi Yue, Yuwei Yang, Fatima Zohra Daha, Ji Li, Zhouhui Lian", "pdf_url": "https://arxiv.org/pdf/2506.10963", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52710, "poster_number": 4711, "tag": "SD-1-4711 | Exhibit Hall C,D,E ", "relevance_score": 62.42, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces MMMG, a benchmark with 4,456 expert-validated knowledge image-prompt pairs spanning 10 disciplines and 6 educational levels, revealing that even GPT-4o achieves only 50.20 MMMG-Score. The benchmark exposes serious reasoning deficits in current text-to-image models including low entity fidelity, weak relational understanding, and visual clutter. The paper also releases FLUX-Reason, an open baseline combining reasoning LLMs with diffusion models trained on 16,000 curated knowledge image pairs.", "description": "The paper proposes knowledge image generation as a new task requiring models to generate educational visuals (diagrams, charts, infographics) from vague prompts by autonomously inferring concepts and relationships. It introduces MMMG-Score, a novel evaluation metric combining knowledge fidelity (via graph edit distance between extracted and reference knowledge graphs) with visual clarity assessment using segmentation models. The benchmark comprehensively evaluates 16 state-of-the-art text-to-image models across diverse academic disciplines and educational complexity levels.", "key_contribution": "The main contribution is the MMMG benchmark for evaluating reasoning capabilities in text-to-image generation through knowledge images, along with MMMG-Score‚Äîa structured evaluation metric based on knowledge graph extraction and visual readability that better aligns with human judgment than existing perceptual metrics like FID or CLIP.", "novelty": "Unlike existing benchmarks that focus on compositionality and prompt-following, MMMG specifically targets reasoning-based image generation requiring multimodal reasoning that fuses world knowledge with pixel-level grounding. The paper addresses the limitation of current evaluation metrics by introducing knowledge graph-based assessment rather than relying on caption similarity or aesthetic scores. It uniquely abstracts diverse knowledge visuals into a unified graph representation enabling objective, format-agnostic evaluation across disciplines and educational levels.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought", "Compositional and Counterfactual Reasoning"], "score": 62.42, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4427549, "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "authors": "Kangrui Wang, Pingyue Zhang, Zihan Wang, Yaning Gao, Linjie Li, Qineng Wang, Hanyang Chen, Yiping Lu, Zhengyuan Yang, ..., Manling Li", "pdf_url": "https://openreview.net/pdf/ed11882b04656406635941c99bb0b5dbf475a088.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52479, "poster_number": 5502, "tag": "SD-1-5502 | Exhibit Hall C,D,E ", "relevance_score": 50.84, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that VLM agents can build internal world models through explicit visual state reasoning, achieving 3√ó performance improvement over untrained counterparts and surpassing proprietary models like GPT-5. The research reveals that optimal state representations are task-dependent: natural language excels for semantic relationships in general tasks, while structured formats are essential for high-precision manipulation. The proposed Bi-Level GAE enables turn-aware credit assignment, significantly improving multi-turn agent training.", "description": "VAGEN is a reinforcement learning framework that trains vision-language model agents to perform multi-turn agentic tasks by explicitly reasoning about visual states through StateEstimation (understanding current state) and TransitionModeling (predicting next state). The framework formulates the problem as a POMDP and uses multi-turn RL with world modeling rewards and bi-level advantage estimation to enable agents to maintain and update internal beliefs across interactions.", "key_contribution": "The main contribution is a principled approach to training VLM agents that architecturally enforces world model reasoning through explicit StateEstimation and TransitionModeling, combined with novel reward shaping (WorldModeling Reward via LLM-as-a-Judge) and Bi-Level GAE for turn-aware credit assignment in multi-turn reinforcement learning.", "novelty": "Unlike prior work that lacks explicit internal world modeling for visual state reasoning, this work systematically integrates world modeling into multi-turn VLM agent training through structured reasoning strategies. It addresses the challenge of partial observability in visual environments by formulating the problem as a POMDP and introduces Bi-Level GAE to solve the credit assignment problem across both turn and token levels. The work also provides comprehensive analysis of how different visual state representations (natural language, symbolic, structured) affect performance across diverse tasks.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Embodied AI and Vision-Language-Action Models"], "score": 50.84, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4413525, "title": "Enhancing Visual Prompting through Expanded Transformation Space and Overfitting Mitigation", "authors": "Shohei Enomoto", "pdf_url": "https://openreview.net/pdf/4d3dececab4908f91a81a58eb3b1b31959a0e01e.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52848, "poster_number": 2704, "tag": "SD-1-2704 | Exhibit Hall C,D,E ", "relevance_score": 52.53, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "ACAVP achieves state-of-the-art accuracy among visual prompting methods by introducing affine and color transformations alongside additive prompts, surpassing linear probing in average accuracy across 12 datasets. The paper identifies overfitting as a critical issue in VP training and demonstrates that TrivialAugment data augmentation provides universal benefits, improving existing VP methods by up to 12 percentage points. ACAVP exhibits superior robustness to distribution shifts while maintaining only 6% computational overhead compared to model inference.", "description": "This paper proposes ACAVP (Affine, Color, and Additive Visual Prompting), an enhanced visual prompting method for parameter-efficient fine-tuning of pre-trained vision models. The approach addresses two critical limitations of conventional VP: restricted expressivity of simple additive transformations and overfitting tendencies when parameter count increases. ACAVP introduces complementary transformation operations (affine for creating task-specific regions, color for emphasizing visual features) and employs TrivialAugment for effective overfitting mitigation.", "key_contribution": "The main contribution is a novel visual prompting framework that expands the transformation space through affine and color transformations while addressing overfitting via data augmentation. This achieves state-of-the-art VP performance with minimal computational overhead, and the overfitting mitigation strategy generalizes to improve existing VP methods significantly.", "novelty": "Unlike existing VP methods that rely solely on additive transformations, ACAVP introduces a richer transformation space combining affine, color, and additive operations that preserve original image information while enhancing task-relevant features. The work is the first to systematically identify and address overfitting as a fundamental limitation in VP training, demonstrating that appropriate data augmentation (TrivialAugment) is universally beneficial across VP methods. This differs from prior work that focused primarily on transformation design without addressing the overfitting problem or computational efficiency trade-offs.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation"], "score": 52.53, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4441730, "title": "DualCnst: Enhancing Zero-Shot Out-of-Distribution Detection via Text-Image Consistency in Vision-Language Models", "authors": "Fayi Le, Wenwu He, Chentao Cao, Dong Liang, Zhuo-Xu Cui", "pdf_url": "https://openreview.net/pdf/76ec8720e26ef14ba02c4eee1e192dc5c75c2ed3.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52606, "poster_number": 5309, "tag": "SD-1-5309 | Exhibit Hall C,D,E ", "relevance_score": 50.54, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "DualCnst achieves state-of-the-art zero-shot OOD detection by integrating both semantic (text-image) and visual (image-image) consistency, reducing FPR95 by 3.95% on ImageNet-1K far-OOD tasks compared to prior methods. The framework demonstrates that synthesizing visual exemplars from text labels via diffusion models can effectively approximate real ID images for OOD detection without requiring access to actual in-distribution data. Theoretical analysis proves that multimodal negative label aggregation reduces score variance and improves ID-OOD separability.", "description": "This paper introduces DualCnst, a training-free framework for zero-shot out-of-distribution detection that leverages vision-language models (CLIP) combined with text-to-image generation (Stable Diffusion). The method generates synthetic images from both ID and mined OOD textual labels, then evaluates test images based on dual consistency: semantic similarity to class labels and visual similarity to synthesized images. The unified scoring function fuses multimodal information across multiple encoder layers to enhance discriminative power for OOD detection.", "key_contribution": "The main contribution is a dual-consistency framework that uniquely combines text-based semantic similarity with visual similarity to synthetic images generated from labels, enabling robust zero-shot OOD detection without requiring real ID images or additional training. The method includes theoretical justification showing variance reduction through multimodal fusion and achieves superior performance across diverse benchmarks.", "novelty": "Unlike existing text-only VLM-based OOD detection methods (e.g., NegLabel, MCM) that rely solely on semantic similarity, DualCnst addresses the limitation of semantically similar but visually distinct OOD samples by incorporating visual features from synthesized images. The approach overcomes the constraint of requiring real ID images by using text-to-image diffusion models to generate visual exemplars, making it practical for privacy-sensitive and open-world scenarios. It provides the first training-free method that systematically fuses semantic and visual consistency across multiple encoder layers with theoretical guarantees on variance reduction.", "ai_categories": ["Hallucination Detection and Model Robustness", "Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization"], "score": 50.54, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4211001, "title": "Understanding Prompt Tuning and In-Context Learning via Meta-Learning", "authors": "Tim Genewein, Li Kevin Wenliang, Jordi Grau-Moya, Anian Ruoss, Laurent Orseau, Marcus Hutter", "pdf_url": "https://openreview.net/pdf/e8c73fa77b9026d1f61c4b015c19f5c062b83022.pdf", "session_id": 538, "session_name": "Mexico City Poster Session 1", "poster_id": 53055, "poster_number": 999, "tag": "MC-1-999 | Foyer ", "relevance_score": 71.95, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper establishes that prompt tuning works by conditioning Bayesian predictors that arise from meta-learning, and identifies fundamental limitations: optimal prompting is only possible when the target task has support in the pretraining distribution and is a single task (not a mixture). Soft prompts can manipulate network activations more effectively than hard tokens of equal length, but still cannot overcome these theoretical limitations. The work provides both theoretical analysis and empirical validation on LSTMs and Transformers.", "description": "The paper provides a theoretical framework for understanding prompt tuning and in-context learning through the lens of meta-learning and Bayesian inference. It analyzes when optimal prompting is and isn't possible, comparing different prefix-tuning methods (hard tokens, simplex, real-valued, and soft prompting) against weight-tuning approaches on controlled coin-flip sequence prediction tasks with known Bayes-optimal solutions.", "key_contribution": "The paper establishes formal conditions under which optimal prompting is theoretically possible or impossible, showing that prompting can only adapt models to single tasks within the pretraining distribution, not to novel tasks or task mixtures. It demonstrates that soft prompts exploit neural circuitry more effectively than hard tokens but remain bound by the same theoretical limitations.", "novelty": "Unlike prior empirical work on prompt optimization, this paper provides a rigorous theoretical characterization of prompting's fundamental capabilities and limitations through Bayesian analysis of meta-learned predictors. It addresses the gap between conceptual understanding and empirical method development by formally proving when prompting can and cannot work, and validates these theoretical predictions with carefully controlled experiments comparing against tractable Bayes-optimal baselines.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation"], "score": 71.95, "session_type": "Mexico City Poster Session 1"}, {"paper_id": 4140838, "title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing", "authors": "Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Xiaorong Zhu, Hao Li, Wenhao Chai, Zicheng Zhang, Renqiu Xia, Guangtao Zhai, ..., Haodong Duan", "pdf_url": "https://arxiv.org/pdf/2504.02826", "session_id": 539, "session_name": "San Diego Oral 2", "poster_id": 53085, "poster_number": 999, "tag": "Oral 2A | 15:30 Exhibit Hall F,G,H ", "relevance_score": 73.23, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Current state-of-the-art visual editing models, including GPT-4o-Image, achieve only 28.9% accuracy on reasoning-informed visual editing tasks, revealing significant limitations in temporal, causal, spatial, and especially logical reasoning (10.6% accuracy). The paper introduces RISEBench, the first benchmark specifically designed to evaluate reasoning-informed visual editing capabilities across 360 carefully curated test cases. An LMM-as-a-judge evaluation framework is validated with strong correlation to human judgments (MAE of 0.5-0.7 on a 1-5 scale).", "description": "This paper presents RISEBench, a benchmark for evaluating Reasoning-Informed viSual Editing (RISE) capabilities of large multimodal models. The benchmark focuses on four key reasoning categories: temporal, causal, spatial, and logical reasoning, with 360 manually annotated test cases. The evaluation framework assesses three dimensions‚Äîinstruction reasoning, appearance consistency, and visual plausibility‚Äîusing both human judges and an LMM-as-a-judge approach.", "key_contribution": "The main contribution is the creation of the first dedicated benchmark for reasoning-informed visual editing, establishing evaluation criteria across four reasoning types and developing a robust LMM-as-a-judge framework that correlates strongly with human expert assessments while enabling scalable automated evaluation.", "novelty": "Unlike traditional image editing benchmarks that focus on simple instruction following or appearance preservation, RISEBench specifically targets complex reasoning capabilities required for visual editing. It addresses the gap in evaluating models' ability to perform intelligent visual modifications based on contextual understanding and logical reasoning, going beyond superficial attribute changes. The work systematically categorizes reasoning types needed for visual editing and provides the first comprehensive evaluation of both open-source and proprietary models on these capabilities.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought", "Compositional and Counterfactual Reasoning"], "score": 73.23, "session_type": "San Diego Oral 2"}, {"paper_id": 4232011, "title": "Spatial Understanding from Videos: Structured Prompts Meet Simulation Data", "authors": "Haoyu Zhang, Meng Liu, Zaijing Li, Haokun Wen, Weili Guan, Yaowei Wang, Liqiang Nie", "pdf_url": "https://openreview.net/pdf/ae7873ba9e6cdcfd29717a979c05e0533e2d1f47.pdf", "session_id": 538, "session_name": "Mexico City Poster Session 1", "poster_id": 53060, "poster_number": 999, "tag": "MC-1-999 | Foyer ", "relevance_score": 80.77, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that combining structured prompting (SpatialMind) with synthetic training data (ScanForgeQA) significantly improves 3D spatial reasoning in vision-language models without architectural modifications, achieving consistent gains across models of varying scales. The work reveals that VLMs better interpret textual spatial descriptions than structured formats like 3D maps or grids, and that prompting benefits larger models more while fine-tuning helps smaller models. The approach achieves up to 8.5% improvement on spatial reasoning benchmarks and shows that VLMs can complement human perception by excelling at quantitative spatial tasks.", "description": "The paper addresses spatial understanding from scanning videos by proposing a dual approach: SpatialMind, a structured Chain-of-Thought prompting strategy that decomposes scenes and questions into interpretable reasoning steps, and ScanForgeQA, a scalable synthetic QA dataset built from 34K simulated 3D scenes. The framework enables pre-trained VLMs to perform 3D spatial reasoning from video input alone, without requiring point clouds or architectural changes, and is validated across multiple benchmarks including VSI-Bench, OpenEQA, ScanQA, and SQA3D.", "key_contribution": "The main contributions are: (1) SpatialMind, a structured prompting strategy that guides VLMs through multi-step spatial reasoning by decomposing both scenes and questions, and (2) ScanForgeQA, an automatically constructed dataset of 925K QA pairs from 34K synthetic single-room scenes that enables spatial commonsense learning through fine-tuning.", "novelty": "Unlike existing methods that rely on expensive 3D sensors and point clouds, this work achieves spatial understanding from vision-only inputs (scanning videos). It addresses data scarcity through scalable synthetic scene generation rather than real-world scanning, and tackles spatial uncertainty through structured decomposition of both visual scenes (into textual descriptions, 3D maps, or 2D grids) and reasoning questions (into step-by-step inference chains). The work uniquely demonstrates that textual spatial descriptions are more interpretable to current VLMs than structured spatial formats, and shows complementary benefits of prompting and fine-tuning strategies.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Video Understanding and Temporal Reasoning", "Prompt Learning and Optimization"], "score": 80.77, "session_type": "Mexico City Poster Session 1"}, {"paper_id": 4442544, "title": "Vinci: Deep Thinking in Text-to-Image Generation using Unified Model with Reinforcement Learning", "authors": "Wang Lin, Wentao Hu, Liyu Jia, Kaihang Pan, Zhang Majun, Zhou Zhao, Fei Wu, Jingyuan Chen, Hanwang Zhang", "pdf_url": "https://openreview.net/pdf/f6be9731a06937642b60f557e3342249472839d1.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53904, "poster_number": 5513, "tag": "SD-2-5513 | Exhibit Hall C,D,E ", "relevance_score": 75.05, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Vinci achieves a +22% improvement over the base model on GenEval benchmarks by integrating Multimodal Chain-of-Thought (MCoT) capabilities into text-to-image generation. The model introduces a momentum-based reward function that dynamically adjusts rewards based on historical improvements, enabling stable iterative refinement across multiple generation steps. This is the first image generation model capable of deep reasoning through interleaved image generation and understanding.", "description": "This paper presents Vinci, a unified framework that enables interleaved image generation and understanding through deep reasoning capabilities using reinforcement learning. The model uses a small amount of MCoT data for cold-start training, then employs RL with multiple reward signals (program-based, QA-based, and momentum process rewards) to guide the integration of generation and understanding tasks. The approach allows the model to observe and reflect on generated images in real-time, iteratively improving outputs through a variable-length reasoning process.", "key_contribution": "The main contribution is the first text-to-image generation model with deep thinking capabilities that achieves end-to-end integration of image generation and understanding through Multimodal Chain-of-Thought reasoning. The introduction of a momentum-based process reward function that considers historical improvements ensures stable optimization across multiple generation iterations.", "novelty": "Unlike existing unified models like Janus-Pro and Show-o that use separate representations for generation and understanding, Vinci achieves true end-to-end integration with unified image representation. Previous approaches like T2I-R1 only applied unimodal CoT (either text-level or token-level), while Vinci introduces interleaved multimodal CoT that allows cross-modal reasoning. The momentum-based reward function is novel in balancing immediate performance with long-term stability by normalizing current improvements against historical volatility, preventing performance fluctuations across iterations.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Prompt Learning and Optimization"], "score": 75.05, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4215000, "title": "Flex-Judge: Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators", "authors": "Jongwoo Ko, Sungnyun Kim, Sungwoo Cho, Se-Young Yun", "pdf_url": "https://openreview.net/pdf/0895cf6fc3c5ff656996f38666b8fb528c975fb4.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53228, "poster_number": 3703, "tag": "SD-2-3703 | Exhibit Hall C,D,E ", "relevance_score": 80.46, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "FLEX-Judge demonstrates that training multimodal judge models on only 1K text-based reasoning examples enables zero-shot generalization across image, video, audio, and molecular modalities without any modality-specific supervision. The model achieves competitive or superior performance compared to commercial APIs (GPT-4o, Gemini) and extensively trained multimodal evaluators, while using significantly less training data. The approach successfully extends to underexplored domains like molecular evaluation where no existing reward models exist.", "description": "This paper proposes FLEX-Judge, a reasoning-guided multimodal evaluator trained solely on a small corpus (1K samples) of high-quality textual reasoning data. The core insight is that structured textual reasoning explanations encode generalizable decision-making patterns that transfer effectively to multimodal judgments across diverse modalities including images, videos, audio, and molecules. The model leverages reasoning-first evaluation where it generates detailed rationales before producing final judgments.", "key_contribution": "The main contribution is demonstrating that minimal text-only reasoning supervision (1K samples) is sufficient to train effective multimodal judge models that generalize across multiple modalities without requiring expensive modality-specific annotations. This provides a cost-effective alternative to traditional annotation-intensive approaches that typically require 100K+ multimodal training samples.", "novelty": "Unlike existing multimodal judge models that require extensive modality-specific annotations (e.g., 113K-150K image-text pairs), FLEX-Judge uses only 1K text reasoning samples to achieve cross-modal generalization. The work addresses the limitation that current LLM-as-a-Judge approaches are restricted to text-only scenarios or require costly multimodal datasets. It introduces a novel reasoning-first training paradigm that prioritizes high-quality textual rationales over modality-specific supervision, enabling application to resource-constrained domains like molecular modeling where comprehensive evaluation benchmarks are scarce.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization"], "score": 80.46, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4388081, "title": "Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning", "authors": "Tianle Zhang, Wanlong Fang, Jonathan Woo, Paridhi Latawa, Deepak A. Subramanian, Alvin Chan", "pdf_url": "https://openreview.net/pdf/01fbf16fb741c1fe74011db4bbab3fdfea94f97c.pdf", "session_id": 537, "session_name": "San Diego Poster Session 1", "poster_id": 52992, "poster_number": 4015, "tag": "SD-1-4015 | Exhibit Hall C,D,E ", "relevance_score": 50.53, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "<cite index=\"1-17\">ICRL is the first training-free framework for integrating non-text modality representations into text-based LLMs</cite>, enabling multimodal reasoning without costly supervised training. <cite index=\"4-5\">An approach based on optimal transport theory, which aligns the distribution of FM representations with that of LLM embeddings, yields promising results</cite>. The paper demonstrates that <cite index=\"4-22\">dimensionality reduction (i.e., PCA) is surprisingly effective</cite> for fitting high-dimensional vectors as text within LLM's context window.", "description": "<cite index=\"1-5,1-6\">This work explores the feasibility of integrating representations from non-text foundational models (FMs) into text-based LLMs in a training-free manner through In-Context Representation Learning (ICRL)</cite>. <cite index=\"1-7\">Unlike traditional in-context learning, which incorporates text-label pairs, ICRL replaces text inputs with FM representations, enabling the LLM to perform multi-modal inference without fine-tuning</cite>. <cite index=\"1-16\">The framework is evaluated on a suite of tasks in the molecular domain, investigating how to map FM representations into LLMs, what factors influence ICRL performance, and what mechanisms underlie its effectiveness</cite>.", "key_contribution": "<cite index=\"5-20\">The paper introduces ICRL as the first training-free framework for integrating non-text modalities into text-based LLMs, provides a comprehensive exploration of design choices and their impact on ICRL performance across molecular tasks, and offers mechanistic insights into how the distribution of projected representations influences performance</cite>.", "novelty": "<cite index=\"1-4\">Existing approaches for integrating non-text modality representations into LLMs typically require additional costly supervised training, restricting on-the-fly adaptation to new domains and modalities</cite>. <cite index=\"3-18\">The proposed approach aims to study training-free methods to derive the projector to avoid costly supervised training</cite>, unlike Vector-ICL which requires pre-training and finetuning. <cite index=\"3-25,3-26\">This work is pioneering in its focus on enabling LLMs to directly learn from and utilize representations derived from diverse modalities beyond text from external models, potentially expanding the utility of LLMs in domains requiring the integration of multiple modalities and knowledge</cite>.", "ai_categories": ["In-Context Learning and Few-Shot Adaptation", "Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization"], "score": 50.53, "session_type": "San Diego Poster Session 1"}, {"paper_id": 4064207, "title": "NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions", "authors": "Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Dong Wang, Ilia Kulikov, Kyunghyun Cho, Yuandong Tian, ..., Xian Li", "pdf_url": "https://arxiv.org/pdf/2502.13124", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53906, "poster_number": 1801, "tag": "SD-2-1801 | Exhibit Hall C,D,E ", "relevance_score": 66.38, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "NaturalReasoning introduces a dataset of 2.8M diverse, challenging reasoning questions spanning multiple domains (STEM, Economics, Social Sciences) extracted from pretraining corpora. The dataset demonstrates superior sample efficiency in knowledge distillation experiments, requiring fewer training examples to achieve better performance than existing reasoning datasets. Models trained on NaturalReasoning show strong performance across reasoning benchmarks and enable effective self-training through both external reward models and self-rewarding mechanisms.", "description": "This paper presents NaturalReasoning, a large-scale reasoning dataset created by using LLMs to analyze pretraining documents, identify high-reasoning content, and synthesize challenging questions with reference answers. The dataset is validated through extensive experiments showing its effectiveness for knowledge distillation from stronger teacher models and unsupervised self-training using reinforcement learning approaches. The questions are longer, more difficult, and more diverse than existing reasoning datasets, covering domains beyond traditional math and coding.", "key_contribution": "The main contribution is a scalable approach for generating diverse, high-quality reasoning questions grounded in pretraining corpora, resulting in a 2.8M question dataset that is more sample-efficient for post-training and supports both supervised distillation and self-training methods without requiring human annotation.", "novelty": "Unlike existing datasets that focus primarily on math (MetaMathQA, OpenMathInstruct-2) or extract questions directly from web pages (WebInstruct), NaturalReasoning synthesizes novel questions grounded in pretraining documents using only LLMs without manual curation. The approach emphasizes question difficulty and diversity across multiple domains, with questions requiring longer chain-of-thought reasoning (434 words median response length vs. 158-354 for other datasets). The dataset uniquely supports both knowledge distillation and various self-training paradigms including self-rewarding mechanisms.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "In-Context Learning and Few-Shot Adaptation", "Reinforcement Learning for Multimodal Models"], "score": 66.38, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4441722, "title": "INST-IT: Boosting Instance Understanding via Explicit Visual Prompt Instruction Tuning", "authors": "Wujian Peng, Lingchen Meng, Yitong Chen, Yiweng Xie, Yang Liu, Tao Gui, Hang Xu, Xipeng Qiu, Zuxuan Wu, Yu-Gang Jiang", "pdf_url": "https://openreview.net/pdf/981b9df93d11c493f23f85073116bf2f42508fe4.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53253, "poster_number": 4618, "tag": "SD-2-4618 | Exhibit Hall C,D,E ", "relevance_score": 67.18, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "INST-IT introduces the first instance-grounded instruction-tuning dataset covering both images and videos with explicit visual prompts (Set-of-Marks). Models trained with INST-IT achieve ~20% improvement on instance understanding benchmarks while also showing significant gains on generic benchmarks (4.4% on AI2D, 7.8% on Egoschema), demonstrating that enhanced instance understanding strengthens overall multimodal comprehension capabilities.", "description": "This paper addresses the limitation of Large Multimodal Models (LMMs) in understanding specific instances within images and videos. The authors propose INST-IT, which includes: (1) an automated pipeline using GPT-4o with Set-of-Marks visual prompting to generate multi-level instance-centric annotations, (2) a large-scale dataset with 51k images and 21k videos containing 836k instance-level captions, and (3) a continuous instruction-tuning paradigm that effectively enhances both instance-level and general understanding.", "key_contribution": "The main contribution is a comprehensive framework for instance-level understanding in multimodal models, consisting of INST-IT Dataset (the first instance-grounded instruction-tuning dataset for both images and videos), INST-IT Bench (a human-verified evaluation benchmark), and a continuous instruction-tuning recipe that leverages explicit visual prompts to significantly improve fine-grained instance comprehension.", "novelty": "Unlike existing datasets that focus on holistic understanding, INST-IT provides multi-granularity annotations (instance-level, frame-level, temporal dynamics, video-level) grounded on individual instances using Set-of-Marks visual prompting. While prior work like ViP-LLaVA explored visual prompts for images only, this is the first to extend instance understanding to videos with temporal dynamics. The approach demonstrates that instance-level training generalizes well to both specialized instance tasks and generic comprehension benchmarks, addressing the gap between coarse-grained and fine-grained multimodal understanding.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Prompt Learning and Optimization", "Multimodal Benchmarks and Evaluation"], "score": 67.18, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4209416, "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents", "authors": "Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, Jun Xu", "pdf_url": "https://openreview.net/pdf/f6767d3d1ddd7b12b85d94abd6d7a313406de8a8.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53987, "poster_number": 1412, "tag": "SD-2-1412 | Exhibit Hall C,D,E ", "relevance_score": 63.18, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper reveals that longer chain-of-thought reasoning actually degrades GUI grounding performance, contrary to assumptions from general R1-Zero training. It identifies two opposing forms of reward hacking in existing reward functions and biases in GRPO that favor easier samples. The proposed GUI-G1-3B achieves state-of-the-art performance (90.3% on ScreenSpot, 37.1% on ScreenSpot-Pro) using only 17K training samples, outperforming larger models while generating significantly fewer output tokens.", "description": "The paper critically analyzes R1-Zero-like training paradigms for GUI grounding agents by decomposing the pipeline into three components: input design (templates), output evaluation (reward functions), and policy update (RL objectives). Through systematic experiments, it identifies fundamental issues with applying general-purpose RL to GUI grounding tasks and proposes targeted solutions including a Fast Thinking Template, box-size-constrained rewards, and difficulty-aware GRPO modifications.", "key_contribution": "The main contribution is a comprehensive analysis of why standard R1-Zero training fails for GUI grounding, along with three targeted solutions: (1) a Fast Thinking Template that eliminates harmful reasoning steps, (2) a box-size reward constraint to prevent reward hacking, and (3) modified GRPO with difficulty weighting and no length normalization to address training biases.", "novelty": "Unlike prior R1-style GUI agents that blindly adopt reasoning-heavy templates and standard reward functions, this work demonstrates that explicit reasoning harms grounding performance due to its reliance on image tokens rather than text tokens. It is the first to identify and address opposing reward hacking behaviors (smaller vs. larger boxes) and training biases (length and difficulty) specific to GUI grounding, achieving superior results with minimal data and computational cost.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Visual Grounding and Spatial Reasoning"], "score": 63.18, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4437500, "title": "EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT", "authors": "Baoqi Pei, Yifei Huang, Jilan Xu, Yuping He, Guo Chen, Fei Wu, Jiangmiao Pang, Yu Qiao", "pdf_url": "https://openreview.net/pdf/1665e46ba22f09a2c8f9a73f99a650efa4e800a8.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53468, "poster_number": 4719, "tag": "SD-2-4719 | Exhibit Hall C,D,E ", "relevance_score": 66.18, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "EgoThinker achieves state-of-the-art performance across multiple egocentric video benchmarks by introducing EgoRe-5M, a large-scale dataset with 5M QA pairs featuring chain-of-thought rationales and fine-grained spatio-temporal grounding. The two-stage training paradigm combining supervised fine-tuning with reinforcement fine-tuning via GRPO significantly enhances both high-level reasoning and low-level hand-object localization capabilities. The model demonstrates substantial improvements in temporal grounding (63.9% R1@0.05 on EgoExoLearn) and spatial grounding (80.3% Loc-Acc on EK-Visor) while maintaining general video understanding abilities.", "description": "This paper presents EgoThinker, a framework that endows multimodal large language models with robust egocentric reasoning capabilities through spatio-temporal chain-of-thought supervision. The approach constructs EgoRe-5M, a dataset of 5M QA pairs from 13M egocentric video clips with detailed CoT annotations and dense hand-object grounding, then employs a two-stage curriculum: supervised fine-tuning on diverse egocentric data followed by reinforcement fine-tuning using GRPO to enhance spatio-temporal localization. The framework addresses unique challenges in egocentric video understanding including inferring hidden intentions, recognizing fine-grained hand-object interactions, and integrating multi-horizon temporal context.", "key_contribution": "The main contribution is EgoRe-5M, a large-scale egocentric QA dataset with chain-of-thought rationales and hand-object annotations, combined with a two-stage training regime (SFT + RFT via GRPO) that effectively couples high-level reasoning with low-level spatio-temporal grounding for egocentric video understanding.", "novelty": "Unlike existing MLLMs that excel at third-person visual reasoning, EgoThinker is the first to enable robust first-person reasoning by explicitly modeling causal relationships through CoT annotations and fine-grained hand-object interactions. The work addresses limitations of existing egocentric datasets that lack explicit reasoning chains and detailed grounding data by mining 8.7M high-quality clips from web data and constructing specialized QA pairs spanning short-term perception, long-term causal reasoning, and fine-grained grounding. The novel application of GRPO with rule-based rewards for spatio-temporal grounding tasks significantly improves localization accuracy while preserving general video understanding capabilities.", "ai_categories": ["Video Understanding and Temporal Reasoning", "Vision-Language Reasoning and Chain-of-Thought", "Embodied AI and Vision-Language-Action Models"], "score": 66.18, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4427313, "title": "All You Need is One: Capsule Prompt Tuning with a Single Vector", "authors": "Yiyang Liu, James Chenhao Liang, Heng Fan, Wenhao Yang, Yiming Cui, Xiaotian Han, Lifu Huang, Dongfang Liu, Qifan Wang, Cheng Han", "pdf_url": "https://openreview.net/pdf/66d1effdb19558fa52a14e3d6f9475451d810c9d.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53890, "poster_number": 2711, "tag": "SD-2-2711 | Exhibit Hall C,D,E ", "relevance_score": 63.07, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "The paper reveals that traditional task-aware soft prompts fail to interact effectively with input sequences, primarily attending to themselves rather than critical structural tokens. Incorporating instance-aware information creates an 'attention anchor' phenomenon where prompts strongly attend to and are attended by input tokens, enabling superior performance with just a single learnable vector per layer. CaPT achieves state-of-the-art results (e.g., 84.03% on T5-Large) while using only 0.003-0.004% of model parameters and eliminating time-consuming prompt length search.", "description": "This paper introduces Capsule Prompt-Tuning (CaPT), a parameter-efficient fine-tuning method that combines instance-aware and task-aware information in prompt-based learning for large language models. The approach uses a single learnable vector per layer that is combined with mean-pooled instance representations to create 'capsule prompts' that serve as attention anchors. Through extensive experiments on SuperGLUE benchmarks across T5, Llama, and Qwen models, the authors demonstrate that CaPT achieves superior performance while being extremely parameter-efficient and eliminating the need for prompt length hyperparameter search.", "key_contribution": "The main contribution is the discovery of the 'attention anchor' phenomenon and the introduction of CaPT, which leverages both instance-aware and task-aware information using only one single learnable vector per layer. This design achieves state-of-the-art performance while being nearly parameter-free (0.003-0.004% of model parameters) and eliminating the need for time-intensive grid search over prompt lengths.", "novelty": "Unlike existing prompt-tuning methods that rely solely on task-aware soft prompts and require extensive hyperparameter search for optimal prompt length, CaPT incorporates off-the-shelf instance-aware semantics that create stronger attention interactions with input sequences. The paper addresses the limitation that traditional soft prompts primarily attend to themselves rather than critical input tokens, and introduces a fixed-length design (single vector) that eliminates grid search while maintaining superior performance. The attention anchor mechanism enables prompts to both attend to structurally important tokens and receive attention from the entire input sequence, creating mutual contextual grounding.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation"], "score": 63.07, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4388622, "title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning", "authors": "Ye Liu, Zongyang Ma, Junfu Pu, Zhongang Qi, Yang Wu, Ying Shan, Chang Wen Chen", "pdf_url": "https://openreview.net/pdf/336fb0f358e8f162033e50e7ebc25e1ba700ce2f.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53897, "poster_number": 4606, "tag": "SD-2-4606 | Exhibit Hall C,D,E ", "relevance_score": 64.25, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "UniPixel achieves state-of-the-art performance on 10 benchmarks across 9 pixel-level tasks, with the 3B model obtaining 62.1 J&F on ReVOS and 72.8% accuracy on VideoRefer-BenchQ, surpassing larger 7B-13B models. The unified approach demonstrates mutual reinforcement between referring and segmentation capabilities, where joint training improves both tasks. The paper introduces a novel PixelQA task requiring simultaneous object-centric referring, segmentation, and question answering in videos.", "description": "This paper presents UniPixel, a large multimodal model that unifies object referring and segmentation for pixel-level visual reasoning in images and videos. The model uses a novel object memory bank to bridge sparse visual prompts (points, boxes) with dense object masks, enabling flexible comprehension of visual prompts and generation of mask-grounded responses. UniPixel processes visual prompts, generates relevant masks on demand, and performs subsequent reasoning conditioned on these intermediate pointers during inference.", "key_contribution": "The main contribution is a unified framework that seamlessly integrates pixel-level perception with general visual understanding through an object memory bank design, enabling flexible pixel-level reasoning that simultaneously supports visual prompt comprehension and mask prediction. This is the first end-to-end method to unify object referring and segmentation in both images and videos.", "novelty": "Unlike existing methods that perform either referring or segmentation independently with rigid input/output templates, UniPixel unifies both capabilities through a novel object memory bank that stores and injects spatial-temporal object information. The approach addresses the limitation of previous models that cannot integrate fine-grained perception with multi-modal reasoning, and introduces a memory pre-filling and injection mechanism that decouples regional understanding from mask prediction while enabling both to benefit from joint training.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Vision-Language Reasoning and Chain-of-Thought", "Video Understanding and Temporal Reasoning"], "score": 64.25, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4151803, "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning", "authors": "Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, Wenhu Chen", "pdf_url": "https://openreview.net/pdf/4caa878eb3dbd25718a328d937d8d6cc4e0d3100.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53353, "poster_number": 4608, "tag": "SD-2-4608 | Exhibit Hall C,D,E ", "relevance_score": 62.37, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "VL-Rethinker achieves state-of-the-art performance on multimodal reasoning benchmarks (80.4% on MathVista, 63.5% on MathVerse) by using direct reinforcement learning without costly distillation from teacher models. The work reveals that multimodal 'slow-thinking' differs fundamentally from text-only models - improvements come from perceptual verification rather than longer reasoning chains. The proposed Selective Sample Replay (SSR) and Forced Rethinking techniques successfully incentivize self-reflection in vision-language models.", "description": "This paper presents VL-Rethinker, a vision-language model trained using reinforcement learning to develop slow-thinking capabilities for complex multimodal reasoning tasks. The approach combines two key techniques: Selective Sample Replay (SSR) to address the vanishing advantages problem in GRPO training, and Forced Rethinking to explicitly encourage self-reflection and self-correction behaviors. The model is trained on 38,870 diverse queries without requiring distillation from stronger teacher models.", "key_contribution": "The main contributions are: (1) Selective Sample Replay (SSR), which mitigates vanishing advantages in GRPO by maintaining a replay buffer of high-value experiences and prioritizing samples based on advantage magnitude, and (2) Forced Rethinking, which appends rethinking triggers during training to explicitly incentivize self-verification and self-correction behaviors in vision-language models.", "novelty": "Unlike existing approaches that rely on costly supervised fine-tuning and distillation from teacher models, this work demonstrates that direct RL training can achieve superior results. The paper identifies and addresses the vanishing advantages problem in GRPO for VLMs through SSR, which dynamically prioritizes training on decision-boundary examples. Critically, it reveals that multimodal slow-thinking is qualitatively different from text-only reasoning - the model learns to 'look twice' for perceptual verification rather than simply extending logical chains.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Hallucination Detection and Model Robustness"], "score": 62.37, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4263224, "title": "RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models", "authors": "Yeongtak Oh, Dohyun Chung, Juhyeon Shin, Sangha Park, Johan Barthelemy, Jisoo Mok, Sungroh Yoon", "pdf_url": "https://openreview.net/pdf/f251aa0193aee75b85b81e71d2046d1764951792.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53950, "poster_number": 4716, "tag": "SD-2-4716 | Exhibit Hall C,D,E ", "relevance_score": 73.06, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "<cite index=\"1-7,3-2\">This is the first RL-based approach to post-train MLLMs for personalized image captioning.</cite> <cite index=\"3-8\">The method significantly enhances both visual recognition and personalized generation capabilities of MLLMs, and consistently outperforms existing SFT-based baselines, especially in the challenging multi-concept image captioning task.</cite> <cite index=\"4-7\">RePIC leverages GRPO with three novel verifiable rewards (object consistency, visual localization, and identity consistency) to mitigate the data-centric limitations of previous SFT-based methods.</cite>", "description": "<cite index=\"1-2,1-3,1-4\">Recent multi-modal large language models (MLLMs) often struggle to generate personalized image captions, even when trained on high-quality captions. Such limitations persist in existing post-training-based MLLM personalization methods. Despite being post-tuned with large-scale caption data through supervised fine-tuning (SFT), these models frequently fail to produce faithful descriptions in real-world scenarios, such as multi-concept image captioning.</cite> <cite index=\"1-6,1-14\">To address the data-centric nature of SFT, the paper proposes a reinforcement learning (RL)-based post-training framework.</cite>", "key_contribution": "<cite index=\"3-10,3-11,3-12,3-13\">The paper proposes RePIC framework with three key components: an object consistency reward that provides direct positive and negative feedback for the output, a visual localization reward that predicts bounding box coordinates based on a query instruction, and an identity consistency reward that explicitly encourages the inclusion of target names in the output.</cite>", "novelty": "<cite index=\"1-5,1-6\">Acquiring large-scale, high-quality captions for complex settings is both costly and difficult. To address the data-centric nature of SFT, the paper proposes a reinforcement learning (RL)-based post-training framework.</cite> <cite index=\"3-14\">The experimental results reveal that SFT-based personalization methods are highly limited for visual recognition and generalization abilities.</cite> <cite index=\"3-27\">Importantly, RePIC does not degrade the original model's general captioning capabilities after post-tuning.</cite>", "ai_categories": ["Reinforcement Learning for Multimodal Models", "Visual Grounding and Spatial Reasoning", "Prompt Learning and Optimization"], "score": 73.06, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4442656, "title": "Stitch and Tell: A Structured Data Augmentation Method for Spatial Understanding", "authors": "Hang Yin, Xiaomin He, Peiwen Yuan, Yiwei Li, Jiayi Shi, Wenxiao Fan, Shaoxiong Feng, Kan Li", "pdf_url": "https://openreview.net/pdf/9de7c7ad156768fbba5018c67e831060150f4fa6.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53308, "poster_number": 1516, "tag": "SD-2-1516 | Exhibit Hall C,D,E ", "relevance_score": 62.75, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "The paper demonstrates that spatial hallucinations in vision-language models stem from sparse spatial information in training data, with existing datasets containing only 1.68-7.32% spatially-aware samples. The proposed Stitch and Tell (SiTe) method achieves significant improvements on spatial understanding benchmarks (e.g., +5.50% on MMEPosition, +4.19% on Spatial-MM) while maintaining or improving general vision-language performance. The method reduces training time by over 20% through increased data density while requiring no manual annotations or large-scale generation models.", "description": "This paper introduces Stitch and Tell (SiTe), a structured multimodal data augmentation method that addresses spatial hallucinations in vision-language models. The approach consists of two steps: (1) Image-Spatial Stitching - combining two images along spatial axes (horizontal/vertical), and (2) Text-Spatial Telling - generating spatially-aware captions using templates and creating spatial QA pairs by extracting object nouns. The method is evaluated across three architectures (LLaVA-v1.5-7B, LLaVA-Qwen2-1.5B, HALVA-7B) and thirteen benchmarks.", "key_contribution": "The main contribution is a simple, annotation-free, plug-and-play data augmentation method that injects explicit spatial supervision into multimodal training data through structured image stitching and template-based caption generation, without requiring costly models or human involvement. This bridges the modality gap between rich spatial features in images and sparse spatial expressions in text.", "novelty": "Unlike traditional augmentation methods (cropping, rotation) that can break image-text alignment or introduce spatial noise, and unlike recent approaches that rely on expensive large models for caption rewriting or image editing, SiTe provides weak spatial supervision through deterministic stitching and template-based text generation. The method addresses the fundamental asymmetry between images (rich spatial features) and text (sparse spatial descriptions) by explicitly encoding spatial relationships through structured layouts and spatially-aware language templates. It is the first to systematically inject spatial structure at scale without manual annotation or model-generated content.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Compositional and Counterfactual Reasoning", "Hallucination Detection and Model Robustness"], "score": 62.75, "session_type": "San Diego Poster Session 2"}, {"paper_id": 3443861, "title": "OmniBench: Towards The Future of  Universal Omni-Language Models", "authors": "Yizhi LI, Ge Zhang, Yinghao Ma, Ruibin Yuan, King Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Noah Wang, ..., Chenghua Lin", "pdf_url": "https://openreview.net/pdf/c52922a7cb62ce2d159c41efd473976ed5a5693f.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53534, "poster_number": 5201, "tag": "SD-2-5201 | Exhibit Hall C,D,E ", "relevance_score": 57.81, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Current open-source omni-language models (OLMs) exhibit critical limitations in tri-modal (image, audio, text) reasoning, with most performing below 50% accuracy even when provided with textual representations of images or audio. Vision-language models generally outperform audio-language models when using textual approximations, and proprietary models like GPT-4o and Claude-3.5-Sonnet significantly outperform open-source alternatives. The benchmark reveals that existing MLLMs struggle to construct consistent contexts from simultaneous image, audio, and text inputs.", "description": "This paper introduces OmniBench, a comprehensive benchmark with 1,142 human-annotated question-answer pairs designed to evaluate multimodal large language models' ability to simultaneously process and reason across visual, acoustic, and textual inputs. The benchmark enforces a unique constraint where accurate responses require integrated understanding of all three modalities. Additionally, the authors curate OmniInstruct, an 84.5K training dataset to facilitate supervised fine-tuning of omni-language models.", "key_contribution": "The main contribution is OmniBench, the first rigorous benchmark specifically designed to evaluate tri-modal (image, audio, text) understanding and reasoning capabilities of multimodal language models, along with OmniInstruct, a curated training dataset for improving omni-language model performance.", "novelty": "Unlike existing benchmarks that focus on single or dual modalities, OmniBench uniquely requires models to integrate information from all three modalities (image, audio, text) simultaneously to answer questions correctly. The work addresses the gap in comprehensive evaluation tools for true omni-modal understanding by implementing rigorous human and automated quality control processes that ensure questions cannot be answered using only one or two modalities. The benchmark also reveals that current MLLM training paradigms overlook the ability to construct consistent contexts from multiple modalities.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Video Understanding and Temporal Reasoning"], "score": 57.81, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4418213, "title": "FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models", "authors": "Shengming Yuan, Xinyu Lyu, Shuailong Wang, Beitao Chen, Jingkuan Song, Lianli Gao", "pdf_url": "https://openreview.net/pdf/21b2b82696dbfdaf174b6c8e52ac2477c419be55.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53122, "poster_number": 5406, "tag": "SD-2-5406 | Exhibit Hall C,D,E ", "relevance_score": 57.06, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper reveals that middle layers in MLLMs are critical for controlling associative reasoning strength, and that hallucinated responses can be exploited to derive effective steering vectors. FlexAC achieves up to 5.8√ó improvement in creativity on Creation-MMBench and 29% reduction in hallucination rate on CHAIR, demonstrating flexible control over the faithfulness-creativity trade-off without requiring training.", "description": "The paper proposes FlexAC (Flexible Association Control), a lightweight training-free framework that enables MLLMs to flexibly modulate associative reasoning strength based on task demands. The approach extracts steering vectors from hallucination-guided intermediate representations in middle layers and applies adaptive calibration during inference to balance between factual accuracy (low association) and creative generation (high association).", "key_contribution": "FlexAC introduces a unified framework for controllable associative reasoning in MLLMs through hallucination-guided steering vectors applied to middle-layer representations, enabling task-aware switching between hallucination suppression and creativity enhancement without any training.", "novelty": "Unlike existing methods that either suppress hallucinations at the cost of creativity or lack mechanisms for controllable modulation, FlexAC provides flexible bidirectional control over associative strength. It addresses the limitation of fixed associative behavior by introducing steering intensity calibration and task-specific directional integration, allowing models to adapt their reasoning style to different task requirements while maintaining general capabilities.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Hallucination Detection and Model Robustness", "Prompt Learning and Optimization"], "score": 57.06, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4384957, "title": "SAMPO: Scale-wise Autoregression with Motion Prompt for Generative World Models", "authors": "Sen Wang, Jingyi Tian, Le Wang, Zhimin Liao, lijiayi, Huaiyi Dong, Kun Xia, Sanping Zhou, Wei Tang, Gang Hua", "pdf_url": "https://openreview.net/pdf/829c7820e5b754498ac9f30bbe8588d4d77b2ddf.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53433, "poster_number": 2213, "tag": "SD-2-2213 | Exhibit Hall C,D,E ", "relevance_score": 57.06, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "SAMPO achieves state-of-the-art performance in action-conditioned video prediction with 4.4√ó faster inference compared to existing autoregressive world models. The model demonstrates competitive results across multiple benchmarks including BAIR, RoboNet, and 1X World Model datasets, while showing strong zero-shot generalization capabilities and effective scaling behavior with larger model sizes.", "description": "This paper presents SAMPO, a scale-wise autoregressive world model that combines visual autoregressive modeling for intra-frame generation with causal modeling for next-frame generation. The framework integrates temporal causal decoding with bidirectional spatial attention, uses an asymmetric multi-scale tokenizer to balance spatial detail and dynamic modeling, and introduces a trajectory-aware motion prompt module to enhance understanding of object-agent interactions in robotic manipulation tasks.", "key_contribution": "The main contribution is a hybrid autoregressive framework that preserves spatial locality through coarse-to-fine multi-scale generation while maintaining temporal consistency, combined with an asymmetric tokenizer and trajectory-aware motion prompts that significantly improve both generation quality and inference efficiency for world models.", "novelty": "Unlike existing autoregressive world models that use raster-scan flattening which disrupts spatial structure, SAMPO introduces next-scale spatial prediction combined with temporal causal generation to preserve spatial locality. The asymmetric multi-scale tokenizer addresses the distinct characteristics of observed versus future frames, while the trajectory-aware motion prompt explicitly conditions on object and robot trajectories to improve dynamic interaction modeling, addressing limitations in physical realism and temporal consistency.", "ai_categories": ["Video Understanding and Temporal Reasoning", "Embodied AI and Vision-Language-Action Models", "Reinforcement Learning for Multimodal Models"], "score": 57.06, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4224314, "title": "SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought", "authors": "Guanghao Li, Wenhao Jiang, Mingfeng Chen, Yan Li, Hao Yu, Shuting Dong, Tao Ren, Ming Tang, Chun Yuan", "pdf_url": "https://openreview.net/pdf/e31dcc29db400a0d8f2b23e4dca55cc1663442a1.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53809, "poster_number": 2805, "tag": "SD-2-2805 | Exhibit Hall C,D,E ", "relevance_score": 56.21, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "SCOUT enables pretrained LLMs to perform multi-step reasoning through recursive refinement without requiring additional pretraining or manually annotated chain-of-thought traces. The framework achieves up to 1.8% accuracy gains across eight reasoning benchmarks while improving both answer quality and explanation coherence. Progressive distillation with capacity-matched teachers at each iteration proves essential for effective recursive reasoning.", "description": "This paper introduces Flow Chain-of-Thought (Flow CoT), a reasoning paradigm that models recursive inference as a progressive trajectory of latent cognitive states, and SCOUT, a lightweight fine-tuning framework that implements Flow CoT. SCOUT uses progressive distillation where each reasoning iteration is supervised by a teacher model of matching capacity (1.5B‚Üí3B‚Üí7B), combined with a cross-attention-based retrospective module that integrates previous outputs while preserving the model's original computation flow.", "key_contribution": "The main contribution is SCOUT, a fine-tuning framework that enables Flow CoT-style recursive reasoning in pretrained LLMs through progressive distillation with iteration-specific teachers and a non-intrusive cross-attention retrospective module, achieving improved reasoning without pretraining or architectural modifications.", "novelty": "Unlike prior recursive reasoning methods that apply uniform supervision across all iterations or require costly pretraining, SCOUT introduces progressive distillation where each iteration is supervised by a capacity-matched teacher, preventing over-regularization in early steps. The cross-attention-based retrospective module allows selective integration of prior reasoning states without disrupting pretrained attention flows, unlike simple concatenation or additive fusion approaches. This provides a principled framework for how reasoning should evolve across iterations rather than treating recursion as black-box repetition.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation"], "score": 56.21, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4441343, "title": "CCL: Causal-aware In-context Learning for Out-of-Distribution Generalization", "authors": "Hoyoon Byun, Gyeongdeok Seo, Joonseong Kang, Taero Kim, Jihee Kim, Kyungwoo Song", "pdf_url": "https://openreview.net/pdf/6d184a907967073156ac2ab81fa7d47b72c88e03.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53512, "poster_number": 3819, "tag": "SD-2-3819 | Exhibit Hall C,D,E ", "relevance_score": 56.35, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "CCL demonstrates that selecting demonstration sets based on causal representations rather than surface-level similarity significantly improves out-of-distribution (OOD) generalization in large language models. The paper provides both theoretical guarantees showing that causal-similarity-based selection yields better parameter convergence and smaller prediction errors, and empirical validation across multiple benchmarks showing consistent improvements, particularly for smaller models like Llama-3.2-3B-IT where average accuracy improved by ~11%.", "description": "This paper proposes Causal-aware In-context Learning (CCL), a novel demonstration selection method for in-context learning that addresses the poor OOD performance of traditional ICL approaches. CCL employs a VAE-based causal representation learning technique to capture invariant causal features across different distributions, selecting demonstrations that share similar causal representations with the query rather than merely similar surface features. The method is validated on synthetic data, multilingual math reasoning (MGSM), and various NLP benchmarks.", "key_contribution": "The main contribution is the first framework to integrate causal representation learning into in-context learning, introducing a VAE-based method that learns domain-invariant causal representations (c) and domain-variant features (s) to construct demonstration sets that improve OOD generalization with theoretical guarantees on parameter convergence and prediction accuracy.", "novelty": "Unlike existing ICL methods that select demonstrations based on input similarity (which can fail under distribution shifts), CCL addresses the fundamental limitation by learning latent causal variables that remain invariant across environments. The paper provides theoretical analysis proving that input proximity can lead to large label discrepancies, while causal similarity ensures better convergence to true parameters. This is the first work to apply causal representation learning principles to the demonstration selection problem in ICL, moving beyond surface-level pattern matching to problem-level invariance.", "ai_categories": ["In-Context Learning and Few-Shot Adaptation", "Prompt Learning and Optimization"], "score": 56.35, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4213246, "title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models", "authors": "Zekai Zhao, Qi Liu, Kun Zhou, Zihan Liu, Yifei Shao, Zhiting Hu, Biwei Huang", "pdf_url": "https://openreview.net/pdf/2b62626f959a636707dfb61efe094e4dcd7bd32f.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53502, "poster_number": 1813, "tag": "SD-2-1813 | Exhibit Hall C,D,E ", "relevance_score": 54.39, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper discovers that a small set of high-impact activations in the last few layers of LLMs govern long chain-of-thought (CoT) reasoning abilities. By simply amplifying these activations and inserting 'wait' tokens, long CoT ability can be invoked without training, significantly increasing self-reflection rate and accuracy. The activation changes follow predictable patterns (sharp rise after trigger tokens, then exponential decay) that can be modeled with analytic functions.", "description": "The paper investigates internal mechanisms behind long chain-of-thought reasoning in LLMs through extensive empirical analysis of activation patterns. It proposes EELo-CoT, a training-free activation control method that uses contrastive examples to identify key activations and applies analytic functions to adjust their values during inference. Additionally, it introduces a parameter-efficient fine-tuning approach that trains only 1.51% of parameters while achieving performance comparable to full fine-tuning.", "key_contribution": "The main contribution is a training-free activation control technique that efficiently elicits long CoT reasoning by identifying and amplifying specific activations using predictable patterns, along with a parameter-efficient fine-tuning method that focuses on learning activation patterns in the last layer with minimal trainable parameters.", "novelty": "Unlike existing approaches that rely on costly reinforcement learning or supervised fine-tuning on distilled data, this work reveals that long-CoT ability already exists in base models and can be awakened through targeted activation intervention. It provides the first systematic analysis of activation dynamics underlying long CoT reasoning and demonstrates that only sparse, localized activations in the last few layers are responsible for this capability. The predictable activation patterns enable a simple, generalizable intervention strategy that works across different LLMs without model-specific training.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation", "Vision-Language Reasoning and Chain-of-Thought"], "score": 54.39, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4417519, "title": "Towards Self-Refinement of Vision-Language Models with Triangular Consistency", "authors": "Yunlong Deng, Guangyi Chen, Tianpei Gu, Lingjing Kong, Yan Li, Zeyu Tang, Kun Zhang", "pdf_url": "https://openreview.net/pdf/3acba55c4efdef3ee37a854031bdd972e9e31008.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53403, "poster_number": 4710, "tag": "SD-2-4710 | Exhibit Hall C,D,E ", "relevance_score": 54.95, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that Vision-Language Models possess inherent self-refinement capabilities, enabling them to autonomously generate high-quality supervised data from unlabeled images without external supervision. Using the Triangular Consistency principle, the framework achieves consistent improvements across multiple benchmarks (e.g., 3.5% on LLaVA-Wild, 2.1% on MMBench) without requiring human annotations or stronger teacher models. The work provides both theoretical analysis from a causal perspective and empirical validation showing that VLMs can learn from self-generated instructions.", "description": "The paper proposes a self-refinement framework for Vision-Language Models based on a Triangular Consistency principle, which states that within an image-query-answer triangle, any masked element should be consistently reconstructed. The framework involves three stages: (1) multi-task instruction tuning to enable instruction generation, (2) generating and filtering image-query-answer triplets using triangular consistency scores, and (3) retraining the model on filtered synthetic data. The approach is validated using LLaVA-1.5 as baseline across multiple benchmarks.", "key_contribution": "The main contribution is introducing the Triangular Consistency principle as a self-supervised quality measure for synthetic visual instructions, enabling VLMs to autonomously refine themselves using only unlabeled images without external supervision from humans or stronger models. The work includes theoretical justification from a causal perspective and demonstrates iterative self-improvement capabilities.", "novelty": "Unlike existing methods that rely on expensive proprietary models (GPT-4V, Gemini) or environmental feedback for supervision, this work enables VLMs to self-refine using only unlabeled images and the model's own capabilities. The Triangular Consistency principle provides a novel intrinsic quality measure by checking if masked elements in image-question-answer triplets can be consistently reconstructed. The approach addresses the bottleneck of requiring superior teacher models and high annotation costs while providing a theoretical foundation through causal analysis of language-image relationships.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization", "Hallucination Detection and Model Robustness"], "score": 54.95, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4441253, "title": "Towards Visualization-of-Thought Jailbreak Attack against Large Visual Language Models", "authors": "Hongqiong Zhong, Qingyang Teng, Baolin Zheng, Guanlin Chen, Yingshui Tan, Zhendong Liu, Jiaheng Liu, Wenbo Su, Xiaoyong Zhu, ..., Kaifu Zhang", "pdf_url": "https://openreview.net/pdf/98bc54bf489ff48cae9cd4c5cff483480722ea9c.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53635, "poster_number": 5319, "tag": "SD-2-5319 | Exhibit Hall C,D,E ", "relevance_score": 55.3, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces VoTA (Visualization-of-Thought Attack), achieving a remarkable 26.71% improvement in attack success rate (from 63.70% to 90.41%) across 15 VLMs compared to state-of-the-art methods. The research reveals a critical vulnerability: VLMs struggle to maintain safety guarantees when processing complex multimodal inputs requiring intricate reasoning, exposing an inherent conflict between logical reasoning capabilities and safety protocols. The attack is particularly effective against advanced commercial models like GPT-4o and Gemini, demonstrating that stronger logical capabilities may paradoxically increase vulnerability.", "description": "The paper presents a novel jailbreak attack framework against Visual Language Models that strategically constructs chains of images with risky visual thoughts combined with verbal thoughts (action sequences). The attack exploits the competition between VLMs' logical reasoning capabilities and safety alignment by decomposing risk scenarios into multi-step visual narratives using text-to-image synthesis, thereby provoking models to generate unsafe content. The framework is fully automated, requiring only a risk category as input, and operates in a black-box setting without requiring model access or iteration.", "key_contribution": "The main contribution is the VoTA framework that reveals and exploits a fundamental tension between logical reasoning and safety in VLMs through multimodal visualization-of-thought inputs. This is the first work to demonstrate that current VLMs cannot safely handle insecure multimodal reasoning chains, achieving state-of-the-art attack performance while exposing a previously unexplored vulnerability class in vision-language models.", "novelty": "Unlike previous attacks that use single images, typographic prompts, or role-playing scenarios, VoTA constructs dynamic multi-step malicious plans using synthesized image sequences that visualize risk implementation processes. The approach shifts from exploiting single visual cues to challenging procedural reasoning over logical chains, targeting the failure to maintain safety across complex cross-modal reasoning rather than simple prompt manipulation. This is the first attack to systematically leverage the gap between advancing logical capabilities and safety alignment in VLMs, demonstrating that stronger reasoning models are paradoxically more vulnerable.", "ai_categories": ["Hallucination Detection and Model Robustness", "Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization"], "score": 55.3, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4442315, "title": "MixPrompt: Efficient Mixed Prompting for Multimodal Semantic Segmentation", "authors": "Zhiwei Hao, Zhongyu Xiao, Jianyuan Guo, Li Shen, Yong Luo, Han Hu, Dan Zeng", "pdf_url": "https://openreview.net/pdf/5c3bb6a639ece979e6fcba7d959066150516cae4.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53826, "poster_number": 4802, "tag": "SD-2-4802 | Exhibit Hall C,D,E ", "relevance_score": 53.57, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "MixPrompt achieves state-of-the-art performance on multimodal semantic segmentation while using nearly half the parameters of two-branch baselines, demonstrating improvements of 4.3, 1.1, 0.4, and 1.1 mIoU on NYU Depth V2, SUN-RGBD, MFNet, and DELIVER datasets respectively. The method successfully integrates auxiliary modalities (depth, thermal, event, lidar) into a frozen pretrained RGB backbone through lightweight prompting modules, requiring only 5.7M trainable parameters compared to 181M+ in competing methods. The key insight is that early layers of pretrained RGB models can effectively extract meaningful features from auxiliary modalities, enabling efficient cross-modal fusion without modality-specific backbones.", "description": "This paper introduces MixPrompt, a prompting-based framework for multimodal semantic segmentation that integrates auxiliary modalities (depth, thermal, event, lidar) into a pretrained RGB segmentation model without architectural modifications. The approach uses a lightweight prompting module initialized from early layers of a pretrained RGB feature extractor, and employs multi-subspace alignment to fuse RGB and auxiliary features at each backbone layer while keeping the RGB backbone frozen during training.", "key_contribution": "The main contribution is a parameter-efficient multimodal fusion framework that achieves superior performance through: (1) reusing early layers of pretrained RGB backbones for auxiliary modality embedding, (2) multi-subspace alignment and mixing strategy that maximizes information transfer with minimal parameters, and (3) a training scheme that freezes the RGB backbone while only updating the lightweight prompting module and segmentation head.", "novelty": "Unlike existing two-branch methods that require parallel backbones and complex fusion modules, or prior prompting methods that use simple addition for feature fusion, MixPrompt introduces multi-subspace alignment with information mixing to better exploit complementary modality information. The work demonstrates that pretrained RGB models can effectively extract features from auxiliary modalities, eliminating the need for modality-specific pretrained extractors. This addresses the dual challenges of model complexity and data scarcity in multimodal segmentation by leveraging large-scale RGB pretraining while requiring minimal additional parameters.", "ai_categories": ["Prompt Learning and Optimization", "Visual Grounding and Spatial Reasoning"], "score": 53.57, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4427092, "title": "VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion", "authors": "Jaekyun Park, Hye Won Chung", "pdf_url": "https://openreview.net/pdf/e8964927372da08d17937b20a3df7acddc069b7f.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53518, "poster_number": 2708, "tag": "SD-2-2708 | Exhibit Hall C,D,E ", "relevance_score": 53.37, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "VIPAMIN identifies two critical failure modes in visual prompt tuning for self-supervised models: prompts exhibit near-uniform attention across tokens and their outputs collapse into the pretrained subspace. By addressing these issues through attention-guided matching and orthogonal subspace injection, VIPAMIN achieves state-of-the-art performance with 3.7% improvement on distribution-shifted tasks and 4.6% on natural tasks over full fine-tuning, while requiring only a single forward pass and lightweight operations.", "description": "This paper introduces VIPAMIN, a visual prompt initialization strategy for self-supervised Vision Transformers that enhances adaptation through two complementary modules: a matching module that aligns prompts with semantically coherent input regions, and an orthogonalizing module that injects novel representational directions beyond the pretrained subspace. The method is evaluated across 19 VTAB-1k tasks and 5 few-shot benchmarks using MoCo-v3 and MAE backbones.", "key_contribution": "The main contribution is a parameter-free, computationally efficient prompt initialization technique that addresses uniform attention and subspace collapse in visual prompt tuning through semantic token selection and orthogonal bias injection, achieving consistent improvements across diverse tasks and data-scarce settings without architectural modifications.", "novelty": "Unlike prior methods that add learnable modules (GatedPT, iVPT) or rely on expensive clustering (SPT), VIPAMIN modifies only initial prompt weights through two lightweight operations. It explicitly addresses the geometric relationship between prompts and the representation space, preventing collapse into the pretrained subspace while enabling semantic specialization. This approach is fundamentally different from prototype-based initialization, as it performs token-level selection and injects orthogonal directions to enhance representational diversity.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation"], "score": 53.37, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4257147, "title": "Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning", "authors": "Ankan Deria, Adinath Madhavrao Dukre, Feilong Tang, Sara Atito, Sudipta Roy, Muhammad Awais, Muhammad Haris Khan, Imran Razzak", "pdf_url": "https://openreview.net/pdf/8d9df6d290dbe7ca260a923b4f2357a4855ede26.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53643, "poster_number": 5013, "tag": "SD-2-5013 | Exhibit Hall C,D,E ", "relevance_score": 53.34, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "ViMaR achieves over 4√ó speedup compared to existing value-guided methods while generating significantly more reliable and factually accurate captions. The method demonstrates strong cross-model generalization, with a value model trained on LLaVA-Mistral-7B effectively guiding stronger unseen models like LLaVA-OneVision-Qwen2-7B. When used for self-training, ViMaR-generated captions yield an average 15.87% improvement across visual comprehension benchmarks and reduce hallucination rates by 30.87%.", "description": "This paper introduces ViMaR, a two-stage inference framework for vision-language models that combines temporal-difference value learning with margin-based reward adjustment. The first stage performs a single pass to identify the highest-value caption among diverse candidates, while the second stage selectively refines only weakly grounded segments. The framework integrates a calibrated margin-based penalty to discourage low-confidence continuations while preserving descriptive richness.", "key_contribution": "The main contribution is a computationally efficient two-stage value-guided inference framework that reduces inference time by 4√ó while improving caption quality through targeted refinement and margin-based reward adjustment. The method enables self-improving VLM pipelines by generating high-quality captions suitable for self-training.", "novelty": "Unlike existing value-guided methods like VisVM that re-score all candidates at each generation step (O(N√óS) overhead), ViMaR performs targeted refinement only on under-grounded segments after an initial best-of pass. The novel margin-based reward adjustment (penalizing CLIP similarity scores below a calibrated threshold) addresses the limitation of unpenalized low-confidence generations that lead to hallucinations. The framework demonstrates unprecedented cross-model generalization, working effectively on models it wasn't trained on.", "ai_categories": ["Reinforcement Learning for Multimodal Models", "Hallucination Detection and Model Robustness", "Vision-Language Reasoning and Chain-of-Thought"], "score": 53.34, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4483875, "title": "Robo2VLM: Improving Visual Question Answering using Large-Scale Robot Manipulation Data", "authors": "Kaiyuan Eric Chen, Shuangyu Xie, Zehan Ma, Pannag Sanketi, Ken Goldberg", "pdf_url": null, "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 54000, "poster_number": 2205, "tag": "SD-2-2205 | Exhibit Hall C,D,E ", "relevance_score": 53.17, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "<cite index=\"1-4,2-5\">This paper explores the reverse paradigm of using rich, real, multi-modal robot trajectory data to enhance and evaluate Vision-Language Models (VLMs)</cite>, rather than using VLMs to assist robots. <cite index=\"1-6,2-7\">Robo2VLM derives ground-truth from non-visual sensory modalities such as end-effector pose, gripper aperture, and force sensing</cite>, enabling automatic generation of grounded VQA questions. <cite index=\"1-17,3-12\">Results suggest that Robo2VLM-1 can benchmark and improve VLM capabilities in spatial and interaction reasoning</cite>.", "description": "<cite index=\"1-5,2-6\">Robo2VLM is a Visual Question Answering (VQA) dataset generation framework for VLMs</cite> that leverages real robot manipulation data. <cite index=\"1-7,1-8\">The framework segments robot trajectories into manipulation phases and uses scene and interaction understanding to identify 3D properties of the robot, task goal, and target object</cite>. <cite index=\"1-15\">These properties generate VQA queries based on spatial, goal-conditioned, and interaction reasoning question templates</cite>.", "key_contribution": "<cite index=\"1-1,2-13\">The paper curates Robo2VLM-1, a large-scale in-the-wild dataset with 684,710 questions covering 463 distinct scenes and 3,396 robotic manipulation tasks from 176k real robot trajectories</cite>. <cite index=\"7-15\">The framework leverages rich, multi-modal data implicitly available in real robot trajectories to automatically generate VQA questions grounded in physical reality</cite>.", "novelty": "<cite index=\"7-13\">The paper addresses the challenge that VLMs often lack fine-grained spatial and interaction understanding crucial for robotics, which is not sufficiently present in standard web-scale image-text datasets</cite>. <cite index=\"7-14,7-15\">Unlike simulation data with sim-to-real gaps or expensive manual labeling, Robo2VLM leverages proprioceptive, kinematic, and force-torque data from real robot trajectories to automatically generate grounded VQA questions</cite>. <cite index=\"2-12\">The framework is applied to 176k diverse, real-world trajectories from the Open X-Embodiment dataset, producing over 3 million VQA samples</cite>.", "ai_categories": ["Embodied AI and Vision-Language-Action Models", "Visual Grounding and Spatial Reasoning", "Multimodal Benchmarks and Evaluation"], "score": 53.17, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4268854, "title": "SMMILE: An expert-driven benchmark for multimodal medical in-context learning", "authors": "Melanie Rieff, Maya Varma, Ossian Rabow, Subathra Adithan, Julie Kim, Ken Chang, Hannah Lee, Nidhi Rohatgi, Christian Bluethgen, ..., Michael Moor", "pdf_url": "https://arxiv.org/pdf/2506.21355", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53312, "poster_number": 4601, "tag": "SD-2-4601 | Exhibit Hall C,D,E ", "relevance_score": 53.14, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Current multimodal large language models (MLLMs) demonstrate poor in-context learning capabilities in medical settings, with even the best models (GPT-4o, Qwen2.5-VL-72B) achieving only ~50% accuracy. MLLMs are highly susceptible to irrelevant examples (single noisy example can degrade performance by 9.5%) and exhibit strong recency bias, where placing relevant examples last can improve performance by up to 71%. Domain-specific medical models do not significantly outperform general-purpose models of comparable size.", "description": "This paper introduces SMMILE, the first expert-driven benchmark for evaluating multimodal in-context learning in medical tasks, consisting of 111 problems (517 question-image-answer triplets) across 6 medical specialties and 13 imaging modalities. Eleven medical experts curated problems with multimodal queries and task-specific in-context examples designed as relevant demonstrations. The benchmark evaluates 15 state-of-the-art MLLMs on both open-ended and closed-ended tasks, revealing critical limitations in current models' ability to learn from multimodal medical context.", "key_contribution": "The main contribution is SMMILE, the first expert-curated benchmark specifically designed to evaluate multimodal in-context learning capabilities of MLLMs in the medical domain, along with comprehensive analysis revealing that current MLLMs struggle with medical ICL, showing only 8% average improvement over zero-shot performance and exhibiting vulnerabilities to example quality and ordering.", "novelty": "Unlike prior medical VQA benchmarks that use randomly selected few-shot examples, SMMILE provides expert-designed task demonstrations that are intentionally curated to support learning. This is the first benchmark to systematically evaluate multimodal ICL in medicine with expert-annotated problems graded by difficulty. The work reveals previously unknown vulnerabilities in MLLMs, including susceptibility to irrelevant examples and strong recency bias in medical contexts, providing insights into effective in-context example selection.", "ai_categories": ["In-Context Learning and Few-Shot Adaptation", "Multimodal Benchmarks and Evaluation"], "score": 53.14, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4442333, "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task", "authors": "Sunqi Fan, Jiashuo Cui, Meng-Hao Guo, Shuojin Yang", "pdf_url": "https://openreview.net/pdf/c3fe7507152ba55f4cdc2d606716d226689509d6.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53218, "poster_number": 1809, "tag": "SD-2-1809 | Exhibit Hall C,D,E ", "relevance_score": 50.06, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces STAR (Spatiotemporal Reasoning Framework), which achieves an 8.2% improvement on VideoMME and 4.6% on LongVideoBench by augmenting GPT-4o with lightweight tools. The framework progressively localizes 3D Regions of Interest (3D RoI) through alternating temporal and spatial tool invocation, addressing the spatiotemporal reasoning limitations of existing MLLMs while significantly reducing computational costs (from 6-8 minutes to 15.8 seconds compared to 72B models).", "description": "The paper presents a comprehensive Video Toolkit with 22 tools and a novel reasoning framework (STAR) for Video Question Answering tasks. The framework strategically schedules temporal and spatial tools in an interleaved manner to progressively narrow down key regions in videos, enabling more accurate and efficient video understanding. The approach combines lightweight specialized models with LLM-based planning to overcome the limitations of existing Video-LLMs and tool-augmented approaches.", "key_contribution": "The main contribution is the STAR framework that enforces spatiotemporal-interleaved toolchain execution, preventing toolchain shortcuts and enabling progressive 3D RoI localization. This is complemented by a comprehensive, extensible Video Toolkit that balances tool quantity and diversity across temporal, spatial, and general-purpose categories.", "novelty": "Unlike existing tool-augmented LLMs that focus on either spatial or temporal dimensions independently, STAR enforces alternating invocation of temporal and spatial tools to enable mutual feedback between dimensions. The framework addresses three key limitations: unidimensional tool utilization, unbalanced tool diversity, and insufficient scheduling strategies. It introduces the concept of progressive 3D RoI localization and avoids the toolchain shortcut problem where LLMs bypass multi-step reasoning by directly invoking general-purpose tools.", "ai_categories": ["Video Understanding and Temporal Reasoning", "Vision-Language Reasoning and Chain-of-Thought", "Visual Grounding and Spatial Reasoning"], "score": 50.06, "session_type": "San Diego Poster Session 2"}, {"paper_id": 3985052, "title": "Face-Human-Bench: A Comprehensive Benchmark of Face and Human Understanding for Multi-modal Assistants", "authors": "lixiong Qin, Shilong Ou, Miaoxuan Zhang, Jiangning Wei, Yuhang Zhang, Xiaoshuai Song, ÂàòÂÆáÊô®, Mei Wang, Weiran Xu", "pdf_url": "https://openreview.net/pdf/ea956a6c039f0b611dcf0d08e7fd7196fd22b665.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 54049, "poster_number": 4501, "tag": "SD-2-4501 | Exhibit Hall C,D,E ", "relevance_score": 51.44, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces Face-Human-Bench, the first comprehensive benchmark specifically designed to evaluate multimodal large language models' (MLLMs) face and human understanding abilities across 18 fine-grained tasks. The evaluation of 25 mainstream MLLMs reveals that the best open-source model (InternVL-Chat-v1.2-Plus) outperforms the best closed-source model (GPT-4o) under zero-shot settings, and identifies specific tasks (deepfake detection, crowd counting, challenging face recognition scenarios) where specialist models significantly outperform MLLMs. The study also discovers that Chain-of-Thought prompting substantially improves closed-source models but has minimal effect on open-source models.", "description": "The paper presents Face-Human-Bench, a hierarchical benchmark with 2,700 problems (900 development, 1800 test) supporting both English and Chinese, built from 16 public datasets using a semi-automatic pipeline. The benchmark evaluates MLLMs across three levels: L1 (face/human understanding and perception/reasoning), L2 (10 abilities including facial attributes, age estimation, expression recognition, face attack detection, face recognition, human attributes, action recognition, spatial relations, social relations, and person re-identification), and L3 (18 fine-grained abilities). The evaluation explores correlation between abilities, impact of target position, and effectiveness of CoT prompting.", "key_contribution": "The main contribution is the creation of the first dedicated benchmark for evaluating face and human understanding in MLLMs, based on a three-level hierarchical ability taxonomy, along with comprehensive evaluation insights including identification of which specialist models can enhance MLLM performance and analysis of factors affecting model performance such as relative position sensitivity and CoT prompting effectiveness.", "novelty": "Unlike existing general MLLM benchmarks that include limited face and human understanding tasks (celebrity recognition, action recognition), this work provides the first comprehensive and scientific evaluation specifically focused on face and human understanding with 18 fine-grained abilities. It addresses the gap by leveraging datasets from the face and human community through a semi-automatic pipeline, and uniquely explores which specialist models should supplement MLLMs for specific tasks, bridging multimodal assistants with domain-specific models. The work also introduces novel metrics like relative position sensitivity score (RPSS) to measure robustness.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought"], "score": 51.44, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4220407, "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models", "authors": "Yi Ding, Ruqi Zhang", "pdf_url": "https://openreview.net/pdf/2cca74fc9d7ace7039b552ae0c9713648f96865a.pdf", "session_id": 541, "session_name": "San Diego Poster Session 2", "poster_id": 53368, "poster_number": 4014, "tag": "SD-2-4014 | Exhibit Hall C,D,E ", "relevance_score": 52.1, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Sherlock demonstrates that existing reasoning VLMs trained with SFT or RL cannot effectively self-correct their errors, with self-correction occurring in fewer than 10% of cases. The framework achieves state-of-the-art performance (65.4% average accuracy after self-correction) using only 20k annotated examples, significantly less than competing methods (LLaVA-CoT: 100k, Mulberry: 260k), while enabling continuous self-improvement without external supervision in the online stage.", "description": "This paper introduces Sherlock, a training framework that teaches Vision-Language Models to perform self-correction in reasoning tasks. The approach consists of three stages: SFT cold-start, offline preference training with trajectory-level objectives, and online iterative self-improvement. Unlike existing methods, Sherlock focuses on correcting only erroneous reasoning steps rather than regenerating entire responses, using visual perturbation for preference data construction and dynamic Œ≤ for stable training.", "key_contribution": "The main contribution is a trajectory-level self-correction objective that enables VLMs to revise only incorrect reasoning steps rather than entire responses, combined with a self-improvement framework that leverages the natural preference relationship between original and corrected responses to continue learning without external supervision.", "novelty": "Unlike prior self-correction approaches that require large-scale critique data or verifiers, Sherlock introduces fine-grained trajectory-level correction that targets specific erroneous steps. The framework addresses the limitation that existing reasoning VLMs cannot self-correct by designing a preference learning objective that explicitly models correction behavior, using visual perturbation to construct controllable preference pairs. The online stage enables self-improvement through self-generated preference data without ground truth labels, making it significantly more data-efficient than existing methods.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Hallucination Detection and Model Robustness"], "score": 52.1, "session_type": "San Diego Poster Session 2"}, {"paper_id": 4215801, "title": "Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model", "authors": "Tianle Li, Jihai Zhang, Yongming Rao, Yu Cheng", "pdf_url": "https://openreview.net/pdf/64ee56c0512b904e0aebcd799494fb96e7bdedba.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 55025, "poster_number": 4701, "tag": "SD-3-4701 | Exhibit Hall C,D,E ", "relevance_score": 93.42, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper reveals that while RL-trained VLMs outperform SFT models in compositional generalization, they still struggle significantly with cross-modal and cross-task reasoning. The study identifies a critical compositional ability gap in current VLMs and demonstrates that explicit visual-to-text grounding (caption-before-thinking) combined with progressive rewards (RL-Ground) substantially improves compositional reasoning, achieving up to 52.8% accuracy on compositional tasks compared to 31.2% for standard RL.", "description": "This paper conducts a systematic study evaluating whether vision-language models (VLMs) trained with reinforcement learning can compose capabilities across modalities and tasks. The authors introduce ComPABench, a diagnostic benchmark with carefully designed tasks that isolate cross-modal composition, cross-task reasoning, and out-of-distribution generalization. Through comprehensive experiments comparing SFT, RL, and hybrid approaches, they analyze compositional limitations and propose RL-Ground as a solution.", "key_contribution": "The main contribution is ComPABench, a diagnostic benchmark for evaluating compositional generalization in VLMs, along with the identification of a significant compositional ability gap in current training strategies. The paper also introduces RL-Ground, which combines caption-before-reasoning prompting with progress rewards to improve visual grounding and compositional reasoning performance.", "novelty": "Unlike previous work that focuses on individual task performance or text-only compositional reasoning in LLMs, this paper systematically probes compositional abilities across modalities (text-to-vision transfer) and tasks (integrating multiple reasoning skills) in VLMs. It addresses the underexplored question of whether RL-based reasoning capabilities transfer from LLMs to VLMs, revealing fundamental limitations in current approaches and proposing a novel solution that enforces visual-to-text alignment before reasoning.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Multimodal Benchmarks and Evaluation", "Compositional and Counterfactual Reasoning"], "score": 93.42, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4442488, "title": "Towards Unified Multimodal Interleaved Generation via Group Relative Policy Optimization", "authors": "Ming Nie, Chunwei Wang, Jianhua Han, Hang Xu, Li Zhang", "pdf_url": "https://openreview.net/pdf/be81012c4b8d2224b305eb1f109bd4f030fe2ac2.pdf", "session_id": 542, "session_name": "Mexico City Poster Session 2", "poster_id": 54072, "poster_number": 999, "tag": "MC-2-999 | Foyer ", "relevance_score": 58.83, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that unified vision-language models can generate high-quality multimodal interleaved text-image outputs using minimal training data (0.3M samples) through a two-stage approach. The proposed Group Relative Policy Optimization (GRPO) framework with hybrid rewards (textual, visual, format) and process-level supervision significantly improves interleaved generation quality, achieving 59.5% on MMIE and 3.13 on InterleavedBench. The method preserves the model's original multimodal understanding and generation capabilities while unlocking new interleaved generation abilities.", "description": "The paper proposes a reinforcement learning-based post-training strategy to enable unified vision-language models to generate coherent multimodal interleaved outputs (alternating text and images). It introduces a warm-up stage using limited curated data to expose models to interleaved patterns, followed by a unified GRPO framework that treats multimodal generation as a single decision-making process. The approach uses hybrid rewards covering textual relevance, visual-text alignment, structural fidelity, and process-level rewards for step-wise guidance.", "key_contribution": "The main contribution is a unified policy optimization framework extending GRPO to multimodal settings, enabling seamless text-image interleaved generation within a single autoregressive trajectory. This is achieved through hybrid reward signals and process-level supervision, without requiring large-scale interleaved training datasets.", "novelty": "Unlike previous methods that apply separate optimization for text and image or rely on extensive interleaved datasets, this work treats multimodal generation as a unified sequential decision process with a single decoding trajectory. It addresses the limitation of existing unified models that struggle with modality transitions by introducing process-level rewards for step-wise guidance and hybrid rewards that jointly optimize across modalities. The approach demonstrates that minimal interleaved supervision (0.3M samples) can unlock latent capabilities in pretrained unified models.", "ai_categories": ["Reinforcement Learning for Multimodal Models", "Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization"], "score": 58.83, "session_type": "Mexico City Poster Session 2"}, {"paper_id": 4407399, "title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning", "authors": "Honglin Lin, Qizhi Pei, Zhuoshi Pan, Yu Li, Xin Gao, Juntao Li, Conghui He, Lijun Wu", "pdf_url": "https://openreview.net/pdf/b5f91855f5922ea89b810ed1560821c78f774fff.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54183, "poster_number": 200, "tag": "SD-3-200 | Exhibit Hall C,D,E ", "relevance_score": 86.53, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Caco introduces a scalable framework for generating high-quality, verifiable reasoning data by converting natural language problems into executable code, then back-translating to create diverse instruction-CoT pairs. Models trained on Caco-1.3M achieve 92.6% on GSM8K and 82.4% on MATH, outperforming strong baselines by significant margins. The code-anchored verification ensures logical correctness while enabling automated, large-scale synthesis without human intervention.", "description": "This paper presents Caco (Code-Assisted Chain-of-ThOught), a framework that automates the synthesis of mathematical reasoning data by first unifying problems into executable Python code, then training a code generator (CodeGen) to produce diverse code-based reasoning traces. These code solutions are validated through execution, filtered for quality, and reverse-engineered into natural language instructions and language CoTs, creating a dataset of 1.3M verified reasoning pairs.", "key_contribution": "The main contribution is a fully automated pipeline that leverages executable code as an intermediate representation to generate verifiable, diverse, and scalable chain-of-thought reasoning data, with automatic validation through code execution ensuring both answer correctness and reasoning step validity.", "novelty": "Unlike prior code-assisted reasoning methods constrained to predefined mathematical problems, Caco scales code generation to diverse reasoning patterns through fine-tuning an unconditional code generator and sampling varied solutions. It addresses the unverifiability and scalability limitations of natural language CoT by grounding reasoning in executable code with automated validation, while introducing both problem-level and pattern-level augmentation for enhanced diversity. The bidirectional translation between code and natural language enables self-sustaining data generation without manual annotation.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization"], "score": 86.53, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4271848, "title": "Visual Structures Help Visual Reasoning:  Addressing the Binding Problem in LVLMs", "authors": "Amirmohammad Izadi, Mohammadali Banayeeanzade, Fatemeh Askari, Ali Rahimiakbar, Mohammad Mahdi Vahedi, Hosein Hasani, Mahdieh Soleymani Baghshah", "pdf_url": "https://openreview.net/pdf/9aa6856102d8ec332673e3b3497a2c653885bfee.pdf", "session_id": 542, "session_name": "Mexico City Poster Session 2", "poster_id": 54096, "poster_number": 999, "tag": "MC-2-999 | Foyer ", "relevance_score": 88.49, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "VISER demonstrates that adding simple visual structures (horizontal lines) to images combined with sequential scanning prompts substantially improves LVLM performance on visual reasoning tasks. The method achieves 25.0%, 26.8%, and 9.5% improvements on visual search, counting, and spatial relationship tasks respectively for GPT-4o, while requiring only single-query inference. Critically, the research shows that purely textual interventions like Chain-of-Thought prompting are insufficient and can even degrade performance, establishing that visual input modification is essential for addressing the binding problem.", "description": "This paper addresses the binding problem in Large Vision-Language Models (LVLMs)‚Äîthe failure to reliably associate visual features with their correct objects‚Äîwhich limits performance on counting, visual search, scene description, and spatial reasoning tasks. The authors propose VISER (Visual Input Structure for Enhanced Reasoning), a simple method that augments images with low-level spatial structures (horizontal lines with row annotations) paired with textual prompts encouraging sequential, spatially-aware parsing. The approach is evaluated across multiple models (GPT-4o, Claude-3.5, LLaMA4, Qwen2.5-VL) on both synthetic and real-world datasets.", "key_contribution": "The main contribution is VISER, a training-free, model-agnostic method that combines explicit visual scaffolding with targeted textual prompts to guide sequential attention in LVLMs. This approach addresses the binding problem by encouraging region-by-region processing, achieving substantial performance gains across core visual reasoning tasks without requiring fine-tuning, multi-query processing, or external tools.", "novelty": "Unlike prior work that relies on purely linguistic reasoning strategies (e.g., Chain-of-Thought) or complex agentic approaches with external tools, VISER demonstrates that simple visual input modifications are both necessary and sufficient for improving feature-object binding. The work provides empirical evidence that textual prompting alone fails to resolve binding failures, and that visual structuring acts as a 'visual analogue' to CoT by injecting spatial inductive bias directly into the input. This challenges the prevailing focus on linguistic reasoning strategies and establishes visual input design as a critical factor for compositional reasoning in LVLMs.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Vision-Language Reasoning and Chain-of-Thought", "Compositional and Counterfactual Reasoning"], "score": 88.49, "session_type": "Mexico City Poster Session 2"}, {"paper_id": 4394974, "title": "CoFFT: Chain of Foresight-Focus Thought for Visual Language Models", "authors": "Xinyu Zhang, Yuxuan Dong, Lingling Zhang, Chengyou Jia, Zhuohang Dang, Basura Fernando, Jun Liu, Mike Zheng Shou", "pdf_url": "https://openreview.net/pdf/cde471b7df444cf85c986002216423b046c56f8d.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54676, "poster_number": 4708, "tag": "SD-3-4708 | Exhibit Hall C,D,E ", "relevance_score": 86.17, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "CoFFT achieves consistent performance improvements of 3.1-5.8% across multiple visual reasoning benchmarks without requiring model retraining. The approach demonstrates that iteratively refining visual focus based on reasoning progression significantly reduces hallucinations and improves fine-grained visual understanding. Performance gains scale positively with model size, with larger models benefiting more from the approach.", "description": "This paper presents Chain of Foresight-Focus Thought (CoFFT), a training-free approach that enhances Vision Language Models' visual reasoning by emulating human visual cognition. The method iteratively cycles through three stages: generating diverse reasoning samples, evaluating them through dual foresight decoding (considering both visual focus and reasoning progression), and dynamically adjusting visual focus to regions most relevant for subsequent reasoning steps.", "key_contribution": "The main contribution is a novel training-free framework that creates a synergistic cycle where reasoning guides visual focus and visual focus informs reasoning, enabled by two key innovations: Dual Foresight Decoding for evaluating reasoning samples based on both visual relevance and logical progression, and Visual Focus Adjustment for dynamically cropping and magnifying image regions relevant to future reasoning steps.", "novelty": "Unlike existing multi-modal chain-of-thought methods that either rely on specialized visual expert models or require extensive retraining, CoFFT operates training-free while enabling dynamic visual manipulation throughout the reasoning process. It addresses VLMs' tendency to be distracted by visually salient but semantically irrelevant information by implementing human-like foresight evaluation of which visual regions will be valuable for future reasoning, rather than processing images only once or using fixed attention windows.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Visual Grounding and Spatial Reasoning", "Hallucination Detection and Model Robustness"], "score": 86.17, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4442537, "title": "VaMP: Variational Multi-Modal Prompt Learning for Vision-Language Models", "authors": "Silin Cheng, Kai Han", "pdf_url": "https://openreview.net/pdf/3ec902e63d7caa0a34c58304a519e30a791073d1.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54155, "poster_number": 4711, "tag": "SD-3-4711 | Exhibit Hall C,D,E ", "relevance_score": 85.89, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "VaMP achieves state-of-the-art performance on few-shot and domain generalization benchmarks by introducing variational inference to multi-modal prompt learning. The method generates instance-conditioned prompts from learned posterior distributions, enabling uncertainty-aware adaptation that improves generalization to novel classes by 1.51% over previous methods. The class-aware prior construction provides semantic regularization that enhances discriminative capacity while maintaining parameter efficiency.", "description": "This paper proposes Variational Multi-Modal Prompt Learning (VaMP), a framework that enables sample-specific, uncertainty-aware prompt tuning for vision-language models like CLIP. VaMP treats prompt tokens as latent variables sampled from learned posterior distributions conditioned on input images, and introduces a class-aware prior derived from instance representations and class prototypes to enhance integration of local and global semantics across multiple transformer layers.", "key_contribution": "The main contribution is a variational framework for multi-modal prompt learning that combines three innovations: (1) token-wise variational modeling across multiple layers to capture fine-grained semantic relationships, (2) multi-modal posterior inference incorporating both visual and textual signals, and (3) class-aware priors that generate more discriminative prompts compared to standard Gaussian priors.", "novelty": "Unlike existing prompt learning methods that use fixed, shared prompts applied uniformly across samples, VaMP introduces probabilistic, instance-conditioned prompts that model uncertainty at the token level across multiple layers. This addresses key limitations of prior work like Bayesian Prompt Learning and Any-Shift Prompting, which only apply variational modeling to text-only prompts with global latent variables and fail to capture hierarchical feature interactions or leverage visual information for cross-modal alignment.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation"], "score": 85.89, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4213219, "title": "Towards General Continuous Memory for Vision-Language Models", "authors": "Wenyi WU, Zixuan Song, Kun Zhou, Yifei Shao, Zhiting Hu, Biwei Huang", "pdf_url": "https://openreview.net/pdf/eb3b13d0293bfe8c1b60b497e2634c96c134f57c.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54923, "poster_number": 4809, "tag": "SD-3-4809 | Exhibit Hall C,D,E ", "relevance_score": 85.28, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that Vision-Language Models (VLMs) can effectively serve as their own continuous memory encoders, compressing multimodal knowledge into just 8 dense embeddings while achieving significant performance improvements (+8.0% on average across benchmarks). The approach requires minimal training (only 1.2% of parameters on 15.6K self-synthesized samples) yet generalizes well across diverse multimodal and multilingual reasoning tasks, outperforming traditional RAG methods that suffer from context length explosion.", "description": "The paper introduces CoMEM (Continuous Memory), a method that uses VLMs as their own memory encoders to compress external multimodal and multilingual knowledge into compact continuous embeddings. Rather than concatenating retrieved image-text pairs as long token sequences (as in RAG), CoMEM employs a lightweight Q-Former to compress each knowledge item into 8 continuous embeddings, which are then plug-and-play integrated with frozen VLMs during inference for enhanced reasoning.", "key_contribution": "The main contribution is a data-efficient and parameter-efficient method to fine-tune VLMs into continuous memory encoders that compress arbitrary multimodal/multilingual knowledge into just 8 embeddings per item, achieving superior performance over RAG methods while avoiding context length explosion and maintaining plug-and-play compatibility with frozen inference models.", "novelty": "Unlike existing RAG methods that concatenate discrete tokens and cause context length explosion, or context compression methods that lose information through token pruning, this work leverages the VLM's own representations as continuous memory with minimal training. The key insight is that VLMs can encode their own memory through cached hidden states, requiring only a small Q-Former (1.2% parameters) trained on 15.6K self-synthesized samples to achieve strong generalization across multimodal and multilingual tasks, addressing both efficiency and effectiveness limitations of prior approaches.", "ai_categories": ["In-Context Learning and Few-Shot Adaptation", "Prompt Learning and Optimization", "Multimodal Benchmarks and Evaluation"], "score": 85.28, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4130715, "title": "Video-R1: Reinforcing Video Reasoning in MLLMs", "authors": "Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, Xiangyu Yue", "pdf_url": "https://openreview.net/pdf/6f889c461f4ed89171060b9ea269b24978cfed1f.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54300, "poster_number": 5404, "tag": "SD-3-5404 | Exhibit Hall C,D,E ", "relevance_score": 72.72, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Video-R1 is the first systematic exploration of the R1 reinforcement learning paradigm for video reasoning in multimodal large language models. The proposed T-GRPO algorithm explicitly encourages temporal reasoning by contrasting model performance on ordered versus shuffled video frames. Video-R1-7B achieves 37.1% accuracy on VSI-Bench, surpassing GPT-4o, demonstrating that RL can unlock complex temporal reasoning capabilities in MLLMs similar to breakthroughs in text-based reasoning.", "description": "This paper introduces Video-R1, a framework that applies reinforcement learning to enhance video reasoning capabilities in multimodal large language models. The work addresses two key challenges: lack of temporal modeling in existing RL methods and scarcity of high-quality video reasoning data. The authors propose T-GRPO (Temporal Group Relative Policy Optimization) and construct two datasets combining image and video reasoning samples for training.", "key_contribution": "The main contribution is T-GRPO, a novel RL algorithm that encourages temporal reasoning by rewarding models only when they perform better on temporally ordered frames compared to shuffled ones. Additionally, the paper contributes Video-R1-CoT-165k and Video-R1-260k datasets that strategically combine image and video reasoning data to overcome data scarcity.", "novelty": "Unlike previous RL approaches that lack explicit temporal modeling, T-GRPO introduces a contrastive reward mechanism that explicitly encourages models to leverage temporal information rather than taking shortcuts based on single frames. The work addresses the limitation of applying standard GRPO to video by incorporating temporal awareness into the reward structure. It also innovatively combines image-based reasoning data with video data to bootstrap general reasoning abilities while training temporal understanding.", "ai_categories": ["Video Understanding and Temporal Reasoning", "Reinforcement Learning for Multimodal Models", "Vision-Language Reasoning and Chain-of-Thought"], "score": 72.72, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4205261, "title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO", "authors": "Yicheng Xiao, Lin Song, Yukang Chen, Yingmin Luo, Yuxin Chen, Yukang Gan, Wei Huang, Xiu Li, XIAOJUAN QI, Ying Shan", "pdf_url": "https://openreview.net/pdf/89e0ca00b9f6b5998d606e3a0c50147368b03e0a.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54398, "poster_number": 4800, "tag": "SD-3-4800 | Exhibit Hall C,D,E ", "relevance_score": 85.84, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "MindOmni introduces Reasoning Generation Policy Optimization (RGPO), a novel reinforcement learning algorithm that enables vision-language models to generate explicit Chain-of-Thought reasoning for image generation tasks. The model achieves state-of-the-art performance with 71% on WISE reasoning generation benchmark and 83% on GenEval, significantly outperforming previous unified models while maintaining strong understanding capabilities (51.6% on MMMU, 83.2% on MMBench).", "description": "This paper presents MindOmni, a unified multimodal large language model that combines vision understanding and image generation with explicit reasoning capabilities. The model employs a three-phase training strategy: pretraining with a decoder-only diffusion module, supervised fine-tuning with Chain-of-Thought instruction data, and reinforcement learning using the proposed RGPO algorithm that leverages multimodal feedback signals from both image and text features to guide policy updates.", "key_contribution": "The main contribution is the RGPO (Reasoning Generation Policy Optimization) algorithm, which extends reinforcement learning to multimodal generation by incorporating dual KL divergence regularizers for text and visual rollouts, along with consistency and format reward functions that enable the model to generate accurate reasoning chains for complex image generation tasks involving mathematical logic, spatial-temporal perception, and world knowledge.", "novelty": "Unlike previous methods that use MLLMs merely as semantic feature extractors, MindOmni unleashes the inherent reasoning capabilities of LLMs for image generation through reinforcement learning. The RGPO algorithm differs from text-only GRPO by generating multimodal feedback signals and applying separate KL regularizers to text and image modalities, preventing knowledge forgetting while enabling explicit reasoning generation. This addresses limitations of existing unified models that struggle with reasoning-intensive generation tasks and lack interpretable thought processes.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Multimodal Benchmarks and Evaluation"], "score": 85.84, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4234254, "title": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning", "authors": "Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, Hongsheng Li", "pdf_url": "https://openreview.net/pdf/82857fa25f2da29b5e82ec3b3093429dd593aab4.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54221, "poster_number": 4913, "tag": "SD-3-4913 | Exhibit Hall C,D,E ", "relevance_score": 81.04, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "MINT-CoT introduces a novel approach to mathematical reasoning in multimodal LLMs by dynamically interleaving fine-grained visual tokens at the token level rather than relying on coarse box-shaped regions. The method achieves significant improvements over baselines: +34.08% on MathVista, +28.78% on GeoQA, and +23.2% on MMStar, demonstrating the effectiveness of token-level visual grounding in mathematical chain-of-thought reasoning.", "description": "This paper presents MINT-CoT, a framework that enables multimodal large language models to perform mathematical reasoning by adaptively selecting and interleaving relevant visual tokens into textual reasoning steps. The approach uses a special 'Interleave Token' that computes similarity scores to dynamically select visual regions of any shape within mathematical figures, accompanied by a 54K dataset with token-level annotations and a three-stage progressive training strategy combining supervised fine-tuning and reinforcement learning.", "key_contribution": "The main contribution is the MINT-CoT framework with its Interleave Token mechanism for fine-grained, token-level visual selection in mathematical reasoning, along with the MINT-CoT dataset containing 54K problems with token-level visual-textual alignment, and a progressive three-stage training strategy (text-only CoT SFT, interleaved CoT SFT, and interleaved CoT RL).", "novelty": "Unlike previous methods that rely on coarse-grained bounding boxes or external tools for visual modification, MINT-CoT performs token-level selection of visual regions of arbitrary shapes directly within the reasoning process. It addresses limitations of existing approaches including reliance on box-shaped regions, limited perception of vision encoders on mathematical content, and dependence on external capabilities. The method learns to adaptively select relevant visual tokens through trainable projectors and similarity scoring, enabling more precise visual grounding without requiring image generation or external tools.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization", "Multimodal Benchmarks and Evaluation"], "score": 81.04, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4219286, "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "authors": "Zhongxing Xu, Chengzhi Liu, Qingyue Wei, Juncheng Wu, James Zou, Xin Eric Wang, Yuyin Zhou, Sheng Liu", "pdf_url": "https://openreview.net/pdf/3843716274c768fa2caeeb1593f7865631cc0905.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 55021, "poster_number": 4700, "tag": "SD-3-4700 | Exhibit Hall C,D,E ", "relevance_score": 78.54, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Multimodal reasoning models that generate extended reasoning chains exhibit significantly increased hallucination rates compared to their non-reasoning counterparts, particularly on perception-focused tasks. Attention analysis reveals that longer reasoning chains systematically reduce visual attention allocation, causing models to drift toward language priors. The relationship between reasoning length and performance is non-monotonic, with optimal reasoning lengths varying across tasks.", "description": "This paper investigates the trade-off between reasoning ability and visual hallucination in multimodal large language models enhanced with test-time compute. The authors introduce RH-AUC, a metric to quantify how perception accuracy changes with reasoning length, and RH-Bench, a diagnostic benchmark with 1,000 samples covering diverse reasoning and perception tasks to evaluate the balance between reasoning quality and perceptual reliability.", "key_contribution": "The paper introduces RH-AUC, a novel metric that captures the dynamic balance between reasoning performance and hallucination across varying reasoning lengths, along with RH-Bench, a comprehensive diagnostic benchmark for jointly assessing reasoning ability and visual grounding in multimodal models.", "novelty": "Unlike prior work focusing solely on improving reasoning performance, this paper systematically reveals and quantifies the amplified hallucination problem in multimodal reasoning models through attention analysis. It demonstrates that longer reasoning chains reduce visual attention allocation, and proposes the first evaluation framework (RH-AUC) that accounts for the non-monotonic relationship between reasoning depth and perceptual accuracy. The work also shows that RL-only training produces better reasoning-hallucination balance than SFT+RL approaches.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Hallucination Detection and Model Robustness", "Multimodal Benchmarks and Evaluation"], "score": 78.54, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4433958, "title": "How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation", "authors": "Yang Zhao, Pu Wang, Hao Frank Yang", "pdf_url": "https://openreview.net/pdf/3069d87176e110b08bf8a1a84618385fcd354a5b.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54660, "poster_number": 2507, "tag": "SD-3-2507 | Exhibit Hall C,D,E ", "relevance_score": 78.07, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "EGO-Prompt achieves 7.32%-12.61% higher F1 scores than state-of-the-art methods on domain-specific tasks by automatically optimizing both prompts and Semantic Causal Graphs (SCGs) through evolutionary textual gradients. The framework enables small models like GPT-4o mini to match the performance of larger reasoning models (o4-mini, o1) at under 20% of the cost. It automatically refines expert-constructed causal graphs, correcting biases and discovering missing relationships while improving interpretability.", "description": "This paper introduces EGO-Prompt, an automated framework for optimizing prompts and reasoning processes for LLMs on domain-specific tasks by integrating expert knowledge through Semantic Causal Graphs. The approach decomposes graph-guided reasoning into two stages: generating instance-specific reasoning guidance from the SCG, then performing reasoning conditioned on that guidance. Both the SCG and prompts are iteratively refined using textual gradients from ground-truth data, tested on public health, transportation, and human behavior tasks.", "key_contribution": "The main contribution is a novel evolutionary optimization framework that jointly refines both prompts and domain-specific causal graphs through textual gradients, enabling automatic correction of incomplete or biased expert knowledge while improving LLM reasoning performance and cost-efficiency on domain tasks.", "novelty": "Unlike existing methods that rely on fixed, complete external knowledge graphs or databases, EGO-Prompt works with partial, imperfect expert-constructed SCGs and actively refines them during optimization. It addresses the limitation of one-way knowledge integration by enabling bidirectional feedback where the model corrects and enhances the causal graph structure. The decomposition into instance-specific reasoning guidance generation followed by conditioned reasoning is a novel approach that filters irrelevant information while preserving causal structure.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation"], "score": 78.07, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4149565, "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement", "authors": "Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, Lijuan Wang", "pdf_url": "https://openreview.net/pdf/758205547dc940c338296a99ef384666df7ca2ac.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54847, "poster_number": 4802, "tag": "SD-3-4802 | Exhibit Hall C,D,E ", "relevance_score": 76.24, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "ThinkLite-VL achieves state-of-the-art visual reasoning performance using only 11k training samples (7B model) and 7.5k samples (72B model)‚Äîan order of magnitude less than prior work‚Äîwithout any knowledge distillation. The 7B model achieves 75.1% on MathVista, surpassing GPT-4o and O1, while the 72B model reaches 79.7%. The key insight is that sample difficulty, measured via MCTS reasoning iterations, is critical for effective reinforcement fine-tuning in vision-language models.", "description": "This paper introduces ThinkLite-VL, a data-efficient visual reasoning framework that uses Monte Carlo Tree Search (MCTS) to identify appropriately challenging training samples for vision-language models. The method repurposes MCTS to measure sample difficulty by counting the reasoning iterations required to solve each instance, then applies reinforcement fine-tuning (RFT) on the filtered high-quality subset without any supervised fine-tuning or knowledge distillation from larger models.", "key_contribution": "The main contribution is a novel MCTS-guided sample selection method that identifies difficulty-appropriate training samples by measuring reasoning iteration count, enabling pure self-improvement through RFT without knowledge distillation. This achieves state-of-the-art performance with 85-90% less training data than existing approaches.", "novelty": "Unlike prior work that relies on extensive knowledge distillation from larger models (GPT-4o, DeepSeek-R1) and uses 59k-260k training samples, this work demonstrates that VLMs can self-improve using only difficulty-curated samples. The novel use of MCTS for offline sample difficulty estimation provides a more fine-grained signal than zero-shot accuracy filtering, capturing both solved and unsolved-but-informative cases. This is the first framework to combine search-based difficulty estimation with RFT for data-efficient visual reasoning.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models"], "score": 76.24, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4262114, "title": "CF-VLMÔºöCounterFactual Vision-Language Fine-tuning", "authors": "Jusheng Zhang, Kaitong Cai, Yijia Fan, Jian Wang, Keze Wang", "pdf_url": "https://openreview.net/pdf/7e5f14766c2d8126be08ddbafcf8266e3b134a68.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54753, "poster_number": 5208, "tag": "SD-3-5208 | Exhibit Hall C,D,E ", "relevance_score": 85.12, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "CF-VLM introduces a novel counterfactual fine-tuning framework that significantly improves vision-language models' causal reasoning and compositional understanding, achieving state-of-the-art results on benchmarks like ConMe (87.57% on Qwen-VL), ARO (93.2%), and VL-Checklist (90.57%). The framework demonstrates that training with minimal but semantically critical counterfactual edits enables models to identify causal decision points rather than relying on superficial correlations. Additionally, CF-VLM shows promise in mitigating visual hallucinations, improving factual consistency in high-stakes applications.", "description": "This paper proposes CF-VLM, a counterfactual vision-language fine-tuning framework that enhances VLMs through three complementary training objectives: maintaining cross-modal alignment, reinforcing factual scene uniqueness against complete counterfactual scenarios, and sharpening sensitivity to minimal causal edits. The approach generates counterfactual samples using SDXL 1.0 for images and Qwen2-72B-Instruct for text, creating both complete counterfactual scenarios and minimally edited samples that target specific attributes, objects, or causal relationships.", "key_contribution": "The main contribution is a comprehensive counterfactual learning framework with three synergistic loss functions (Lalign, Lcsd, Lfcd) that jointly optimize for cross-modal alignment, counterfactual scenario discrimination, and fine-grained causal discrimination. This enables VLMs to move beyond surface-level pattern matching to understand why minimal semantic changes affect image-text alignment, addressing the core limitation of existing models that rely on superficial statistical correlations.", "novelty": "Unlike prior work that focuses on either text-only counterfactuals (TripletCLIP, CPL) or image-only perturbations (Goyal et al., Rao et al.), CF-VLM uniquely handles both modalities simultaneously with complete jointly-edited scenarios and minimal targeted interventions. The framework explicitly models causal decision points through its fine-grained causal discrimination loss, teaching models to identify which specific attributes or relations determine semantic matching rather than just distinguishing matching from non-matching pairs. This represents a shift from coarse-grained discrimination to intervention-aware causal reasoning in vision-language understanding.", "ai_categories": ["Compositional and Counterfactual Reasoning", "Hallucination Detection and Model Robustness", "Vision-Language Reasoning and Chain-of-Thought"], "score": 85.12, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4430976, "title": "Towards Single-Source Domain Generalized Object Detection via Causal Visual Prompts", "authors": "Chen Li, Huiying Xu, Changxin Gao, Zeyu Wang, Yun Liu, Xinzhong Zhu", "pdf_url": "https://openreview.net/pdf/3086489f466993ba21652ce8a79637805cd873f9.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54177, "poster_number": 5009, "tag": "SD-3-5009 | Exhibit Hall C,D,E ", "relevance_score": 69.34, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that single-source domain generalized object detection models suffer from spurious correlations (e.g., over-reliance on color features), leading to poor generalization. The proposed Cauvis method achieves 15.9-31.4% performance gains over existing methods by using causal visual prompts with cross-attention that mathematically implement backdoor adjustment from causal inference. The dual-branch adapter with Fourier-based frequency decomposition successfully disentangles causal from spurious features without requiring multi-domain training data.", "description": "The paper addresses single-source domain generalized object detection (SDGOD), where models trained on one domain must generalize to unseen domains without cross-domain prior knowledge. It proposes Cauvis, which introduces visual prompts combined with cross-attention mechanisms to mitigate spurious correlations, and employs a dual-branch adapter that uses Fourier transforms to extract high-frequency domain-invariant features while suppressing domain-specific noise. The method is evaluated on autonomous driving datasets across various weather conditions.", "key_contribution": "The main contribution is the Cauvis framework that theoretically connects visual prompts with causal inference through cross-attention implementing backdoor adjustment, combined with a dual-branch adapter using Fourier analysis to explicitly disentangle causal and spurious features for improved domain generalization in object detection.", "novelty": "Unlike existing domain generalization methods that rely on data augmentation or heuristic constraints, this work provides a principled causal inference framework showing that cross-attention with visual prompts is mathematically equivalent to backdoor adjustment for eliminating spurious correlations. It is the first to introduce DINOv2 as a frozen backbone for SDGOD and explicitly addresses spurious correlations through frequency-domain feature decomposition. The approach differs from conventional visual prompting by treating prompts as pseudo-modal signals independent of image domains, enabling causal intervention without additional training samples.", "ai_categories": ["Prompt Learning and Optimization", "Visual Grounding and Spatial Reasoning", "Compositional and Counterfactual Reasoning"], "score": 69.34, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4441319, "title": "MemEIC: A Step Toward Continual and Compositional Knowledge Editing", "authors": "Jin Seong, Jiyun Park, Wencke Liermann, Hongseok Choi, Yoonji Nam, Hyun Kim, Soojong Lim, Namhoon Lee", "pdf_url": "https://openreview.net/pdf/e5209240370185240edeb5bc9a5296481fd5702b.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54525, "poster_number": 4706, "tag": "SD-3-4706 | Exhibit Hall C,D,E ", "relevance_score": 65.9, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "MemEIC introduces the first benchmark (CCKEB) for continual and compositional knowledge editing in large vision-language models, demonstrating that separating visual and textual knowledge updates prevents catastrophic forgetting and representation collapse. The brain-inspired knowledge connector achieves 80.56% compositional reliability, an 18.51-point improvement over the best baseline, while maintaining stable performance across sequential editing gaps.", "description": "This paper addresses the challenge of continuously updating large vision-language models (LVLMs) by proposing MemEIC, a framework that combines external memory retrieval with internal model editing using modality-specific adapters. The approach employs dual external memory for cross-modal evidence retrieval, dual LoRA adapters for disentangled parameter updates, and a brain-inspired knowledge connector that selectively integrates visual and textual information for compositional reasoning.", "key_contribution": "The main contribution is MemEIC, a novel framework for continual and compositional knowledge editing that integrates external retrieval memory with internal modality-specific adapters inspired by brain lateralization, coupled with a selective knowledge connector that enables robust multimodal reasoning while preventing catastrophic forgetting and cross-modal interference.", "novelty": "Unlike existing methods that focus on single-modality edits or store all updates in a unified space, MemEIC separates visual and textual knowledge into distinct internal memory spaces (dual LoRA adapters) inspired by brain hemispheric specialization. It addresses the limitations of both external-only methods (which suffer from retrieval errors and internalization issues) and internal-only methods (which experience catastrophic forgetting and representation collapse) by combining both approaches with a brain-inspired connector that selectively fuses modalities only when compositional reasoning is required.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "In-Context Learning and Few-Shot Adaptation", "Compositional and Counterfactual Reasoning"], "score": 65.9, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4234122, "title": "Quantifying Cross-Modality Memorization in Vision-Language Models", "authors": "Yuxin Wen, Yangsibo Huang, Tom Goldstein, Ravi Kumar, Badih Ghazi, Chiyuan Zhang", "pdf_url": "https://openreview.net/pdf/e5e5cfc7f6638a6081384cd92bec67f1ceee43ab.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54482, "poster_number": 1311, "tag": "SD-3-1311 | Exhibit Hall C,D,E ", "relevance_score": 64.46, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper reveals that vision-language models exhibit significant asymmetric cross-modality memorization gaps, where knowledge learned in one modality (text or image) transfers imperfectly to the other modality. The transfer from image to text is more effective than text to image, and this gap persists even with larger models and diverse training data. The study also identifies cross-modal challenges in machine unlearning and multi-hop reasoning, where models struggle to unlearn facts or perform complex reasoning across modalities.", "description": "This paper systematically investigates cross-modality memorization in vision-language models using a novel synthetic persona dataset comprising diverse synthetic person images and textual descriptions. The authors train models on single modalities and evaluate their ability to recall factual knowledge (person names and favorite numbers) in the opposite modality, quantifying the cross-modal transferability gap across various scenarios including model scaling, unlearning, and multi-hop reasoning.", "key_contribution": "The paper introduces the first systematic study of cross-modality factual knowledge memorization in VLMs, revealing fundamental asymmetries in how knowledge transfers between vision and text modalities. It provides a controlled synthetic persona dataset and demonstrates that augmenting training with in-distribution image-caption pairs can partially mitigate the cross-modal transferability gap.", "novelty": "Unlike prior work that focuses on unimodal memorization (text in LLMs or images in diffusion models), this work addresses the unique characteristics of cross-modality memorization in multimodal models. It reveals previously unknown asymmetries in knowledge transfer between modalities and identifies new challenges in cross-modal unlearning and multi-hop reasoning. The controlled synthetic persona dataset enables systematic investigation of these phenomena without data contamination concerns.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Hallucination Detection and Model Robustness", "Vision-Language Reasoning and Chain-of-Thought"], "score": 64.46, "session_type": "San Diego Poster Session 3"}, {"paper_id": 3981201, "title": "Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning", "authors": "Zhifang Zhang, Shuo He, Haobo Wang, Bingquan Shen, Lei Feng", "pdf_url": "https://openreview.net/pdf/ca844585c6a13050f54ca0b88e52a39da65aacfd.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54217, "poster_number": 3909, "tag": "SD-3-3909 | Exhibit Hall C,D,E ", "relevance_score": 57.14, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper reveals that CLIP's vulnerability to backdoor attacks stems from its tendency to encode non-predictive features beyond in-dataset patterns, resulting in low visual feature resistivity to input perturbations. The proposed RVPT defense method achieves state-of-the-art results by reducing attack success rate from 89.70% to 2.76% while tuning only 0.27% of parameters. The approach successfully generalizes across multiple datasets and backdoor attack types, including the advanced BadCLIP attack.", "description": "The paper proposes Repulsive Visual Prompt Tuning (RVPT), a parameter-efficient defense method against backdoor attacks on multimodal contrastive learning models like CLIP. RVPT employs deep visual prompt tuning with a feature-repelling loss that adversarially repels encoded features from deeper layers while optimizing cross-entropy loss, forcing the model to encode only predictive features relevant to downstream tasks. This approach requires only few-shot clean samples and maintains CLIP's open-vocabulary capabilities.", "key_contribution": "The main contribution is a novel defense framework that enhances CLIP's perturbation resistivity by constraining it to encode only in-dataset predictive features through a dual-objective optimization combining feature-repelling loss and cross-entropy loss. This achieves superior backdoor defense performance with minimal parameter tuning (0.27%) and limited clean data.", "novelty": "Unlike existing defenses that require poisoned data access or full model fine-tuning, RVPT introduces a parameter-efficient approach using visual prompt tuning with a specially designed feature-repelling mechanism. The paper provides new insights into CLIP's vulnerabilities by introducing the Perturbation Resistivity (PR) metric and demonstrating that CLIP's low PR stems from encoding excessive non-predictive features during pre-training. RVPT addresses this by creating an adversarial learning environment that forces selective encoding of only task-relevant features.", "ai_categories": ["Prompt Learning and Optimization", "Hallucination Detection and Model Robustness"], "score": 57.14, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4302709, "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning", "authors": "Senqiao Yang, Junyi Li, Xin Lai, Jinming Wu, Wei Li, Zejun MA, Bei Yu, Hengshuang Zhao, Jiaya Jia", "pdf_url": "https://openreview.net/pdf/a1ce03f1786df2e67c61dd99ba4f40d2d92f913b.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54622, "poster_number": 4804, "tag": "SD-3-4804 | Exhibit Hall C,D,E ", "relevance_score": 59.08, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "VisionThink demonstrates that most real-world VQA tasks can be solved with 1/4 resolution images, while only OCR-related tasks require high resolution. The model achieves 102% relative performance compared to baselines while using only ~51% of visual tokens on average, and successfully applies reinforcement learning to general VQA tasks through an LLM-as-Judge strategy.", "description": "This paper presents VisionThink, a vision-language model that dynamically processes images at different resolutions based on task requirements. The model starts with downsampled images and autonomously decides whether to request higher-resolution inputs through reinforcement learning (GRPO). Unlike fixed token reduction methods, VisionThink makes sample-specific decisions, maintaining strong performance on OCR-heavy tasks while achieving significant efficiency gains on general VQA tasks.", "key_contribution": "The main contribution is a new paradigm for visual token reduction where the model autonomously decides whether to process high-resolution images on a per-sample basis, trained via reinforcement learning with an LLM-as-Judge strategy that enables effective RL training on diverse general VQA tasks without requiring cold-start phases.", "novelty": "Unlike existing efficient VLM methods that apply fixed pruning ratios or thresholds uniformly across all samples, VisionThink makes dynamic, content-aware decisions about resolution requirements. The work introduces the LLM-as-Judge approach to enable RL training on general VQA tasks beyond structured domains like visual math, and proposes a carefully designed reward function with penalty mechanisms to achieve stable training without cold-start data collection.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Prompt Learning and Optimization"], "score": 59.08, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4399114, "title": "VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning", "authors": "Wenhao Li, Qiangchang Wang, Xianjing Meng, Zhibin Wu, Yilong Yin", "pdf_url": "https://openreview.net/pdf/521c2d2f24d89aa54641a2bb0276e4fd1a5cebf4.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54436, "poster_number": 5012, "tag": "SD-3-5012 | Exhibit Hall C,D,E ", "relevance_score": 55.25, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "VT-FSL achieves state-of-the-art performance across ten few-shot learning benchmarks, improving classification accuracy by 4.2% on average. The method successfully generates visually grounded class descriptions by conditioning LLMs on both class names and support images through structured reasoning, enabling zero-shot synthesis of semantically consistent images. The proposed kernelized volume-based contrastive alignment captures global and nonlinear relationships among textual, support, and synthetic visual features more effectively than traditional pairwise methods.", "description": "This paper proposes VT-FSL, a framework that bridges vision and text modalities using large language models for few-shot learning. The approach consists of two main components: Cross-modal Iterative Prompting (CIP) that generates precise class descriptions through four structured reasoning stages (strategy, perception, refinement, conclusion), and Cross-modal Geometric Alignment (CGA) that aligns fused textual, support, and synthetic visual representations by minimizing the kernelized volume of their 3D parallelotope.", "key_contribution": "The main contribution is a novel framework that constructs complementary cross-modal prompts (textual descriptions and synthetic images) conditioned on both class names and support images, and integrates them through a geometry-aware alignment mechanism that captures global and nonlinear cross-modal relationships via kernelized volume-based contrastive learning.", "novelty": "Unlike previous methods that rely solely on class names for semantic generation (leading to hallucinations) or use simple pairwise contrastive learning, VT-FSL conditions LLMs on both class names and support images through structured iterative prompting to generate grounded descriptions. It introduces a kernelized volume-based contrastive loss that simultaneously aligns three types of embeddings (textual, support, synthetic visual) by minimizing their parallelotope volume, capturing global relationships that pairwise methods miss. This eliminates the need for costly manual corrections required by methods like SemFew and ECER.", "ai_categories": ["In-Context Learning and Few-Shot Adaptation", "Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization"], "score": 55.25, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4432059, "title": "TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning", "authors": "Xudong Yan, Songhe Feng", "pdf_url": "https://openreview.net/pdf/9ca0e483d5c15956c3f1d6258e602d9ca3d8975f.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54466, "poster_number": 5013, "tag": "SD-3-5013 | Exhibit Hall C,D,E ", "relevance_score": 54.28, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "TOMCAT achieves state-of-the-art performance on four CZSL benchmarks by accumulating multimodal knowledge from unlabeled test data to overcome label distribution shift. The method introduces adaptive prototype updating through Knowledge Accumulation Modules (KAMs) and a dynamic priority queue that stores high-confidence historical images, enabling continuous improvement during testing without catastrophic forgetting.", "description": "This paper addresses Compositional Zero-Shot Learning (CZSL), which aims to recognize novel attribute-object compositions based on knowledge from seen compositions. The work proposes TOMCAT, a test-time adaptation framework that updates both textual and visual prototypes using unsupervised test data through learnable KAMs, adaptive update weights, and a priority queue mechanism while maintaining multimodal alignment through collaborative representation learning.", "key_contribution": "The main contribution is introducing test-time comprehensive knowledge accumulation for CZSL through multimodal prototype updating. This is the first work to leverage unsupervised test data at test time to improve CZSL models by accumulating both textual and visual knowledge while adaptively controlling the degree of prototype adjustment.", "novelty": "Unlike existing CZSL methods that freeze model parameters and prototypes after training, TOMCAT continuously adapts to label distribution shifts by accumulating knowledge from test samples. It addresses the limitation of performance degradation on unseen compositions by introducing learnable KAMs for prototype refinement, a dynamic priority queue for visual knowledge retention, and adaptive update weights that balance knowledge accumulation with catastrophic forgetting prevention.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation", "Compositional and Counterfactual Reasoning"], "score": 54.28, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4155130, "title": "ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness", "authors": "Yijun Liang, Ming Li, Chenrui Fan, Ziyue Li, Dang Nguyen, Kwesi Cobbina, Shweta Bhardwaj, Jiuhai Chen, Fuxiao Liu, Tianyi Zhou", "pdf_url": "https://arxiv.org/pdf/2504.10514", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54779, "poster_number": 4716, "tag": "SD-3-4716 | Exhibit Hall C,D,E ", "relevance_score": 55.12, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper reveals that current VLMs have significant weaknesses in color understanding, with even the best models achieving only ~60% accuracy on color-centric tasks. The scaling law holds for color understanding but is much weaker than general tasks and mainly depends on language model size rather than vision encoder size. Surprisingly, Chain-of-Thought reasoning improves performance even on vision-centric color tasks and robustness to color perturbations, though color can also mislead models in illusion and mimicry scenarios.", "description": "ColorBench is the first comprehensive benchmark for evaluating vision-language models' color understanding capabilities across three dimensions: perception (recognizing and extracting colors), reasoning (using color for logical inference), and robustness (maintaining performance under color variations). The benchmark contains 1,448 instances across 11 tasks covering real-world applications like medical test reading, satellite imagery analysis, and shopping, evaluated on 32 VLMs ranging from 0.5B to 78B parameters.", "key_contribution": "The main contribution is introducing ColorBench, the first dedicated benchmark systematically evaluating VLMs' color perception, reasoning, and robustness capabilities through 11 fine-grained tasks with over 5,800 image-text questions, revealing critical limitations in current models' color understanding that have been largely neglected by the community.", "novelty": "Unlike existing VLM benchmarks that only include basic color recognition questions, ColorBench provides the first integrated evaluation framework specifically designed for color understanding, including novel assessments of color-based reasoning tasks and robustness to color perturbations. The work addresses the gap in understanding whether VLMs can perceive and leverage color information as humans do, introducing new tasks like color proportion estimation, color illusion detection, and systematic color robustness evaluation through controlled recoloring strategies.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Visual Grounding and Spatial Reasoning", "Hallucination Detection and Model Robustness"], "score": 55.12, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4174530, "title": "SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning", "authors": "Wufei Ma, Yu-Cheng Chou, Qihao Liu, Xingrui Wang, Celso M de Melo, Jianwen Xie, Alan Yuille", "pdf_url": "https://openreview.net/pdf/b2e9fe6cf3d6e72577b4271ec01aa5d2a50e35f9.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54623, "poster_number": 5413, "tag": "SD-3-5413 | Exhibit Hall C,D,E ", "relevance_score": 54.25, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "SpatialReasoner achieves state-of-the-art 3D spatial reasoning performance, outperforming Gemini 2.0 by 9.2% on 3DSRBench through explicit 3D representations. The work demonstrates that reinforcement learning enables better generalization to novel question types compared to supervised fine-tuning alone, and reveals that most errors in 3D spatial reasoning stem from failures in 3D perception rather than computation.", "description": "This paper introduces SpatialReasoner, a large vision-language model that performs 3D spatial reasoning using explicit 3D representations (locations, orientations) as an interface across perception, computation, and reasoning stages. The model employs a two-stage training strategy: supervised fine-tuning to equip the model with explicit 3D representations, followed by reinforcement learning to develop robust and generalizable 3D thinking capabilities.", "key_contribution": "The main contribution is a novel LVLM architecture that uses explicit 3D representations as a coherent interface for multi-stage spatial reasoning, combined with a hybrid SFT+RL training strategy that achieves both improved performance and better generalization to novel 3D spatial reasoning questions.", "novelty": "Unlike previous implicit reasoning approaches that rely on natural language descriptions, SpatialReasoner performs explicit 3D computation using numerical representations in calibrated camera space. This addresses the limitation of existing models that fail on trivial spatial questions despite long chain-of-thought reasoning, and enables compositional reasoning and interpretability. The work also provides new insights into training strategies, showing that RL fosters more flexible reasoning while SFT is more scalable with noisy data.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models"], "score": 54.25, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4441575, "title": "Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition", "authors": "Pei Peng, Ming-Kun Xie, Hang Hao, Tong Jin, Sheng-Jun Huang", "pdf_url": "https://openreview.net/pdf/9cc6a292fd51139e30693138af29bfc25cd09a5c.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54819, "poster_number": 5111, "tag": "SD-3-5111 | Exhibit Hall C,D,E ", "relevance_score": 70.29, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "<cite index=\"1-4,2-2\">The paper achieves state-of-the-art zero-shot recognition performance by estimating the Total Direct Effect and simulating intervention to subtract background-only activation, preserving beneficial object-context interactions while mitigating hallucinated scores, substantially improving both worst-group and average accuracy on context-sensitive benchmarks without retraining or prompt design.</cite> <cite index=\"2-3\">The framework provides a lightweight representation-level counterfactual approach, offering a practical causal avenue for debiased and reliable multimodal reasoning.</cite>", "description": "<cite index=\"1-1,1-2\">The paper addresses object-context shortcuts in vision-language models that undermine zero-shot reliability when test-time scenes differ from familiar training co-occurrences by recasting this issue as a causal inference problem, asking whether predictions would remain if objects appeared in different environments.</cite> <cite index=\"1-3\">The method estimates object and background expectations within CLIP's representation space and synthesizes counterfactual embeddings by recombining object features with diverse alternative contexts sampled from external datasets, batch neighbors, or text-derived descriptions.</cite>", "key_contribution": "<cite index=\"2-6,2-17\">The main contribution is a counterfactual estimation method that operates directly at the representation level, developing a lightweight, inference-only debias scheme that employs targeted interventions and Total Direct Effect estimation to eliminate hallucinated effects while preserving beneficial object-context interactions.</cite>", "novelty": "<cite index=\"2-15,2-19\">Unlike existing methods that rely on generative models, auxiliary networks, or counterfactual losses which increase model complexity and limit applicability in zero-shot settings, this work proposes a lightweight, inference-only framework that intervenes directly within the CLIP representation space, enabling causal debiasing without modifying the model architecture or relying on handcrafted prompts.</cite> <cite index=\"2-18\">The method synthesizes counterfactual embeddings by recombining objects with diverse, label-agnostic scene contexts, thus suppressing background-induced illusions while preserving useful object-context interactions.</cite>", "ai_categories": ["Compositional and Counterfactual Reasoning", "Hallucination Detection and Model Robustness", "Prompt Learning and Optimization"], "score": 70.29, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4218197, "title": "Thinker: Learning to Think Fast and Slow", "authors": "Stephen Chung, Wenyu Du, Jie Fu", "pdf_url": "https://openreview.net/pdf/f23194e92e9932bef9aa81c91ed4d6aa1d835472.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54598, "poster_number": 300, "tag": "SD-3-300 | Exhibit Hall C,D,E ", "relevance_score": 52.55, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "The Thinker task improves LLM reasoning by decomposing question-answering into four stages (Fast Thinking, Verification, Slow Thinking, Summarization), achieving 6.7% performance gains for Qwen2.5-1.5B and 11.1% for DeepSeek-R1-Distill-Qwen-1.5B. Notably, the Fast Thinking mode alone achieves competitive accuracy (25.2%) using fewer than 1000 tokens, demonstrating substantial inference efficiency. The structured approach reduces redundant reflection patterns and response length while improving both intuition and deliberative reasoning capabilities.", "description": "This paper introduces the Thinker task, a novel RL environment that decomposes standard question-answering into four structured stages inspired by Dual Process Theory: Fast Thinking (rapid answer with token budget), Verification (self-evaluation), Slow Thinking (refinement with larger budget), and Summarization (distillation). Each stage has distinct reward signals to train specific capabilities‚Äîintuition, evaluation, refinement, and integration‚Äîenabling more precise temporal credit assignment than standard QA tasks.", "key_contribution": "The main contribution is a structured multi-stage RL task design that explicitly trains distinct cognitive capabilities (intuition, verification, refinement, integration) through stage-specific rewards, improving both reasoning performance and inference efficiency compared to standard single-turn QA tasks while being compatible with any RL algorithm.", "novelty": "Unlike prior work that focuses on token budget control within single responses or prompt-based self-correction, this work introduces a structured multi-step RL environment with explicit stage-specific rewards for different cognitive abilities. It addresses the inefficient temporal credit assignment problem in methods like GRPO by isolating learning signals for each task, and differs from concurrent token efficiency work by providing a general task framework rather than algorithm-specific modifications. The approach enables a virtuous loop where Fast Thinking provides promising initial paths while Slow Thinking refines flawed intuition.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Prompt Learning and Optimization"], "score": 52.55, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4352500, "title": "Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization", "authors": "Frank R√∂der, Jan Benad, Manfred Eppe, Pradeep Kr. Banerjee", "pdf_url": "https://openreview.net/pdf/7c307a9a6f2b1e42e901aea2422257dc706fe28c.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 55046, "poster_number": 212, "tag": "SD-3-212 | Exhibit Hall C,D,E ", "relevance_score": 51.0, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "DALI achieves up to 96.4% performance gains over context-unaware baselines and often surpasses ground-truth context-aware baselines in extrapolation tasks, demonstrating robust zero-shot generalization. The learned latent space exhibits physically consistent counterfactuals where perturbing gravity-encoding dimensions produces imagined rollouts with plausible physical behaviors. Theoretical analysis proves that DALI's context encoder achieves near-optimal context inference with O(T/K) sample complexity gain over standard approaches.", "description": "This paper introduces Dynamics-Aligned Latent Imagination (DALI), a framework that extends DreamerV3 to enable zero-shot generalization in contextual MDPs by inferring latent context representations from agent-environment interactions. DALI uses a self-supervised context encoder trained with forward dynamics prediction to capture environmental variations (e.g., gravity, friction) without requiring explicit context labels. The inferred context representations condition both the world model and policy, enabling adaptation to unseen environmental conditions.", "key_contribution": "The main contribution is a theoretically grounded framework that integrates a dynamics-aligned context encoder into model-based RL, proving it is essential for efficient context inference and demonstrating strong empirical zero-shot generalization on challenging cMDP benchmarks without access to ground-truth context variables.", "novelty": "Unlike existing approaches that rely on explicit context conditioning or struggle with latent contexts, DALI introduces a dedicated self-supervised context encoder that learns from forward dynamics, overcoming DreamerV3's information bottleneck. The work provides formal theoretical guarantees showing the encoder achieves near-optimal sample complexity O(K/Œ¥¬≤) versus O(T/Œ¥¬≤) for full episodes, and demonstrates that shallow integration of context acts as effective regularization. DALI addresses the limitation that standard recurrent models compress context information inefficiently, leading to poor generalization on out-of-distribution contexts.", "ai_categories": ["Reinforcement Learning for Multimodal Models", "Embodied AI and Vision-Language-Action Models", "In-Context Learning and Few-Shot Adaptation"], "score": 51.0, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4209413, "title": "The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation", "authors": "Patrick Kahardipraja, Reduan Achtibat, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin", "pdf_url": "https://openreview.net/pdf/4680606493cfdf0ac8851fe0e353c1878d147253.pdf", "session_id": 546, "session_name": "Mexico City Poster Session 3", "poster_id": 55082, "poster_number": 999, "tag": "MC-3-999 | Foyer ", "relevance_score": 67.67, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper reveals that attention heads in LLMs specialize into distinct functional roles during retrieval-augmented generation: in-context heads (which further specialize into task heads for instruction comprehension and retrieval heads for copying information) and parametric heads (which store relational knowledge). The authors demonstrate that these heads can be manipulated through function vectors or attention weight modifications to control answer generation, and develop a probe-based method for tracking knowledge sources with 95%+ accuracy.", "description": "The paper investigates how large language models process information during retrieval-augmented question answering by analyzing attention head specialization. Using attribution-based methods (AttnLRP), the authors identify and characterize different types of attention heads, demonstrate their causal roles through controlled interventions, and develop techniques for source attribution that can distinguish between parametric and contextual knowledge.", "key_contribution": "The main contribution is a comprehensive mechanistic understanding of in-context retrieval augmentation, including: (1) an attribution-based method to identify and categorize specialized attention heads, (2) demonstration of how these heads can be manipulated via function vectors to induce specific behaviors, and (3) an efficient probe for tracking knowledge provenance in retrieval-augmented systems.", "novelty": "Unlike prior work that only identified retrieval heads using attention weights, this paper provides a more nuanced view showing that in-context heads specialize into task and retrieval heads with distinct functions. The work goes beyond Jin et al.'s discovery of in-context and parametric heads by demonstrating their causal roles through function vector extraction and attention modification, and introduces a practical method for source tracking that is more efficient than full backward passes while maintaining high accuracy.", "ai_categories": ["In-Context Learning and Few-Shot Adaptation", "Hallucination Detection and Model Robustness"], "score": 67.67, "session_type": "Mexico City Poster Session 3"}, {"paper_id": 4353246, "title": "CogVLA: Cognition-Aligned Vision-Language-Action Models via Instruction-Driven Routing & Sparsification", "authors": "Wei Li, Renshan Zhang, Rui Shao, Jie He, Liqiang Nie", "pdf_url": "https://openreview.net/pdf/e67f52c1abdf244a6b56b29468a234e1c2b8d202.pdf", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54766, "poster_number": 2305, "tag": "SD-3-2305 | Exhibit Hall C,D,E ", "relevance_score": 51.85, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "CogVLA achieves state-of-the-art performance on robotic manipulation tasks (97.4% success rate on LIBERO, 70.0% on real-world tasks) while reducing training costs by 2.5√ó and inference latency by 2.8√ó compared to OpenVLA. The model introduces a cognition-aligned architecture that performs instruction-driven visual sparsification, reducing visual tokens to 12.5% of original input while maintaining superior task performance. The biologically-inspired three-stage progressive design effectively balances computational efficiency with cross-modal semantic consistency.", "description": "This paper presents CogVLA, a Vision-Language-Action model that addresses the computational inefficiency of existing VLA models through instruction-driven routing and sparsification. The framework employs a three-stage progressive architecture inspired by human multimodal coordination: EFA-Routing for instruction-aware visual aggregation, LFP-Routing for semantic token pruning in the language model, and V-L-A Coupled Attention for coherent action generation. The approach is evaluated on both the LIBERO simulation benchmark and real-world robotic manipulation tasks using the Cobot Agilex ALOHA platform.", "key_contribution": "The main contribution is a cognition-aligned framework that achieves efficient vision-language-action modeling through instruction-driven sparsification across three stages: visual encoding (EFA-Routing), language processing (LFP-Routing), and action decoding (CAtten). This enables 8√ó reduction in visual tokens while improving both performance and efficiency compared to existing VLA models.", "novelty": "Unlike existing sparsification methods that focus on isolated optimization within language models, CogVLA introduces cross-modal instruction-driven routing that maintains semantic coupling across vision, language, and action modalities. The work addresses the limitation of semantic degradation in modular pipelines by establishing task-semantic-consistent joint optimization inspired by human cognitive processes (VAS, SMA, PMC). It introduces novel mechanisms including FiLM-based aggregation routing in vision encoders and task-guided pruning routing in LLMs, combined with a hybrid attention mechanism that enables parallel action decoding while preserving temporal coherence.", "ai_categories": ["Embodied AI and Vision-Language-Action Models", "Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization"], "score": 51.85, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4441664, "title": "InstructHOI: Context-Aware Instruction for Multi-Modal Reasoning in Human-Object Interaction Detection", "authors": "Jinguo Luo, Weihong Ren, Quanlong Zheng, Yanhao Zhang, Zhenlong Yuan, Zhiyong Wang, Haonan Lu, Honghai LIU", "pdf_url": "https://openreview.net/pdf/9256d13b1c0d87dee1265c615f5a083bea101d9c.pdf", "session_id": 546, "session_name": "Mexico City Poster Session 3", "poster_id": 55101, "poster_number": 999, "tag": "MC-3-999 | Foyer ", "relevance_score": 91.35, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "InstructHOI achieves state-of-the-art performance on HICO-DET and V-COCO benchmarks, outperforming previous methods by significant margins (e.g., 2.64 mAP improvement over SICHOI). The method demonstrates superior zero-shot generalization across four settings, with improvements of up to 3.14 mAP in rare first unseen combination settings. By leveraging multi-modal reasoning capabilities of large foundation models rather than just knowledge transfer, the approach shows substantial gains in both supervised and open-world interaction detection.", "description": "This paper proposes InstructHOI, a novel method for Human-Object Interaction (HOI) detection that leverages context-aware instructions to guide multi-modal reasoning in large foundation models. The approach performs HOI-domain fine-tuning on InternVL2 using a generated dataset of 140K interaction-reasoning image-text pairs, develops a Context-aware Instruction Generator (CIG) that fuses visual interactive context with linguistic instructions, and employs an Interest Token Selector (ITS) to align reasoning with interaction regions.", "key_contribution": "The main contribution is exploiting the reasoning capabilities of large foundation models for HOI detection through context-aware multi-modal instructions, rather than simply transferring knowledge via prompts. This includes HOI-domain fine-tuning, visual-linguistic instruction fusion at the human-object pair level, and adaptive token selection for region-aligned reasoning.", "novelty": "Unlike existing LFM-based methods that focus on knowledge transfer through predefined or learnable prompts, InstructHOI directly leverages the reasoning capabilities of foundation models through tailored instructions. The method introduces visual interactive context (appearance and spatial) into linguistic instructions at the human-object pair level, providing multi-modal reasoning guidance. It addresses the limitation of language-only instructions by incorporating visual context and aligns the reasoning process with specific interaction regions through adaptive token filtering.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Visual Grounding and Spatial Reasoning", "Prompt Learning and Optimization"], "score": 91.35, "session_type": "Mexico City Poster Session 3"}, {"paper_id": 4209117, "title": "Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought", "authors": "Zihui Cheng, Qiguang Chen, Xiao Xu, Jiaqi WANG, Weiyun Wang, Hao Fei, Yidong Wang, Alex Jinpeng Wang, Zhi Chen, ..., Libo Qin", "pdf_url": "https://openreview.net/pdf/0822568f152da6a15e1119aef56e6106b5054824.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55148, "poster_number": 4814, "tag": "SD-4-4814 | Exhibit Hall C,D,E ", "relevance_score": 90.24, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper reveals that multimodal chain-of-thought (MCoT) improvements stem from incorporating 'visual thoughts' - intermediate reasoning steps that convey visual information from images to deeper transformer layers. The study demonstrates that visual thoughts function as a cache-like mechanism, with effectiveness depending on expression clarity and conciseness rather than faithful image reproduction. Analysis shows visual thoughts enable 50%+ efficiency gains in complex scenarios and serve as primary intermediaries transmitting visual information to advanced reasoning processes.", "description": "This paper provides a unified framework for understanding multimodal chain-of-thought reasoning by introducing the concept of visual thoughts. The authors systematically analyze four distinct forms of visual thought expressions (Natural Language, Structured Language, Edited Image, and Generative Image) across multiple benchmarks and models. Through attention mechanism analysis and information flow studies, they reveal how visual thoughts bridge raw visual inputs and linguistic reasoning in Large Vision-Language Models.", "key_contribution": "The paper introduces 'visual thoughts' as a unified perspective explaining how different MCoT paradigms (Textual-MCoT and Interleaved-MCoT) enhance LVLMs, and provides the first comprehensive analysis of four visual thought expression strategies with empirical evidence showing they function as visual information caches that enable deeper reasoning.", "novelty": "Unlike prior work that treats Textual-MCoT and Interleaved-MCoT as separate paradigms, this paper unifies them under the visual thoughts framework, showing performance depends on expression clarity/conciseness rather than modality. The work addresses the lack of understanding about MCoT mechanisms by revealing that visual thoughts serve as intermediaries enabling visual information flow to deeper transformer layers, rather than simply replicating image content. This is the first systematic exploration of internal mechanisms (attention patterns, information flow) underlying visual reasoning in MCoT.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization", "Multimodal Benchmarks and Evaluation"], "score": 90.24, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4325374, "title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks", "authors": "Philip Schroeder, Ondrej Biza, Thomas Weng, Hongyin Luo, James R. Glass", "pdf_url": "https://openreview.net/pdf/194ef43930192ab08c960e21945eb03ce919797a.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55845, "poster_number": 4711, "tag": "SD-4-4711 | Exhibit Hall C,D,E ", "relevance_score": 88.88, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "ROVER introduces a recursive framework that enables VLMs to decompose long-horizon video trajectories into subtask-specific segments, achieving significantly better performance than baselines on task progress estimation, frame-level reasoning, and video QA. The method reduces hallucinations by 50-70% during non-expert trajectory moments and scales linearly with video length (versus quadratic for baselines), while maintaining global context through hierarchical decomposition.", "description": "This paper presents ROVER (Reasoning Over VidEo Recursively), a framework that enables vision-language models to reason over long video sequences in embodied robotics tasks by recursively decomposing videos into subtask-specific segments. The method uses in-context learning with a sliding window approach (maximum 3 frames) within each subtask, and is evaluated on a new dataset of 543 videos across 27 robotic manipulation tasks showing both expert and perturbed non-expert trajectories.", "key_contribution": "The main contribution is a recursive video reasoning framework that decomposes long-horizon tasks into subtasks, enabling VLMs to maintain focused temporal context (3 frames maximum) while preserving global task understanding. This achieves linear time complexity scaling with video length and dramatically reduces hallucinations, especially during unexpected or non-optimal trajectory moments.", "novelty": "Unlike prior approaches that either reason over small frame sets (losing global context) or concatenate all frames (causing inefficiency and hallucinations), ROVER recursively segments videos by subtask while using a sliding window within each segment. The paper also introduces a novel protocol for generating diverse non-expert trajectories with ground-truth progress labels from expert demonstrations, and demonstrates that VLM hallucinations primarily occur when reasoning over more than 10 frames, which ROVER mitigates through its decomposition strategy.", "ai_categories": ["Video Understanding and Temporal Reasoning", "Embodied AI and Vision-Language-Action Models", "Vision-Language Reasoning and Chain-of-Thought"], "score": 88.88, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4210765, "title": "RBench-V: A Primary Assessment for Visual Reasoning Models with Multimodal Outputs", "authors": "Meng-Hao Guo, Xuanyu Chu, Qianrui Yang, Zhe-Han Mo, Yiqing Shen, Pei-lin Li, Xinjie Lin, Jinnian Zhang, Xin-Sheng Chen, ..., Shi-min Hu", "pdf_url": "https://arxiv.org/pdf/2505.16770", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55549, "poster_number": 5201, "tag": "SD-4-5201 | Exhibit Hall C,D,E ", "relevance_score": 91.55, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "RBench-V reveals a significant gap between current AI models and human experts in visual reasoning requiring multimodal outputs, with the best model (o3) achieving only 25.8% accuracy compared to 82.3% human performance. The benchmark demonstrates that scaling model size, omni-modal capabilities, and long text-only chain-of-thought approaches are insufficient for solving vision-indispensable reasoning tasks. Models tend to use algebraic shortcuts rather than genuine multimodal reasoning, converting geometric problems to text-based solutions instead of visual reasoning.", "description": "This paper introduces RBench-V, a benchmark with 803 carefully curated questions across math, physics, counting, and games designed to evaluate models' ability to generate multimodal outputs during visual reasoning processes. Unlike existing benchmarks that focus on multimodal inputs with text-only outputs, RBench-V requires models to create and manipulate images (e.g., drawing auxiliary lines, tracing paths, constructing figures) as part of the reasoning chain-of-thought. The benchmark evaluates numerous open- and closed-source models including GPT-4o, Gemini 2.5 Pro, o3, and various vision-language models.", "key_contribution": "The main contribution is the first benchmark specifically designed to assess multimodal output capabilities in visual reasoning processes (multimodal chain-of-thought), addressing a critical gap in evaluation frameworks that previously focused only on multimodal inputs with text-based reasoning outputs.", "novelty": "Unlike MMLU and MMMU which evaluate multimodal understanding with text-only outputs, RBench-V uniquely requires models to generate visual content during the reasoning process, such as drawing geometric figures, adding auxiliary lines, or tracing paths. The benchmark addresses the limitation that existing evaluations neglect the output modality dimension, which is crucial for assessing omni-models and reasoning-driven systems. It reveals that current models lack genuine multimodal reasoning capabilities and often rely on 'multimodal reasoning shortcuts' by converting visual problems into purely algebraic ones.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought", "Compositional and Counterfactual Reasoning"], "score": 91.55, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4483959, "title": "FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges", "authors": "Kevin Hayes, Micah Goldblum, Vikash Sehwag, Gowthami Somepalli, Ashwinee Panda, Tom Goldstein", "pdf_url": "https://arxiv.org/pdf/2512.02161", "session_id": 545, "session_name": "San Diego Poster Session 3", "poster_id": 54476, "poster_number": 4310, "tag": "SD-3-4310 | Exhibit Hall C,D,E ", "relevance_score": 50.55, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "FineGRAIN reveals that current T2I models systematically fail at specific tasks like counting objects (0% success for multiple objects across all models) and generating long text (0% success for 50+ tokens), while VLMs struggle to accurately identify these failures. The framework demonstrates that VQAScore has only 57.7% agreement with human judgments, while FineGRAIN achieves 67.4% agreement through fine-grained, failure-mode-specific evaluation. The work exposes critical gaps in both T2I generation capabilities and VLM evaluation reliability across 27 distinct failure modes.", "description": "This paper presents FineGRAIN, a joint evaluation framework for assessing both text-to-image (T2I) models and vision-language models (VLMs) through a structured, hierarchical approach. The framework uses VLMs as judges to identify 27 specific failure modes in images generated by T2I models, creating a dataset of 3,750+ images from 5 T2I models annotated by VLMs and validated against human ground truth. The methodology enables fine-grained analysis of prompt adherence failures such as incorrect object counts, color bindings, text rendering, and spatial relationships.", "key_contribution": "The main contribution is a comprehensive benchmark that jointly evaluates T2I and VLM models through 27 fine-grained failure modes with objective, interpretable boolean scores and explanations. The framework provides both a curated dataset with human annotations and an agentic evaluation pipeline that produces actionable insights into specific model weaknesses, enabling targeted improvements in multimodal AI systems.", "novelty": "Unlike prior benchmarks that evaluate T2I or VLMs separately with coarse-grained metrics, FineGRAIN provides fine-grained decomposition of failure modes (e.g., separating counting from shape attributes) and evaluates both components of the generation-evaluation pipeline simultaneously. The work addresses limitations of existing metrics like VQAScore and CLIPScore by providing objective, boolean evaluations with interpretable reasoning rather than continuous scores, and introduces application-specific failure mode analysis that reveals systematic errors missed by aggregate evaluations.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Hallucination Detection and Model Robustness"], "score": 50.55, "session_type": "San Diego Poster Session 3"}, {"paper_id": 4216097, "title": "Point-RFT: Improving Multimodal Reasoning with Visually Grounded Reinforcement Finetuning", "authors": "Minheng Ni, Zhengyuan Yang, Linjie Li, Chung-Ching Lin, Kevin Lin, Wangmeng Zuo, Lijuan Wang", "pdf_url": "https://openreview.net/pdf/5084e959f05b99951639c06f88dbb5747c300a40.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55238, "poster_number": 5506, "tag": "SD-4-5506 | Exhibit Hall C,D,E ", "relevance_score": 88.28, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Point-RFT demonstrates that visually grounded Chain-of-Thought reasoning significantly outperforms text-only CoT for multimodal reasoning, achieving 90.04% accuracy on ChartQA (up from 70.88% baseline and 83.92% text-only RL). The approach shows superior out-of-domain generalization with 53.16% average accuracy across five benchmarks, compared to 33.80% for text-only RL. The work establishes that explicit visual grounding through point annotations enables better perception-reasoning alignment and reduces hallucinations.", "description": "This paper introduces Point-RFT, a two-stage framework for multimodal reasoning that combines supervised format finetuning on a curated 71K Point-CoT dataset with reinforcement learning using GRPO. The Point-CoT dataset integrates step-by-step reasoning with explicit point-level visual grounding, where each reasoning step is spatially anchored to relevant visual elements. The framework uses a hybrid pipeline combining GPT-4o for reasoning generation and Molmo-7B for visual grounding to ensure spatial-textual consistency.", "key_contribution": "The main contribution is demonstrating that reinforcement learning with visually grounded CoT (where reasoning steps are explicitly linked to visual points) is more effective for multimodal reasoning than text-only CoT, along with providing a 71K Point-CoT dataset that enables models to learn grounded reasoning patterns.", "novelty": "Unlike previous multimodal CoT approaches that rely on text-only reasoning or prompt-driven visual tools, Point-RFT performs end-to-end reinforcement finetuning with explicit point-level visual grounding integrated directly into the reasoning chain. The work addresses limitations of text-only CoT in vision-language tasks by forcing spatial attention before answer generation, and introduces a validation mechanism that filters hallucinated visual elements through cross-verification between reasoning and grounding models. This is the first work to systematically apply RL optimization to visually grounded chain-of-thought reasoning.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Visual Grounding and Spatial Reasoning"], "score": 88.28, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4244825, "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "authors": "Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, Tieniu Tan", "pdf_url": "https://openreview.net/pdf/0da039d9be67a74526e2ce4408db4cde8bdca6f5.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55344, "poster_number": 4815, "tag": "SD-4-4815 | Exhibit Hall C,D,E ", "relevance_score": 82.84, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces a novel 'drawing to reason in space' paradigm that enables vision-language models to perform spatial reasoning through iterative visual drawing operations (bounding boxes and auxiliary lines) rather than purely text-based reasoning. The approach achieves an average 18.4% improvement over existing methods across five spatial reasoning benchmarks, with particularly strong performance on maze navigation (98.2% accuracy) and video-based reasoning tasks. The three-stage training framework (cold-start training, reflective rejection sampling, and reinforcement learning) successfully cultivates self-correction capabilities and visual reasoning abilities in open-source models.", "description": "The paper presents VILASR, a vision-language model that performs spatial reasoning by decomposing complex problems into interpretable visual drawing and thinking steps. Unlike existing approaches that rely on text-centric reasoning or external perception tools, VILASR uses elementary drawing operations (annotating bounding boxes and drawing auxiliary lines) to directly manipulate and analyze spatial relationships in visual space. The model is trained through a three-stage framework combining synthetic data, reflective rejection sampling to encourage self-correction, and reinforcement learning with carefully designed rewards.", "key_contribution": "The main contribution is the 'drawing to reason in space' paradigm that enables LVLMs to reason through elementary visual drawing operations, avoiding the limitations of text-only reasoning and dependence on external perception tools. The paper also introduces a principled three-stage training framework that progressively cultivates visual reasoning capabilities, including a novel reflective rejection sampling mechanism that enhances self-correction behaviors.", "novelty": "Unlike existing methods that either confine reasoning to textual form or rely on black-box perception tools with fixed capabilities, this work enables models to perform flexible visual manipulation through basic drawing operations that can be iteratively refined based on intermediate results. The approach addresses the fundamental limitation that spatial details are lost when converting visual information to text, and introduces reflective rejection sampling to cultivate self-correction capabilities that are absent in prior tool-augmented reasoning methods. The work demonstrates that open-source models can achieve sophisticated visual reasoning through proper training, narrowing the gap with proprietary models.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Visual Grounding and Spatial Reasoning", "Reinforcement Learning for Multimodal Models"], "score": 82.84, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4204683, "title": "SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning", "authors": "Yang Liu, Ming Ma, Xiaomin Yu, Pengxiang Ding, Han Zhao, Mingyang Sun, Siteng Huang, Donglin Wang", "pdf_url": "https://openreview.net/pdf/25e4a0d5d0d7bb0f7006a8d400a662a0c31c22d7.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55836, "poster_number": 5302, "tag": "SD-4-5302 | Exhibit Hall C,D,E ", "relevance_score": 81.46, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "SSR introduces a novel framework that transforms raw depth data into structured textual rationales, achieving significant improvements in spatial reasoning tasks (up to 22.5% on SSRBENCH spatial tasks). The method uses knowledge distillation to compress rationales into compact latent embeddings, enabling plug-and-play integration into existing VLMs without retraining. The paper also contributes SSR-COT, a million-scale dataset with spatial reasoning annotations, and SSRBENCH, a comprehensive multi-task benchmark.", "description": "This paper proposes SSR (Spatial Sense and Reasoning), a framework that enhances Vision-Language Models' depth perception and spatial reasoning by converting depth information into interpretable textual rationales. The approach uses a Mamba-based Image-Depth Interpreter (MIDI) module that generates depth-aware latent tokens from monocular depth estimates, which are then integrated into VLMs to improve spatial understanding without requiring specialized sensors like LiDAR.", "key_contribution": "The main contribution is a rationale-guided spatial reasoning framework that bridges low-level depth perception and higher-level reasoning through structured textual rationales, combined with a knowledge distillation strategy that enables efficient, plug-and-play integration into existing VLMs while significantly enhancing their spatial understanding capabilities.", "novelty": "Unlike existing methods that superficially incorporate depth as supplementary input, SSR treats depth as a fundamental component for complex spatial reasoning by converting it into interpretable rationales that mimic human-like implicit reasoning processes. The work addresses the limitation that current VLMs fail to effectively exploit depth information for higher-order reasoning, introducing a novel two-stage training paradigm where rationales are first generated and aligned, then optionally co-trained with the VLM. The plug-and-play MIDI module can be integrated into existing VLMs without full retraining, distinguishing it from end-to-end approaches like Meteor.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Vision-Language Reasoning and Chain-of-Thought", "Multimodal Benchmarks and Evaluation"], "score": 81.46, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4178506, "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT", "authors": "Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, Hongsheng Li", "pdf_url": "https://openreview.net/pdf/ffb79a572869428a5be310d74404eb5faac1291f.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 56058, "poster_number": 5306, "tag": "SD-4-5306 | Exhibit Hall C,D,E ", "relevance_score": 78.81, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "<cite index=\"1-14\">The paper achieves 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1</cite>. <cite index=\"3-20\">T2I-R1 better understands complex prompts, reasons about user intentions, and handles uncommon scenarios with greater robustness, establishing a new paradigm for reasoning-centric generative systems</cite>.", "description": "<cite index=\"1-4\">T2I-R1 is a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process</cite>. <cite index=\"1-5\">The model identifies two levels of CoT: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation</cite>. <cite index=\"1-6\">BiCoT-GRPO with an ensemble of generation rewards seamlessly optimizes both generation CoTs within the same training step</cite>.", "key_contribution": "<cite index=\"3-13,3-14\">The paper identifies a dual-level reasoning process in autoregressive image generation by introducing semantic-level and token-level CoT, and develops BiCoT-GRPO, a new reinforcement learning framework that jointly optimizes both levels of CoT reasoning</cite>.", "novelty": "<cite index=\"1-2,1-3\">While recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance, applying such reasoning strategies to the visual generation domain remains largely unexplored</cite>. <cite index=\"3-16,3-18\">T2I-R1 is the first reasoning-enhanced text-to-image model powered by a bi-level CoT reasoning process, integrating both levels through BiCoT-GRPO, a reinforcement learning framework incorporating two levels of CoT within the same training step</cite>.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Prompt Learning and Optimization"], "score": 78.81, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4215425, "title": "InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts", "authors": "Tianchi Xie, Minzhi Lin, Mengchen Liu, Yilin Ye, Changjian Chen, Shixia Liu", "pdf_url": "https://arxiv.org/pdf/2505.19028", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55176, "poster_number": 4613, "tag": "SD-4-4613 | Exhibit Hall C,D,E ", "relevance_score": 77.12, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "InfoChartQA reveals a substantial performance gap between MLLMs on infographic charts versus plain charts, with even top models like Claude 3.5 Sonnet dropping from 81.37% to 62.80% accuracy. The benchmark demonstrates that visual elements, particularly metaphorical imagery, significantly challenge current MLLMs, with models scoring only ~55% on metaphor-related questions. Analysis shows that proximity between icons and data values, visual complexity, and label ordering critically affect model performance.", "description": "This paper introduces InfoChartQA, a benchmark containing 5,948 paired infographic and plain charts sharing identical underlying data but differing in visual presentation. The benchmark includes 50,920 text-based questions and 7,937 visual-element-based questions (including basic and metaphor-related) specifically designed to evaluate MLLMs' ability to understand infographic charts with pictorial visual elements like pictograms, icons, and metaphorical imagery.", "key_contribution": "The first benchmark providing paired infographic and plain charts with the same data, enabling controlled comparison and fine-grained error analysis. It introduces visual-element-based questions specifically targeting infographic-unique features like metaphors and pictorial elements, revealing critical gaps in current MLLMs' multimodal reasoning capabilities.", "novelty": "Unlike existing chart QA benchmarks that lack paired plain chart counterparts, InfoChartQA enables diagnosis of whether failures stem from data complexity or visual elements through controlled ablation studies. It addresses the gap in evaluating understanding of pictorial visual elements and metaphors in infographic charts, which existing benchmarks overlook. The benchmark introduces metaphor-related questions and demonstrates that visual elements are the primary cause of performance degradation through systematic removal experiments.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought", "Visual Grounding and Spatial Reasoning"], "score": 77.12, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4438327, "title": "Latent Chain-of-Thought for Visual Reasoning", "authors": "Guohao Sun, Hang Hua, Jian Wang, Jiebo Luo, Sohail Dianat, MAJID RABBANI, Raghuveer Rao, Zhiqiang Tao", "pdf_url": "https://openreview.net/pdf/afce18f769b150f82c90b46e4582f2c422ebb699.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55566, "poster_number": 501, "tag": "SD-4-501 | Exhibit Hall C,D,E ", "relevance_score": 74.95, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "LaCoT achieves state-of-the-art performance on visual reasoning benchmarks, with the 7B model improving 6.6% over its base model and 10.6% over GRPO. The method enables diverse, high-likelihood chain-of-thought sampling through amortized variational inference, avoiding reward hacking while improving generalization. The proposed Bayesian inference-scaling strategy (BiN) provides a principled, computationally efficient alternative to Best-of-N sampling without requiring external reward models.", "description": "This paper reformulates visual chain-of-thought reasoning in Large Vision-Language Models as a posterior inference problem and proposes LaCoT, a training framework based on amortized variational inference using GFlowNets. The method introduces reference-guided policy exploration (RGFN) with token-level reward approximation to enable diverse rationale sampling, and a Bayesian inference-scaling strategy (BiN) that uses marginal likelihood to rank answers without external critics.", "key_contribution": "The main contribution is a novel training algorithm that treats visual reasoning as latent variable inference, enabling diverse chain-of-thought sampling through GFlowNets-based amortized variational inference with efficient token-level rewards, combined with a principled Bayesian inference-time scaling method that eliminates the need for biased reward models.", "novelty": "Unlike existing methods (SFT, PPO, GRPO) that rely on deterministic sampling or are constrained by KL penalties limiting exploration, this work explicitly models the posterior distribution over reasoning chains using amortized variational inference. It addresses catastrophic forgetting through reference-guided exploration without KL constraints, introduces efficient token-level reward approximation via linear interpolation for long sequences, and replaces computationally expensive Best-of-N sampling with a theoretically grounded marginal likelihood approach that improves both efficiency and interpretability.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Prompt Learning and Optimization"], "score": 74.95, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4215496, "title": "SeePhys:  Does Seeing Help Thinking? ‚Äì Benchmarking Vision-Based Physics Reasoning", "authors": "Kun Xiang, Heng Li, Terry Jingchen Zhang, Yinya Huang, Zirong Liu, Peixin Qu, Jixi He, Jiaqi Chen, Yu-Jie Yuan, ..., Xiaodan Liang", "pdf_url": "https://arxiv.org/pdf/2505.19099", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55162, "poster_number": 4603, "tag": "SD-4-4603 | Exhibit Hall C,D,E ", "relevance_score": 68.27, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "SeePhys reveals that even state-of-the-art reasoning models (Gemini-2.5-Pro, o4-mini) achieve only sub-60% accuracy on physics problems requiring visual understanding, with 75% of problems being vision-essential. The benchmark demonstrates that current MLLMs struggle to establish meaningful connections between diagram interpretation and physics reasoning, often relying on textual cues as cognitive shortcuts rather than truly leveraging visual information.", "description": "SeePhys is a large-scale multimodal benchmark containing 2,000 rigorously validated physics questions paired with 2,245 images, spanning 8 knowledge levels (middle school to PhD) across 7 physics domains and 21 diagram types. The benchmark evaluates MLLMs' ability to integrate visual understanding with physics reasoning through four experimental settings (Text+Vision, Text+Caption, Text Only, Vision Only) to assess varying levels of visual information dependency.", "key_contribution": "The paper introduces the first comprehensive physics reasoning benchmark that systematically distinguishes between vision-essential (75%) and vision-optional (25%) problems, enabling fine-grained analysis of how MLLMs utilize visual information versus textual cues in physics problem-solving across the full knowledge spectrum from basic to advanced levels.", "novelty": "Unlike prior physics benchmarks that treat visual elements as supplementary, SeePhys prioritizes vision-essential problems where diagrams contain analytically indispensable information. The benchmark introduces a novel evaluation framework with four settings to quantify models' visual dependency, revealing that models show performance improvements even with non-essential diagrams and exhibit distinct failure patterns in visual misinterpretation and modeling flaws. This is the first work to provide comprehensive coverage across all knowledge levels with detailed diagram type annotations and purely visual question variants.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Visual Grounding and Spatial Reasoning", "Vision-Language Reasoning and Chain-of-Thought"], "score": 68.27, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4483983, "title": "RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in Open-domain Question Answering", "authors": "Rongyang Zhang, Yuqing Huang, Chengqiang Lu, Qimeng Wang, Yan Gao, YIWU, Yao Hu, Yin Xu, Wei Wang, ..., Enhong Chen", "pdf_url": "https://arxiv.org/pdf/2512.05119", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55974, "poster_number": 5101, "tag": "SD-4-5101 | Exhibit Hall C,D,E ", "relevance_score": 69.52, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces RAG-IGBench, the first comprehensive benchmark specifically designed for evaluating interleaved image-text generation in open-domain question answering. The benchmark includes 6,057 curated samples with innovative multi-dimensional evaluation metrics that assess text quality, image quality, and image-text consistency without relying on potentially biased MLLM-based scoring. Extensive experiments reveal significant performance gaps between proprietary and open-source models, particularly in image-to-text matching capabilities.", "description": "The paper presents RAG-IGBench, a benchmark for evaluating Retrieval-Augmented Generation-based Interleaved Generation (RAG-IG) that combines multimodal large language models with retrieval mechanisms to generate coherent sequences of interleaved text and images. The approach uses retrieved multimodal content to produce markdown-formatted responses with image placeholders that are replaced with corresponding retrieved images. The benchmark introduces novel rule-based evaluation metrics including ROUGE for text, Edit Distance and Kendall Score for images, and CLIP Score with a new Alignment Score for image-text consistency.", "key_contribution": "The main contribution is a comprehensive benchmark framework that addresses the evaluation gap in interleaved image-text generation through: (1) a novel RAG-based generation approach, (2) a high-quality dataset with systematic human annotation, (3) innovative automated evaluation metrics validated against human assessments, and (4) extensive analysis of state-of-the-art MLLMs revealing their capabilities and limitations in handling complex multimodal tasks.", "novelty": "Unlike existing benchmarks that use MLLM-based evaluation (which introduces bias and instability) or focus on single-modality metrics, this work proposes rule-based multi-dimensional metrics specifically designed for interleaved generation. The RAG-IG framework differs from previous approaches by retrieving rather than generating images, avoiding semantic inconsistencies and quality issues inherent in diffusion-based or unified transformer methods. The benchmark uniquely evaluates open-domain interleaved generation with an average of 13 input images per query, significantly more complex than existing datasets.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Hallucination Detection and Model Robustness"], "score": 69.52, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4067725, "title": "Object-centric binding in Contrastive Language-Image Pretraining", "authors": "Rim Assouel, Pietro Astolfi, Florian Bordes, Michal Drozdzal, Adriana Romero-Soriano", "pdf_url": "https://openreview.net/pdf/e080621881f559915bf7ebef0728a061e50884bf.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55413, "poster_number": 4912, "tag": "SD-4-4912 | Exhibit Hall C,D,E ", "relevance_score": 76.28, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "OC-CLIP introduces object-centric inductive biases to CLIP-based models, achieving significant improvements in compositional understanding: +16.5% on SugarCrepe swap-attribute split, +89% on COCO-spatial, and +92% on GQA-spatial. The approach demonstrates superior sample efficiency compared to hard-negative augmentation methods, solving binding problems that data-centric approaches cannot address even with extensive hard negatives. When trained from scratch on noisy data, OC-CLIP shows +12.8% improvement on ImageNet zero-shot classification while maintaining compositional understanding gains.", "description": "This paper addresses CLIP's fundamental limitation in understanding compositional scenes with multiple objects by integrating object-centric representation learning principles. The approach introduces a binding module that connects scene graphs extracted from text with slot-structured image representations, and proposes a structured similarity score that evaluates both object presence and relational constraints. The method is evaluated across synthetic controlled environments and real-world compositional benchmarks for attribute binding and spatial relationship understanding.", "key_contribution": "The main contribution is OC-CLIP, a novel pretraining method that endows CLIP-based architectures with object-centric binding capabilities through: (1) a binding module using inverted cross-attention to create text-conditioned visual slots, and (2) a structured similarity score that separately evaluates object matching and relational constraints from scene graphs, fundamentally addressing CLIP's bag-of-words behavior.", "novelty": "Unlike existing approaches that rely on hard-negative augmentations or dense captioning to improve compositional understanding, this work addresses the root cause by changing CLIP's representation format from single vectors to structured slot-based representations. The paper demonstrates that CLIP's compositional failures stem from representational limitations rather than data insufficiency, showing that hard-negative methods plateau while OC-CLIP's inductive biases enable better generalization. This is the first work to successfully integrate object-centric learning principles from unsupervised visual representation learning into vision-language pretraining.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Visual Grounding and Spatial Reasoning", "Compositional and Counterfactual Reasoning"], "score": 76.28, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4234112, "title": "Counterfactual reasoning: an analysis of in-context emergence", "authors": "Moritz Miller, Bernhard Sch√∂lkopf, Siyuan Guo", "pdf_url": "https://openreview.net/pdf/94255f9f374c1c8987130333640df5f6b55fd57b.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55580, "poster_number": 2603, "tag": "SD-4-2603 | Exhibit Hall C,D,E ", "relevance_score": 68.14, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that Transformers can perform in-context counterfactual reasoning through a designated noise abduction mechanism. The study shows that self-attention, model depth, and pre-training data diversity are critical factors for counterfactual reasoning performance. The work provides both theoretical guarantees (extending identifiability results to exchangeable data) and mechanistic evidence that latent concepts are linearly encoded in the residual stream, with emergence of specialized noise abduction heads.", "description": "The paper investigates whether large language models can perform in-context counterfactual reasoning - the ability to predict consequences of hypothetical scenarios while preserving noise from factual observations. Using controlled synthetic tasks (linear regression and Lotka-Volterra SDEs), the authors study the three-step counterfactual process: noise abduction, intervention, and prediction. They analyze how Transformers learn to infer unobserved latent concepts and copy contextual noise to make accurate counterfactual predictions.", "key_contribution": "The paper establishes that counterfactual reasoning in a broad class of invertible functions can be reduced to transformations on in-context observations (Lemma 1), provides identifiability guarantees under exchangeability (Theorem 1), and mechanistically identifies designated noise abduction heads that emerge during training to enable counterfactual reasoning in Transformers.", "novelty": "Unlike prior work on counterfactual story generation that focuses on empirical evaluation of off-the-shelf models, this work provides rigorous theoretical foundations and controlled experimental evaluation of counterfactual reasoning capabilities. It extends existing identifiability results from Markovian settings to exchangeable data and demonstrates that counterfactual reasoning requires noise abduction - a capability not needed in standard in-context learning tasks. The work also provides preliminary evidence for counterfactual story generation through experiments on sequential dynamical systems (SDEs).", "ai_categories": ["Compositional and Counterfactual Reasoning", "In-Context Learning and Few-Shot Adaptation", "Prompt Learning and Optimization"], "score": 68.14, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4432236, "title": "Teaching Language Models to Reason with Tools", "authors": "Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang Wang, ..., Dayiheng Liu", "pdf_url": "https://openreview.net/pdf/406d2cc67059647e62684368005578debbc188e9.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 56019, "poster_number": 1607, "tag": "SD-4-1607 | Exhibit Hall C,D,E ", "relevance_score": 67.7, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "CoRT (Code-Optimized Reasoning Training) achieves significant improvements in mathematical reasoning by teaching LRMs to effectively use code interpreters, delivering 4-8% absolute accuracy gains while reducing token usage by 30-50%. The framework introduces Hint-Engineering, a strategic data synthesis method that addresses the conflict between internal probabilistic reasoning and external deterministic knowledge from code execution. Remarkably, just 30 high-quality manually annotated samples combined with rejection sampling and RL can match or exceed performance of models trained on much larger datasets.", "description": "This paper presents CoRT, a post-training framework that teaches Large Reasoning Models (LRMs) to effectively integrate Code Interpreters (CIs) for mathematical problem-solving. The approach uses Hint-Engineering to strategically inject hints at optimal points in reasoning paths, addressing inefficiencies like delayed code computation and code result distrust. The framework combines supervised fine-tuning, rejection fine-tuning, and reinforcement learning to optimize the multi-round interleaving of CI usage and internal reasoning.", "key_contribution": "The main contribution is the Hint-Engineering data synthesis strategy that strategically inserts targeted hints to guide LRMs toward efficient code-integrated reasoning, combined with a complete training pipeline (SFT-RFT-RL) that demonstrates the 'less is more' principle‚Äîachieving superior performance with just 30 high-quality training samples versus hundreds of automatically generated ones.", "novelty": "Unlike prior work that uses random hint insertion or simple prompting for code integration, this work identifies and addresses two specific inefficiency patterns (delayed code computation and code result distrust) through strategically positioned hints. The approach differs from existing tool-integrated reasoning methods by explicitly modeling and resolving the conflict between LRMs' internal probabilistic reasoning and external deterministic knowledge from code execution. The framework achieves both performance improvements and dramatic efficiency gains (30-50% token reduction), whereas previous work typically optimized only for accuracy.", "ai_categories": ["Prompt Learning and Optimization", "Reinforcement Learning for Multimodal Models", "In-Context Learning and Few-Shot Adaptation"], "score": 67.7, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4400065, "title": "Towards Reliable and Holistic Visual In-Context Learning Prompt Selection", "authors": "Wenxiao Wu, Jing-Hao Xue, Chengming Xu, Chen Liu, Xinwei Sun, Changxin Gao, Nong Sang, Yanwei Fu", "pdf_url": "https://openreview.net/pdf/fb03fb96ae01e3fd443a1ed7d1588bb263abe314.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55751, "poster_number": 4716, "tag": "SD-4-4716 | Exhibit Hall C,D,E ", "relevance_score": 66.69, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that the widely-assumed similarity-priority heuristic in Visual In-Context Learning (VICL) - that visually similar images make better in-context examples - is statistically weak and unreliable. The proposed RH-Partial2Global method achieves consistent performance improvements across segmentation, detection, and colorization tasks by using conformal prediction to identify reliable prompts and covering designs to ensure comprehensive pairwise preference sampling.", "description": "The paper addresses the problem of selecting optimal in-context examples for Visual In-Context Learning by enhancing the Partial2Global framework. It introduces two key strategies: (1) a jackknife conformal prediction-guided approach to construct reliable alternative sets of candidates, and (2) a covering design-based sampling strategy to ensure holistic and uniform coverage of pairwise preferences during global ranking aggregation.", "key_contribution": "The main contribution is RH-Partial2Global, which provides the first statistical evidence challenging the similarity-priority assumption in VICL and introduces theoretically-grounded methods using conformal prediction and covering designs to achieve more reliable and comprehensive in-context example selection without requiring additional model training.", "novelty": "Unlike previous VICL methods that rely on the unvalidated similarity-priority assumption, this work provides statistical evidence (via Spearman correlation tests) showing its weakness and proposes principled alternatives. It addresses two critical limitations: (1) the lack of statistical foundation for similarity-based selection through conformal prediction, and (2) incomplete/redundant pairwise preference coverage through optimal covering designs, replacing random sampling with systematic structured sampling.", "ai_categories": ["In-Context Learning and Few-Shot Adaptation", "Prompt Learning and Optimization"], "score": 66.69, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4268729, "title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner", "authors": "Junhao Shi, Zhaoye Fei, Siyin Wang, Qipeng Guo, Jingjing Gong, Xipeng Qiu", "pdf_url": "https://openreview.net/pdf/c4dfdf9e91cc4527c4c879b52d8cc8d352f626ce.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55572, "poster_number": 2404, "tag": "SD-4-2404 | Exhibit Hall C,D,E ", "relevance_score": 66.44, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that open-source vision-language models can achieve superior embodied planning performance (60.7 absolute improvement in task success rates) using only raw visual observations, outperforming proprietary systems like GPT-4o and Claude-3.5-Sonnet. The framework shows particularly strong gains in commonsense reasoning (+60.0) and long-horizon planning (+70.0), establishing new state-of-the-art results on the EB-ALFRED benchmark while operating under strict closed-loop constraints without privileged environmental feedback.", "description": "The paper introduces World-Aware Planning Narrative Enhancement (WAP), a framework that enhances Large Vision-Language Models for embodied planning tasks by systematically developing four cognitive capabilities: visual appearance modeling, spatial reasoning, functional abstraction, and syntactic grounding. Using curriculum learning with multi-dimensional instruction augmentation and step-wise reasoning generation, the approach trains models to plan complex tasks using only egocentric visual observations and natural language instructions.", "key_contribution": "The main contribution is a multi-dimensional cognitive enhancement framework that infuses LVLMs with comprehensive environmental understanding through structured narrative augmentation across four cognitive dimensions, combined with curriculum learning, enabling robust embodied planning without relying on privileged environmental feedback or auxiliary signals.", "novelty": "Unlike existing environment-agnostic imitation learning approaches that treat task instructions and environmental contexts as disconnected elements, this work systematically integrates world knowledge through four complementary cognitive dimensions. It addresses the limitation of models relying on supplementary cues rather than visual reasoning by operating exclusively on raw visual observations in a closed-loop manner, more closely mirroring human cognitive processes. The curriculum learning strategy progressively builds cognitive capabilities from basic perception-action mapping to complex semantic reasoning.", "ai_categories": ["Embodied AI and Vision-Language-Action Models", "Vision-Language Reasoning and Chain-of-Thought", "In-Context Learning and Few-Shot Adaptation"], "score": 66.44, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4215473, "title": "ChartSketcher: Reasoning with Multimodal Feedback and Reflection for Chart Understanding", "authors": "Muye Huang, Lingling Zhang, Jie Ma, Han Lai, Fangzhi Xu, Yifei Li, Wenjun Wu, Yaqiang Wu, Jun Liu", "pdf_url": "https://openreview.net/pdf/e0a6d52ba473df422b4d1442df349437256e50ac.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55948, "poster_number": 5318, "tag": "SD-4-5318 | Exhibit Hall C,D,E ", "relevance_score": 67.42, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "ChartSketcher introduces a novel multimodal feedback-driven reasoning approach that enables MLLMs to annotate intermediate reasoning steps directly onto charts using programmatic sketching, achieving state-of-the-art performance on chart understanding benchmarks. The method demonstrates that visual grounding through iterative sketch-based feedback significantly improves complex visual reasoning, with the model learning to identify and correct its own errors through reflection mechanisms trained via reinforcement learning.", "description": "This paper presents ChartSketcher, a multimodal large language model that performs step-by-step reasoning for chart understanding by sketching directly on chart images. The approach employs Sketch-CoT (Chain-of-Thought with sketching) where the model generates programmatic drawing commands to mark key visual elements, receives visual feedback of these annotations, and iteratively refines its reasoning. Training involves a two-stage process: cold start phase using cross-modal distillation from LLMs to learn sketch-based reasoning patterns, followed by off-policy reinforcement learning with a novel Sketch-MCTS algorithm to enhance reflection and generalization capabilities.", "key_contribution": "The main contribution is the Sketch-CoT framework that enables MLLMs to perform visual reasoning through programmatic sketching with multimodal feedback loops, combined with a two-stage training methodology (cold start + RL with Sketch-MCTS) that teaches the model to visually ground its reasoning and self-correct errors through reflection.", "novelty": "Unlike existing text-based CoT methods that struggle with visual reasoning or cropping-based approaches limited to single regions, ChartSketcher enables simultaneous analysis of multiple visual elements through programmatic sketching. The approach addresses the fundamental limitation that current MLLMs cannot refine reasoning when errors stem from flawed visual understanding, by introducing a closed-loop mechanism of action, perception, and reflection through visual feedback. The novel Sketch-MCTS algorithm enables effective off-policy RL training on unlabeled data while maintaining step-level contrastive learning between correct and incorrect reasoning paths.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Visual Grounding and Spatial Reasoning"], "score": 67.42, "session_type": "San Diego Poster Session 4"}, {"paper_id": 3227182, "title": "REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning", "authors": "Sungho Jeon, Xinyue Ma, Kwang In Kim, Myeongjae Jeon", "pdf_url": "https://openreview.net/pdf/6ac620875c43ec32aad37e9348fd7ad1d1c6e2b0.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55441, "poster_number": 4011, "tag": "SD-4-4011 | Exhibit Hall C,D,E ", "relevance_score": 66.34, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "REP achieves up to 51% reduction in training time and 41% reduction in memory usage for prompt-based continual learning methods with only marginal accuracy loss (0.0-1.2%). The framework demonstrates that shallower layers in pre-trained ViTs are more sensitive to new tasks than deeper layers, enabling non-uniform optimization strategies. REP's techniques are generalizable beyond prompting methods, showing 37-48% training time reduction when applied to non-prompting rehearsal-free CL methods.", "description": "This paper introduces Resource-Efficient Prompting (REP), a framework that optimizes computational and memory efficiency of prompt-based rehearsal-free continual learning methods on vision transformers for edge deployment. REP employs three complementary techniques: a lightweight surrogate model for prompt selection, adaptive token merging (AToM) that preserves task-specific features while merging redundant tokens, and adaptive layer dropping (ALD) that non-uniformly skips model layers based on their importance to task adaptation.", "key_contribution": "The main contribution is a comprehensive framework combining three novel techniques (surrogate-based prompt selection, AToM, and ALD) that achieve substantial resource efficiency gains in prompt-based continual learning while maintaining accuracy. The key innovation is the adaptive, non-uniform approach to computation skipping that accounts for layer-wise differences in task-specific information preservation.", "novelty": "Unlike conventional token merging and layer dropping methods that treat all layers uniformly, REP introduces adaptive strategies that preserve critical task-specific information in shallower layers while aggressively optimizing deeper layers. The work addresses the limitation of existing prompt-based CL methods being too resource-intensive for edge deployment by analyzing attention patterns across layers and developing layer-aware optimization. REP is the first to systematically optimize both prompt selection and update stages with complementary techniques specifically designed for continual learning scenarios.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation"], "score": 66.34, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4442661, "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models", "authors": "Xiaoyu Zhan, Wenxuan Huang, Hao Sun, Xinyu Fu, Changfeng Ma, Shaosheng Cao, Bohan Jia, Shaohui Lin, Zhenfei Yin, ..., Yanwen Guo", "pdf_url": "https://openreview.net/pdf/1321dbcd7c95604245189132069ca5ac16d3058c.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55680, "poster_number": 4701, "tag": "SD-4-4701 | Exhibit Hall C,D,E ", "relevance_score": 65.66, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Current MLLMs struggle with spatial reasoning because they rely on 2D visual cues rather than understanding 3D consistency across views. The paper demonstrates that explicit training on viewpoint understanding tasks significantly activates spatial reasoning abilities in MLLMs, leading to substantial improvements on both in-domain and out-of-domain benchmarks. The proposed Actial model achieves performance comparable to or better than larger proprietary models on spatial reasoning tasks.", "description": "This paper introduces Actial, a framework to activate spatial reasoning abilities in Multimodal Large Language Models through viewpoint learning. The approach uses a two-stage fine-tuning strategy: first, supervised fine-tuning on the Viewpoint-100K dataset (100K object-centric image pairs with viewpoint-related questions) to inject foundational spatial knowledge, followed by reinforcement learning using GRPO on the SAT dataset to enhance generalization. A hybrid cold-start initialization method with pseudo chain-of-thought templates maintains coherent reasoning while learning viewpoint representations.", "key_contribution": "The main contribution is demonstrating that foundational viewpoint learning can effectively activate spatial reasoning in MLLMs. The paper introduces the Viewpoint-100K dataset for viewpoint understanding and a two-stage training strategy combining supervised fine-tuning with reinforcement learning that significantly improves performance on diverse spatial reasoning benchmarks without requiring additional 3D features or visual prompts.", "novelty": "Unlike prior work that adds 3D features, visual prompts, or bounding boxes to improve spatial reasoning, this paper focuses on teaching MLLMs to recognize that images are projections of 3D-consistent objects rather than just 2D pixel sequences. The key insight is that current MLLMs rely on 2D continuity instead of understanding 3D consistency, and this can be corrected through targeted training on foundational viewpoint tasks. The hybrid cold-start initialization combining viewpoint learning with chain-of-thought reasoning is also novel, preventing the degradation of instruction-following capabilities during fine-tuning.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Vision-Language Reasoning and Chain-of-Thought", "Multimodal Benchmarks and Evaluation"], "score": 65.66, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4219775, "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning", "authors": "Qiuchen Wang, Ruixue Ding, Yu Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun Xie, Fei Huang, Feng Zhao", "pdf_url": "https://openreview.net/pdf/facbdeba1c5f4027c295d801ab7c24a818c3957f.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55776, "poster_number": 1502, "tag": "SD-4-1502 | Exhibit Hall C,D,E ", "relevance_score": 61.71, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "VRAG-RL introduces a novel reinforcement learning framework that significantly improves vision-language models' ability to retrieve and reason over visually rich documents, achieving 20-30% performance improvements over existing methods. The framework introduces a visual perception action space allowing models to crop and zoom into information-dense regions, combined with a fine-grained reward function that evaluates both retrieval efficiency and answer quality. This approach enables models to gather information from coarse-to-fine perspectives while maintaining stable multi-turn reasoning.", "description": "This paper presents VRAG-RL, a reinforcement learning framework designed for retrieval-augmented generation with visually rich documents. The framework enables vision-language models to interact with search engines through iterative reasoning, using visual perception tokens to select, crop, and zoom into regions of interest within retrieved images. The approach combines GRPO-based training with a specialized reward function that integrates retrieval performance metrics (NDCG-based) with model-based outcome evaluation.", "key_contribution": "The main contribution is a complete RL framework for visual RAG that introduces (1) a visual perception action space enabling dynamic region selection and re-encoding at higher resolution, (2) a comprehensive reward function combining retrieval efficiency and answer quality, and (3) stable multi-turn training strategies that significantly outperform existing prompt-based and RL-based baselines on visually rich document understanding tasks.", "novelty": "Unlike previous visual RAG methods that simply embed images into context, this work introduces visual perception actions (cropping, zooming) that allow models to focus on information-dense regions progressively. It addresses the limitation of inefficient retrieval by incorporating retrieval performance directly into the reward structure using a modified NDCG metric, bridging the gap between user queries and retriever capabilities. The framework also solves the instability problem in multi-turn visual reasoning through carefully designed sampling strategies and model-based rewards, moving beyond simple outcome-based evaluation.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Visual Grounding and Spatial Reasoning"], "score": 61.71, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4172556, "title": "Fast-Slow Thinking GRPO for Large Vision-Language Model Reasoning", "authors": "Wenyi Xiao, Leilei Gan", "pdf_url": "https://openreview.net/pdf/52fe68c14c7abde636af8fd396bd6a026575c7f0.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55607, "poster_number": 1601, "tag": "SD-4-1601 | Exhibit Hall C,D,E ", "relevance_score": 65.03, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "FAST-GRPO achieves state-of-the-art accuracy with over 10% relative improvement compared to base models while reducing token usage by 32.7-67.3% compared to previous slow-thinking approaches. The method demonstrates that LVLMs can maintain reasoning performance across different response lengths, and that question difficulty acts as an implicit regulator of reasoning length. Empirical analysis reveals that current RL-only methods struggle to scale reasoning length effectively, while SFT-RL methods produce overly verbose responses with only marginal accuracy gains.", "description": "This paper presents FAST-GRPO, a variant of Group Relative Policy Optimization that enables large vision-language models to dynamically adapt reasoning depth based on question characteristics. The approach introduces two complementary metrics to estimate question difficulty (extrinsic difficulty via pass@k and intrinsic difficulty via image complexity), incorporates adaptive length-based rewards that encourage concise responses for simple questions and detailed reasoning for complex ones, and implements difficulty-aware KL divergence to modulate exploration constraints during reinforcement learning training.", "key_contribution": "The main contribution is a reinforcement learning framework that achieves adaptive fast-slow thinking in vision-language models through difficulty-aware length rewards and dynamic KL regularization, effectively balancing reasoning accuracy and efficiency without requiring large-scale distilled data or behavior cloning from slow-thinking models.", "novelty": "Unlike existing approaches that either fail to scale reasoning length (RL-only methods) or produce uniformly verbose outputs (SFT-RL methods), FAST-GRPO dynamically adjusts reasoning depth based on question characteristics. The work introduces novel difficulty estimation metrics combining visual complexity (GLCM entropy and ViT classification entropy) with empirical success rates, and implements difficulty-conditioned reward shaping that encourages concise reasoning for simple questions while promoting thorough exploration for complex ones. This is the first work to effectively address the overthinking phenomenon in LVLMs through adaptive curriculum learning and difficulty-aware optimization.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Prompt Learning and Optimization"], "score": 65.03, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4434221, "title": "Bridging the gap to real-world language-grounded visual concept learning", "authors": "Whie Jung, Semin Kim, Junee Kim, Seunghoon Hong", "pdf_url": "https://openreview.net/pdf/0bcb1846aed7fb3d788d751b0129765e4861a9ab.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55840, "poster_number": 5010, "tag": "SD-4-5010 | Exhibit Hall C,D,E ", "relevance_score": 65.09, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces a scalable framework for learning diverse visual concepts in real-world images by adaptively discovering concept axes using vision-language models, rather than relying on predefined primitive axes. The method achieves superior performance on visual concept editing tasks and demonstrates strong compositional generalization, successfully creating novel concept combinations that don't exist in training data. The compositional anchoring objective enables both concept disentanglement and preservation of image-specific visual nuances.", "description": "The paper presents a framework for language-grounded visual concept learning that automatically identifies image-related concept axes (e.g., age, fur color, breed) using a pretrained VLM with universal prompting, then encodes visual concepts along these axes using a universal concept encoder. A compositional anchoring loss ensures concepts remain disentangled by verifying that changes in one axis only affect that axis in generated images, while preserving instance-specific details that text-based methods cannot capture.", "key_contribution": "The main contribution is a scalable framework that combines adaptive concept axis discovery via VLM prompting, a universal concept encoder architecture that handles arbitrary axes without specialized encoders, and a novel compositional anchoring objective that achieves concept disentanglement while preserving visual nuances better than prior methods.", "novelty": "Unlike prior work that requires manually predefined primitive axes (color, shape) and uses specialized encoders per concept, this work adaptively discovers diverse concept axes for each image and uses a single universal encoder. The compositional anchoring loss addresses the fundamental trade-off in previous methods between concept disentanglement and encoding image-specific details by verifying independence in the generated output space rather than directly aligning with text embeddings. This enables the method to scale to complex real-world datasets like ImageNet with diverse, image-dependent concepts.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Compositional and Counterfactual Reasoning", "Prompt Learning and Optimization"], "score": 65.09, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4441423, "title": "Unlabeled Data Can Provably Enhance In-Context Learning of Transformers", "authors": "Renpu Liu, Jing Yang", "pdf_url": "https://openreview.net/pdf/fbbc09553e40eaaefc993ce079b6a800e2f94653.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55582, "poster_number": 3108, "tag": "SD-4-3108 | Exhibit Hall C,D,E ", "relevance_score": 60.77, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper provides the first theoretical proof that unlabeled data can provably enhance in-context learning (ICL) performance of transformers. The authors demonstrate that with chain-of-thought prompting, a multi-layer transformer can implement an EM-style algorithm to leverage both labeled and unlabeled data, achieving excess risk of O(1/‚àöN¬∑poly(M)) compared to the Œ©(1/‚àöN) lower bound for classifiers using only labeled data. They also prove linear convergence of transformer parameters under teacher forcing training.", "description": "The paper introduces an augmented ICL framework where transformers process prompts containing both a small set of labeled examples and a larger set of unlabeled inputs for multi-class linear classification. Using chain-of-thought prompting, the transformer iteratively refines class mean estimates through an expectation-maximization approach, extracting useful statistical information from unlabeled data to improve prediction accuracy beyond what is possible with labeled data alone.", "key_contribution": "The main contribution is the first theoretical characterization showing that transformers can provably leverage unlabeled data to improve ICL performance, with explicit construction of a 4-layer transformer that implements an EM algorithm via CoT prompting and proof of linear convergence under teacher forcing training.", "novelty": "Unlike prior work on ICL that relies solely on labeled demonstrations or generates pseudo-labels (which inherit model biases), this work directly utilizes unlabeled data within the prompt without labeling. It provides the first non-asymptotic convergence guarantees for semi-supervised ICL in multi-class settings with realistic transformer architectures including softmax attention, contrasting with concurrent work that focuses on linear transformers in binary classification with asymptotic analysis.", "ai_categories": ["In-Context Learning and Few-Shot Adaptation", "Prompt Learning and Optimization"], "score": 60.77, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4213234, "title": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "authors": "Shuang Zeng, Xinyuan Chang, Mengwei Xie, Xinran Liu, Yifan Bai, Zheng Pan, Mu Xu, Xing Wei", "pdf_url": "https://openreview.net/pdf/fbba0847160a598188317e0885484ad6c0596097.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55968, "poster_number": 5017, "tag": "SD-4-5017 | Exhibit Hall C,D,E ", "relevance_score": 64.03, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "FutureSightDrive introduces visual spatio-temporal Chain-of-Thought (CoT) reasoning for autonomous driving, enabling VLAs to think in images rather than text. The method achieves superior trajectory planning performance on nuScenes and NAVSIM benchmarks while maintaining competitive future frame generation quality (FID 10.1) using lightweight autoregressive generation. The unified pre-training paradigm activates visual generation capabilities in existing MLLMs with only 0.3% of the training data required by previous methods.", "description": "This paper proposes FSDrive, a Vision-Language-Action framework that uses visual spatio-temporal CoT for end-to-end autonomous driving. The model functions as both a world model (generating unified future frames with lane dividers and 3D boxes overlaid) and an inverse dynamics model (planning trajectories from current observations and visual CoT). A unified pre-training paradigm enables MLLMs to perform both visual generation and understanding by expanding the vocabulary to include visual tokens while preserving semantic understanding through VQA tasks.", "key_contribution": "The main contribution is a visual spatio-temporal CoT framework that unifies future scene representation and perception outputs in image format, eliminating cross-modal semantic gaps. The progressive easy-to-hard generation method first predicts coarse physical priors (lanes, 3D boxes) then completes detailed future frames, enabling safer trajectory planning through visual reasoning.", "novelty": "Unlike existing methods that use textual CoT or image-text CoT combinations, FSDrive reasons entirely in the visual domain, avoiding information loss from cross-modal conversions. The work demonstrates that visual generation capabilities can be activated in existing MLLMs with minimal training data (0.3% of previous approaches) without complex architectural redesigns. The progressive generation approach enforces physical constraints before detail completion, addressing limitations of direct future frame prediction that may violate physical laws.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Video Understanding and Temporal Reasoning", "Embodied AI and Vision-Language-Action Models"], "score": 64.03, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4441415, "title": "Generative Caching for Structurally Similar Prompts and Responses", "authors": "Sarthak Chakraborty, Suman Nath, Xuchao Zhang, Chetan Bansal, Indranil Gupta", "pdf_url": "https://openreview.net/pdf/df29ee5226893fdb3de7942c122869fbc8f4ced9.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55254, "poster_number": 104, "tag": "SD-4-104 | Exhibit Hall C,D,E ", "relevance_score": 56.52, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "GenCache introduces a novel generative caching approach that achieves 83% cache hit rate on structurally similar prompts while maintaining correctness, compared to semantic caching which produces incorrect responses and exact matching which achieves 0% hits. In agentic workflows, GenCache improves cache hit rates by ~20% and reduces end-to-end execution latency by ~34%, while providing at least 35% token cost savings compared to no caching.", "description": "This paper presents GenCache, a client-side caching system for LLM applications that handles structurally similar prompts with controlled variations. Instead of storing exact responses like traditional caches, GenCache uses an LLM to identify common patterns across similar prompt-response pairs, generates executable Python programs that capture these patterns, validates them, and stores these programs as cache entries that can synthesize customized responses for new prompts.", "key_contribution": "The main contribution is a generative caching technique that stores programs instead of responses, enabling variation-aware response generation for structurally similar prompts. This balances the performance-correctness trade-off by achieving high cache reuse rates without compromising accuracy through local program execution.", "novelty": "Unlike existing exact matching (which fails on similar but non-identical prompts) and semantic caching (which incorrectly returns verbatim responses for similar prompts), GenCache generates custom responses tailored to each prompt's variations. It addresses the limitation of semantic caches producing incorrect results by automatically discovering structural patterns, encoding them as validated programs, and executing these programs locally to synthesize correct, variation-aware responses for prompts following similar templates.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation"], "score": 56.52, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4163509, "title": "CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning", "authors": "Man Ho LAM, Chaozheng Wang, Jen-tse Huang, Michael Lyu", "pdf_url": "https://openreview.net/pdf/9b9883a392dca607cfb2b26eccc24136b0e5cf90.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55928, "poster_number": 2610, "tag": "SD-4-2610 | Exhibit Hall C,D,E ", "relevance_score": 57.13, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "LLMs exhibit severe performance degradation (23.2% average drop) when code contains misleading natural language cues, revealing they over-rely on comments and print statements rather than executable semantics. Chain-of-Thought reasoning only partially mitigates this (13.8% drop), exposing distractibility and rationalization behaviors. Large Reasoning Models show improved robustness but suffer from a novel 'Reasoning Collapse' phenomenon where plausible incorrect hints trigger pathological overthinking, consuming 2-3x more tokens and causing catastrophic failures in QwQ-32B.", "description": "This paper introduces CodeCrash, a stress-testing framework with 1,279 questions evaluating LLM robustness in code reasoning under structural perturbations and misleading natural language contexts. The framework systematically tests 17 LLMs and 3 Large Reasoning Models using functionally equivalent code transformations including aggregated structural changes (PSC-ALL), contextual-level misleading comments/prints (MCC/MPS), and reasoning-level incorrect hints (MHC) to assess whether models prioritize executable semantics over superficial NL cues.", "key_contribution": "The paper reveals that current LLMs lack critical reasoning capability to distinguish actual code behavior from misleading natural language cues, demonstrating systematic rationalization and shortcut-taking behaviors. It identifies a novel 'Reasoning Collapse' failure mode in Large Reasoning Models where plausible incorrect hints trigger uncontrolled recursive self-verification and cognitive dissonance.", "novelty": "Unlike prior work focusing on structural code mutations or prompt-level variations in code generation tasks, this is the first benchmark to systematically use NL-embedded misleading perturbations within code to evaluate reasoning robustness. It extends rationalization findings from multiple-choice tasks to code reasoning, revealing that models treat comments as authoritative execution rather than non-functional context. The work also uncovers pathological overthinking in LRMs, showing that enhanced reasoning mechanisms can amplify rather than resolve uncertainty when faced with plausible contradictions.", "ai_categories": ["Hallucination Detection and Model Robustness", "In-Context Learning and Few-Shot Adaptation", "Compositional and Counterfactual Reasoning"], "score": 57.13, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4224425, "title": "Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules", "authors": "Yueqi Zhang, Peiwen Yuan, Yiwei Li, Shaoxiong Feng, Xinglin Wang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li", "pdf_url": "https://openreview.net/pdf/ccc57d1f272b13a4853fce78582f6e5b42ba120e.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55614, "poster_number": 3517, "tag": "SD-4-3517 | Exhibit Hall C,D,E ", "relevance_score": 56.3, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces QuAda, a lightweight training-based method that enables LLMs to understand and respond to quotations in dialogue by modulating attention on quoted text spans. QuAda achieves state-of-the-art performance across five quotation scenarios while adding less than 2.8% additional parameters and requiring no prompt modifications. The work includes the first benchmark for quotation-aware dialogue covering diverse real-world quoting behaviors.", "description": "The paper formalizes quotation-aware dialogue as span-conditioned generation, where users can reference specific text spans from conversation history along with an intent. It proposes an automated data pipeline to generate training data and benchmarks across five diagnostic scenarios (Base, Multi-Span, Exclude, Info-Combine, and Coref). QuAda uses bottleneck projections attached to attention heads to dynamically amplify or suppress attention to quoted spans without modifying prompts.", "key_contribution": "The main contribution is QuAda, a plug-and-play adapter that injects position-aware span information into LLM attention mechanisms through lightweight query-side and value-side modulation, enabling models to follow quotation-based instructions across diverse scenarios without prompt overhead or significant parameter increases.", "novelty": "Unlike existing methods that rely on content duplication, explicit markers, or static attention scaling, QuAda learns to dynamically modulate attention based on token-level span positions through training. It addresses limitations of prior work by handling multi-span queries, negative constraints (exclusion), context integration, and positional resolution (coreference) in a unified framework. The automated data pipeline with multi-stage consistency checks ensures high-quality training data and benchmark creation.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation"], "score": 56.3, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4210398, "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning", "authors": "Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping", "pdf_url": "https://openreview.net/pdf/4dbb7f099e0ae9ed52f26be78bbd5f18b4adc8a6.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55492, "poster_number": 209, "tag": "SD-4-209 | Exhibit Hall C,D,E ", "relevance_score": 56.36, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that large-scale reinforcement learning can significantly enhance reasoning capabilities of small- and mid-sized models (7B/14B), achieving performance competitive with or surpassing state-of-the-art distillation-based approaches. Notably, math-only RL improves not only math benchmarks (+14.6%/+17.2% on AIME 2025 for 7B/14B) but also code reasoning tasks (+6.8%/+5.8% on LiveCodeBench), with subsequent code-only RL further improving code performance with minimal degradation in math results. The work reveals that RL both elicits foundational reasoning capabilities and pushes models to solve previously unsolvable problems.", "description": "The paper presents AceReason-Nemotron, a systematic study of reinforcement learning for math and code reasoning in language models. The authors propose a sequential training approach: first performing math-only RL with stage-wise length extension (8K‚Üí32K tokens), followed by code-only RL. They develop a robust data curation pipeline for collecting high-quality, verifiable math problems and coding tasks, and conduct extensive ablations to identify key training insights including curriculum learning with progressive response length increases and on-policy parameter updates for stability.", "key_contribution": "The main contribution is demonstrating that RL can significantly outperform distillation for small/mid-sized models when using proper training recipes, and introducing a sequential math-then-code RL approach that achieves cross-domain improvements. The work provides a comprehensive open-source data curation pipeline and detailed ablation studies revealing critical implementation details for successful RL training at scale.", "novelty": "Unlike prior work suggesting distillation is superior to RL for smaller models, this paper shows RL can achieve competitive or better results with proper training strategies. The key novelty is the discovery that math-only RL improves code reasoning performance, enabling a sequential training curriculum that avoids catastrophic forgetting typical of domain-specific supervised fine-tuning. The work also addresses the lack of reproducible implementation details in frontier models by providing comprehensive ablations on curriculum learning, length extension strategies, and on-policy training stabilization techniques.", "ai_categories": ["Reinforcement Learning for Multimodal Models", "Prompt Learning and Optimization"], "score": 56.36, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4209266, "title": "Exploring the Limits of Vision-Language-Action Manipulation in Cross-task Generalization", "authors": "Jiaming Zhou, Ke Ye, Jiayi LIU, Teli Ma, Zifan Wang, Ronghe Qiu, Kun-Yu Lin, Zhilin Zhao, Junwei Liang", "pdf_url": "https://openreview.net/pdf/ba782206825d0f1f5219718d27497e7e2d001e66.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55631, "poster_number": 2406, "tag": "SD-4-2406 | Exhibit Hall C,D,E ", "relevance_score": 54.01, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Current vision-language-action (VLA) models struggle significantly with zero-shot cross-task generalization, with even state-of-the-art models failing on many unseen tasks. The proposed X-ICM method achieves 6.0% improvement over œÄ0 and 7.9% over VoxPoser on the AGNOSTOS benchmark by leveraging in-context learning with dynamics-guided sample selection. The benchmark reveals that existing VLA models, despite training on diverse datasets, lack the ability to generalize to tasks with novel object-action combinations.", "description": "This paper introduces AGNOSTOS, the first benchmark specifically designed to evaluate zero-shot cross-task generalization in robotic manipulation, comprising 23 unseen tasks across two difficulty levels. The authors propose X-ICM (Cross-task In-context Manipulation), a method that uses large language models conditioned on demonstrations from seen tasks to predict actions for unseen tasks, guided by a dynamics-aware sample selection strategy. The work systematically evaluates three categories of VLA models (foundation, human-video pretrained, and in-domain) to reveal fundamental limitations in cross-task generalization.", "key_contribution": "The main contributions are: (1) AGNOSTOS benchmark with 23 unseen tasks for rigorous cross-task generalization evaluation, (2) X-ICM method combining in-context learning with dynamics-guided demonstration selection, and (3) comprehensive evaluation revealing that current VLA models fail to generalize effectively to unseen task combinations despite diverse training data.", "novelty": "Unlike existing benchmarks that focus on within-task visual generalization, AGNOSTOS is the first to systematically evaluate zero-shot cross-task generalization where models must handle entirely new combinations of objects and actions. The work extends in-context learning paradigms from within-task to cross-task settings, introducing a novel dynamics diffusion model that captures task dynamics to select relevant demonstrations across different tasks. This addresses the critical gap in understanding VLA models' ability to generalize beyond their training task distributions.", "ai_categories": ["Embodied AI and Vision-Language-Action Models", "Multimodal Benchmarks and Evaluation", "In-Context Learning and Few-Shot Adaptation"], "score": 54.01, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4441577, "title": "DynaAct: Large Language Model Reasoning with Dynamic Action Spaces", "authors": "Xueliang Zhao, Wei Wu, Jian Guan, Qintong Li, Lingpeng Kong", "pdf_url": "https://openreview.net/pdf/45820087f0186edb228a8b251bb2af4a3cca38b5.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55896, "poster_number": 5304, "tag": "SD-4-5304 | Exhibit Hall C,D,E ", "relevance_score": 53.45, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "DynaAct introduces a novel framework for automatically constructing compact, dynamic action spaces in LLM reasoning using submodular optimization. The method achieves significant improvements across six benchmarks, with a notable 6.8% absolute gain over rStar on MATH-500, while maintaining efficient inference without substantial latency overhead. The approach demonstrates that principled action space construction can enhance sequential reasoning more effectively than manually designed or unstructured action spaces.", "description": "This paper proposes DynaAct, a framework for enhancing LLM reasoning in complex problem-solving by dynamically constructing compact action spaces. The method first estimates a proxy action space by extracting general reasoning patterns from a diverse corpus, then uses a submodular function that jointly evaluates candidate actions based on utility and diversity to select optimal action sets at each reasoning step. The framework integrates with Monte Carlo Tree Search for action evaluation while keeping the base LLM frozen.", "key_contribution": "The main contribution is a submodular optimization framework for dynamic action space construction in LLM reasoning that balances utility and diversity. This enables scalable, compact action spaces that significantly improve reasoning performance across general, reasoning, and math tasks without requiring manual engineering or introducing substantial computational overhead.", "novelty": "Unlike existing approaches that either use manually defined action spaces (limiting scalability) or search the entire natural language space (requiring powerful models), DynaAct automatically learns action spaces from data while maintaining compactness through submodular optimization. The method addresses the limitation of heuristically designed action spaces by using a principled Q-learning objective to train embeddings that capture action utility, and employs a greedy algorithm with theoretical guarantees to efficiently select diverse, high-value action subsets at each reasoning step.", "ai_categories": ["Chain-of-Thought", "Prompt Learning and Optimization", "Reinforcement Learning for Multimodal Models"], "score": 53.45, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4442042, "title": "Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation", "authors": "Wenbin An, Jiahao Nie, Feng Tian, Haonan Lin, mingxiang cai, Yaqiang Wu, QianYing Wang, Xiaoqin Zhang, Shijian Lu", "pdf_url": "https://openreview.net/pdf/40d94836204a19bf22a4813c820925434476760b.pdf", "session_id": 551, "session_name": "San Diego Oral 5", "poster_id": 56123, "poster_number": 999, "tag": "Oral 5B | 10:00 Upper Level Ballroom 6AB ", "relevance_score": 50.5, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper identifies two critical factors limiting knowledge utilization in Multimodal Large Language Models (MLLMs): attention bias toward visual tokens over contextual tokens, and conflicts between parametric and contextual knowledge. The proposed ALFAR method achieves substantial performance improvements (up to 6.6% on multi-choice datasets) across multiple MLLMs and benchmarks without requiring additional training, demonstrating that MLLMs can better leverage retrieved knowledge through adaptive attention reallocation and logits fusion.", "description": "The paper presents ALFAR (Adaptive Logits Fusion and Attention Reallocation), a training-free plug-and-play approach that enhances Multimodal Retrieval Augmented Generation (MRAG) by addressing how MLLMs utilize retrieved contextual knowledge. ALFAR operates through two mechanisms: (1) adaptively reallocating attention from visual tokens to query-relevant context tokens based on retrieval similarity and query-context relevance, and (2) decoupling and dynamically weighting parametric versus contextual knowledge at the output logits level to mitigate knowledge conflicts.", "key_contribution": "The main contribution is a training-free framework that simultaneously addresses attention bias and knowledge conflicts in MLLMs through adaptive attention reallocation and dynamic logits fusion, enabling more effective utilization of retrieved contextual knowledge for knowledge-intensive vision-language tasks.", "novelty": "Unlike prior methods that focus primarily on contrastive decoding or hallucination mitigation, ALFAR uniquely addresses both attention distribution issues and knowledge conflicts simultaneously. It introduces query-context relevance-based attention reallocation in shallow and deep layers, and adaptively balances parametric and contextual knowledge based on attention weights rather than treating them uniformly. This approach differs from existing RAG methods by explicitly modeling and mitigating the attention bias toward visual tokens and providing a principled way to resolve conflicts between internal and external knowledge sources.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Hallucination Detection and Model Robustness", "In-Context Learning and Few-Shot Adaptation"], "score": 50.5, "session_type": "San Diego Oral 5"}, {"paper_id": 4209419, "title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex", "authors": "Muquan Yu, Mu Nan, Hossein Adeli, Jacob S. Prince, John A. Pyles, Leila Wehbe, Margaret Marie Henderson, Michael J. Tarr, Andrew Luo", "pdf_url": "https://openreview.net/pdf/940e87e2f014c1f3d9c0b85364c63c2d437a1d9e.pdf", "session_id": 549, "session_name": "San Diego Poster Session 4", "poster_id": 55743, "poster_number": 2113, "tag": "SD-4-2113 | Exhibit Hall C,D,E ", "relevance_score": 53.1, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "BraInCoRL achieves 94-99% of fully-trained model performance using only 100 in-context images (versus 9,000 for traditional approaches), demonstrating 5-10x data efficiency. The model successfully generalizes across different scanners, subjects, and acquisition protocols without any fine-tuning. When paired with vision-language models, it enables zero-shot natural language-based characterization of cortical selectivity.", "description": "This paper presents BraInCoRL, a meta-learning framework that uses transformer-based in-context learning to predict voxelwise neural responses in human visual cortex from fMRI data. The model treats each voxel as a meta-learning task and learns across multiple subjects during training, enabling it to adapt to new subjects with minimal data by conditioning on a small set of image-response pairs without any parameter updates.", "key_contribution": "The first meta-learned, in-context visual cortex encoder that can predict subject-specific voxelwise fMRI responses from novel stimuli using only a handful of examples, without requiring subject-specific fine-tuning or extensive data collection.", "novelty": "Unlike existing approaches that require thousands of images and subject-specific training for each individual, BraInCoRL leverages meta-learning across voxels and subjects combined with in-context learning across stimuli. This enables training-free adaptation to new subjects and scanners, addressing the critical limitation of expensive, time-intensive fMRI data collection. The voxel-level parameterization handles inter-subject anatomical and functional variability without assuming spatial correspondence.", "ai_categories": ["In-Context Learning and Few-Shot Adaptation", "Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought"], "score": 53.1, "session_type": "San Diego Poster Session 4"}, {"paper_id": 4440161, "title": "Test-Time Adaptive Object Detection with Foundation Model", "authors": "Yingjie Gao, Yanan Zhang, Zhi Cai, Di Huang", "pdf_url": "https://openreview.net/pdf/b306a99a829c5ac70afc15277fe7ba935e41fc66.pdf", "session_id": 550, "session_name": "Mexico City Poster Session 4", "poster_id": 56111, "poster_number": 999, "tag": "MC-4-999 | Foyer ", "relevance_score": 53.74, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces the first foundation model-powered test-time adaptive object detection method that eliminates the need for source data and overcomes traditional closed-set limitations. The method achieves state-of-the-art performance on cross-corruption and cross-dataset benchmarks, outperforming previous methods by significant margins (11.0% improvement on Pascal-C). The approach successfully adapts to arbitrary cross-domain and cross-category target data through multi-modal prompt tuning and instance memory mechanisms.", "description": "The paper proposes a Multi-modal Prompt-based Mean-Teacher framework for vision-language detector-driven test-time adaptation, built on Grounding DINO. The method incorporates text and visual prompt tuning to adapt both language and vision representation spaces on test data in a parameter-efficient manner. It introduces an Instance Dynamic Memory (IDM) module with Memory Enhancement and Memory Hallucination strategies to leverage high-quality pseudo-labels from previous test samples for improved adaptation.", "key_contribution": "The main contribution is the first foundation model-powered test-time adaptive object detection method that requires no source data while possessing open-vocabulary capability. The method combines parameter-efficient multi-modal prompt tuning with a Test-time Warm-start strategy and novel memory-based strategies (Memory Enhancement and Memory Hallucination) to effectively adapt vision-language detectors during test time.", "novelty": "Unlike existing TTAOD methods that require source domain statistical characteristics and assume identical category spaces between source and target domains, this work leverages vision-language foundation models to enable source-free and open-vocabulary adaptation. The paper introduces novel components including visual prompt tuning with Test-time Warm-start initialization, and Instance Dynamic Memory with two strategies (Memory Enhancement and Memory Hallucination) to maintain and leverage high-quality pseudo-labels across the test stream. This addresses the limitations of traditional closed-set TTAOD while achieving parameter-efficient adaptation.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation", "Visual Grounding and Spatial Reasoning"], "score": 53.74, "session_type": "Mexico City Poster Session 4"}, {"paper_id": 4415825, "title": "BLINK-Twice: You see, but do you observe?  A Reasoning Benchmark on Visual Perception", "authors": "junyan ye, DONGZHI JIANG, Jun He, Baichuan Zhou, Zilong Huang, Zhiyuan Yan, Hongsheng Li, Conghui He, Weijia Li", "pdf_url": "https://arxiv.org/pdf/2510.09361", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56742, "poster_number": 5203, "tag": "SD-5-5203 | Exhibit Hall C,D,E ", "relevance_score": 94.91, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Current state-of-the-art MLLMs struggle with vision-centric reasoning tasks, with even GPT-4o and Gemini-2.5 Pro achieving suboptimal performance on metrics requiring genuine visual understanding. While chain-of-thought reasoning improves performance, it often produces redundant reasoning chains, and models benefit significantly from repeated image observation rather than single-pass perception. The latest o3 model demonstrates a promising new paradigm through active visual reasoning via dynamic image operations like cropping and rotation.", "description": "BLINK-Twice is a vision-centric reasoning benchmark containing 345 challenging images across 7 types of visual challenges (visual misleading, dislocation, art illusion, occlusion, forced perspective, physical illusion, and motion illusion). The benchmark includes 103 natural adversarial image pairs generated using GPT-4o's editing capabilities and 1,725 annotated reasoning steps to evaluate not just final answers but the quality of reasoning processes. It evaluates 20 leading MLLMs including foundation models and reasoning-enhanced variants.", "key_contribution": "The paper introduces a comprehensive benchmark that shifts evaluation from language-based reasoning to image-grounded reasoning, requiring models to truly 'observe' rather than just 'see' by incorporating natural adversarial samples that enforce visual reliance and annotated reasoning chains for fine-grained process evaluation beyond answer accuracy.", "novelty": "Unlike existing multimodal reasoning benchmarks that treat visual input as replaceable context and focus on language-based reasoning, BLINK-Twice requires reasoning from visual content alone through challenging perceptual tasks. It addresses the limitation that prior benchmarks assess shallow perception by introducing tasks requiring fine-grained observation and analytical reasoning, combined with natural adversarial pairs that prevent shortcut learning. The benchmark also evaluates reasoning process quality through annotated chains, revealing that current models need new paradigms involving active visual interaction rather than single-pass perception.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought", "Visual Grounding and Spatial Reasoning"], "score": 94.91, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4209881, "title": "GRIT: Teaching MLLMs to Think with Images", "authors": "Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Xinze Guan, Xin Eric Wang", "pdf_url": "https://openreview.net/pdf/87fbb0dd229346697981e700a4a0dd02546ee154.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56194, "poster_number": 4711, "tag": "SD-5-4711 | Exhibit Hall C,D,E ", "relevance_score": 87.58, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "GRIT enables MLLMs to generate reasoning chains that interleave natural language with explicit bounding box coordinates, achieving exceptional data efficiency with as few as 20 training samples. The method successfully unifies grounding and reasoning abilities that were previously disconnected in MLLMs, demonstrating that generated bounding boxes increase visual attention in subsequent reasoning steps and improve both answer accuracy and grounding precision.", "description": "This paper introduces GRIT (Grounded Reasoning with Images and Text), a method for training Multimodal Large Language Models to perform visual reasoning by generating chains of thought that mix natural language with bounding box coordinates pointing to relevant image regions. The approach uses a novel reinforcement learning algorithm called GRPO-GR that employs rewards based on answer accuracy and output format, eliminating the need for annotated reasoning chains or bounding box labels.", "key_contribution": "The main contribution is a novel grounded reasoning paradigm where MLLMs generate reasoning chains interleaving text and bounding boxes, trained via GRPO-GR reinforcement learning that requires only image-question-answer triplets without dense annotations. This achieves the first open-source approach for MLLMs to 'think with images' in an interpretable and verifiable manner.", "novelty": "Unlike existing visual reasoning models that generate pure natural language reasoning or treat grounding and reasoning as separate tasks, GRIT enables dynamic interplay where visual evidence directly informs textual logic within a single generative trace. The method addresses the limitation of requiring expensive human-annotated reasoning chains and bounding box labels by using task-level reward signals alone, and differs from supervised approaches by learning to integrate grounding into the reasoning process rather than just mimicking surface forms.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Visual Grounding and Spatial Reasoning"], "score": 87.58, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4272135, "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning", "authors": "Xi Chen, Mingkang Zhu, Shaoteng Liu, Xiaoyang Wu, Xiaogang Xu, Yu Liu, Xiang Bai, Hengshuang Zhao", "pdf_url": "https://openreview.net/pdf/96a88a23f63cd49accb6442bf71166bb7c2723de.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56986, "poster_number": 5303, "tag": "SD-5-5303 | Exhibit Hall C,D,E ", "relevance_score": 93.71, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "MiCo achieves state-of-the-art performance on multi-image reasoning benchmarks without requiring any human-annotated question-answer pairs, using only self-supervised visual constraints. The method demonstrates that reasoning abilities learned through visual comparison tasks generalize effectively to diverse multi-image understanding scenarios, significantly outperforming existing models including GPT-4o on VLM2-Bench.", "description": "This paper presents MiCo (Multi-image Contrast), a method that enables Vision-Language Models to perform Chain-of-Thought reasoning across multiple images through self-supervised learning. The approach constructs image triplets from video frames and editing datasets, where models learn to compare subtle visual differences through reinforcement learning, without relying on manually curated QA pairs. An Augmented GRPO strategy samples trajectories on weakly augmented images and optimizes on strongly augmented ones.", "key_contribution": "The main contribution is a self-supervised framework that leverages inherent visual constraints in images (via contrastive triplets) as supervision signals for reinforcement learning, eliminating the need for human-annotated data while achieving superior multi-image reasoning capabilities through visual comparison tasks.", "novelty": "Unlike existing methods that rely on manually curated or model-generated question-answer pairs, MiCo draws inspiration from self-supervised contrastive learning to use images as their own source of supervision. The work introduces Augmented GRPO, which samples reasoning trajectories from easier augmented views and optimizes on harder ones, and demonstrates that meta-cognitive visual comparison abilities learned from simple same/different tasks generalize to complex multi-image reasoning scenarios.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Multimodal Benchmarks and Evaluation"], "score": 93.71, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4209968, "title": "Pixel Reasoner: Incentivizing Pixel Space Reasoning via Curiosity-Driven Reinforcement Learning", "authors": "Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, Wenhu Chen", "pdf_url": "https://openreview.net/pdf/1cff63c2ec284033d02664f1b348bc05165216ea.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56688, "poster_number": 511, "tag": "SD-5-511 | Exhibit Hall C,D,E ", "relevance_score": 91.35, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces pixel-space reasoning, enabling Vision-Language Models to perform reasoning operations directly on visual inputs (zoom-in, select-frame) rather than purely in text. The 7B Pixel-Reasoner model achieves state-of-the-art results on visually-intensive benchmarks (84% on V*bench, 74% on TallyQA-Complex, 84% on InfographicsVQA), outperforming larger models and even proprietary systems like Gemini-2.5-Pro. The work identifies a 'learning trap' during training where models bypass nascent visual operations, and addresses it through curiosity-driven reinforcement learning.", "description": "The paper presents a framework for training Vision-Language Models to reason directly in pixel-space by incorporating visual operations (zoom-in for images, select-frame for videos) as integral reasoning steps. The approach uses a two-phase training strategy: warm-start instruction tuning with synthesized reasoning traces (7,500 examples), followed by curiosity-driven reinforcement learning that incentivizes exploration of pixel-space operations while preventing premature abandonment of these skills. The method enables VLMs to actively manipulate and interrogate visual inputs rather than relying solely on textual reasoning.", "key_contribution": "The main contribution is introducing the novel concept of pixel-space reasoning where VLMs perform reasoning operations directly on visual inputs through computational manipulations, combined with a curiosity-driven RL training approach that overcomes the learning trap of imbalanced competence between textual and visual reasoning capabilities.", "novelty": "Unlike existing VLMs that perform chain-of-thought reasoning exclusively in textual space, this work enables models to reason directly in the visual modality through operations like zoom-in and frame selection. It addresses the limitation that models with imbalanced capabilities tend to bypass nascent skills during standard RL training by introducing a curiosity-driven reward scheme that provides intrinsic bonuses for attempting pixel-space operations. The approach differs from prior tool-augmented VLMs by cultivating these capabilities as native reasoning skills rather than external tool calls.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Visual Grounding and Spatial Reasoning"], "score": 91.35, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4212868, "title": "Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning", "authors": "Van Yang, Zirui Liu, Hongye Jin, Qingyu Yin, Vipin Chaudhary, Xiaotian Han", "pdf_url": "https://openreview.net/pdf/3aa73c81186850fe236acaa4a81d2af3f1e5c910.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56501, "poster_number": 4200, "tag": "SD-5-4200 | Exhibit Hall C,D,E ", "relevance_score": 77.52, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that long-context capability is fundamentally important for reasoning performance in LLMs, even on short-input tasks. Models with stronger long-context abilities (e.g., 128k vs 32k tokens) consistently achieve 3-6% higher accuracy on mathematical reasoning benchmarks after supervised fine-tuning. The study reveals that enhancing long-context capacity before reasoning fine-tuning yields significant performance gains, with improvements from 85.04% to 88.70% on MATH500 and 15.00% to 28.00% on AIME.", "description": "The paper investigates the previously unexplored relationship between long-context processing capability and reasoning performance in large language models. Through controlled experiments using models with identical architectures but varying context lengths (achieved via RoPE theta scaling and model merging), the authors systematically evaluate how long-context ability affects reasoning after supervised fine-tuning on both short (0-8k tokens) and long (8-16k tokens) reasoning datasets across multiple benchmarks including MATH500, AIME, and GSM8K.", "key_contribution": "The main contribution is establishing that long-context capability serves as a critical foundation for reasoning ability, proposing a training recipe that advocates extending context length (e.g., to 128k tokens) before reasoning fine-tuning. This finding challenges the conventional focus solely on training paradigms and dataset quality for improving reasoning.", "novelty": "Unlike prior work that focuses on supervised fine-tuning strategies and dataset quality, this paper is the first to systematically investigate how long-context modeling capability affects reasoning performance. The work addresses the limitation that modern reasoning datasets contain sequences exceeding 10k tokens, yet models' effective context handling has not been studied in this context. The novel approach uses controlled experiments with RoPE scaling and model merging to isolate long-context ability as the key variable while keeping other factors constant.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Multimodal Benchmarks and Evaluation"], "score": 77.52, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4440080, "title": "Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection", "authors": "Chanhyeong Yang, Taehoon song, Jihwan Park, Hyunwoo J. Kim", "pdf_url": "https://openreview.net/pdf/50b2801b8c0fac609e33847c31b795508b0bb09f.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56626, "poster_number": 4911, "tag": "SD-5-4911 | Exhibit Hall C,D,E ", "relevance_score": 83.02, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that zero-shot HOI detection suffers from two key challenges: intra-class visual diversity (same verb appearing in varied poses/contexts) and inter-class visual entanglement (different verbs producing similar visual patterns). The proposed VDRP framework achieves state-of-the-art performance across four zero-shot settings on HICO-DET by addressing both challenges through visual diversity-aware prompt learning and region-aware prompt augmentation. Quantitative analysis shows verbs exhibit significantly higher intra-class diversity (0.364¬±0.060) compared to objects (0.274¬±0.048), validating the need for variance-aware modeling.", "description": "The paper presents VDRP (Visual Diversity and Region-aware Prompt learning), a framework for zero-shot Human-Object Interaction detection that leverages CLIP vision-language models. The approach introduces two complementary modules: (1) visual diversity-aware prompt learning that injects group-wise visual variance and Gaussian perturbation into context embeddings to capture intra-class variation, and (2) region-aware prompt augmentation that retrieves and fuses region-specific concepts from human, object, and union regions to enhance verb discriminability.", "key_contribution": "The main contribution is a dual-module framework that models both intra-class visual diversity through variance-aware prompt learning and inter-class discriminability through region-specific concept retrieval, achieving state-of-the-art zero-shot HOI detection performance while using fewer trainable parameters (4.50M) than competing methods.", "novelty": "Unlike existing prompt learning methods that use static single prompts per verb, this work explicitly models visual diversity by injecting group-wise variance computed from visual features and applying Gaussian perturbation to prompt embeddings. It addresses the limitation of region-agnostic prompts by retrieving and incorporating LLM-generated region-specific concepts (human, object, union) using Sparsemax selection. This variance-grounded and region-aware approach differs from prior works that either ignore visual diversity or rely solely on semantic descriptions without visual grounding.", "ai_categories": ["Prompt Learning and Optimization", "Visual Grounding and Spatial Reasoning"], "score": 83.02, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4385508, "title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model", "authors": "Pengteng Li, Pinhao Song, Wuyang Li, Huizai Yao, Weiyu Guo, Yijie Xu, Dugang Liu, Hui Xiong", "pdf_url": "https://openreview.net/pdf/bcef03fe8da8c459269e00eeb6626d6584a0ba4c.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56463, "poster_number": 5206, "tag": "SD-5-5206 | Exhibit Hall C,D,E ", "relevance_score": 80.89, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "SEE&TREK is the first training-free and GPU-free prompting framework that enhances spatial understanding in Multimodal Large Language Models (MLLMs) using only visual inputs. The method achieves consistent performance improvements (up to +3.5%) across multiple MLLMs on challenging spatial reasoning benchmarks (VSI-BENCH and STI-BENCH) through Maximum Semantic Richness Sampling and Motion Reconstruction. The framework requires only a single forward pass and can be seamlessly integrated into existing MLLMs without any model modifications.", "description": "This paper addresses the spatial understanding limitations of MLLMs that arise from visual homogeneity and unknown motion in sampled video frames. SEE&TREK employs two core strategies: (1) Maximum Semantic Richness Sampling using off-the-shelf perception models (YOLO) to extract semantically diverse keyframes via a Balanced-TopK strategy, and (2) Motion Reconstruction using Visual Odometry to simulate camera trajectories and encode spatial-temporal information into frames. The method is evaluated on VSI-BENCH and STI-BENCH across various open-source and proprietary MLLMs.", "key_contribution": "The main contribution is a training-free, plug-and-play spatial prompting framework that enhances MLLMs' spatial reasoning by combining semantic keyframe selection with explicit motion trajectory encoding, requiring no additional training or GPU resources while being compatible with both open-source and commercial models.", "novelty": "Unlike prior work that incorporates depth maps, point clouds, or 3D priors requiring specialized training pipelines, SEE&TREK operates purely on visual inputs without any training or fine-tuning. It addresses visual homogeneity through intelligent semantic sampling rather than uniform temporal sampling, and tackles unknown motion by reconstructing camera trajectories using Visual Odometry and encoding them directly into frames via spatiotemporal markers. This approach differs from existing methods by being completely training-free, GPU-free, and requiring only a single forward pass while still achieving significant improvements in spatial understanding.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Prompt Learning and Optimization", "Video Understanding and Temporal Reasoning"], "score": 80.89, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4427184, "title": "Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions", "authors": "Jihoon Kwon, Kyle Min, Jy-yong Sohn", "pdf_url": "https://openreview.net/pdf/26f0105deb76b3529c3f5d5f168a3e97ff0d230f.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56356, "poster_number": 4608, "tag": "SD-5-4608 | Exhibit Hall C,D,E ", "relevance_score": 76.06, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "READ-CLIP achieves state-of-the-art performance across five major compositional reasoning benchmarks, outperforming the strongest baseline (FSC-CLIP) by up to 4.1% and NegCLIP by an average of 4.5%. The two auxiliary objectives‚Äîtoken-level reconstruction and sentence-level alignment‚Äîprovide complementary benefits: reconstruction encourages capturing relationships between words within captions, while alignment ensures consistent representations for paraphrased sentences. Reconstructing alternative captions rather than original ones mitigates overfitting and enhances generalization.", "description": "This paper introduces READ (REconstruction and Alignment of text Descriptions), a fine-tuning method designed to enhance compositional reasoning in vision-language models like CLIP. The method adds two auxiliary objectives to standard contrastive learning: (1) a token-level reconstruction objective where a frozen decoder reconstructs alternative captions from the original caption's embedding, and (2) a sentence-level alignment objective that explicitly aligns paraphrased sentences in the embedding space. The approach specifically targets the text encoder, which has been identified as the primary bottleneck for compositional reasoning.", "key_contribution": "The main contribution is a novel fine-tuning framework (READ) that enhances compositional reasoning in VLMs by introducing two complementary auxiliary objectives specifically targeting the text encoder: token-level reconstruction of alternative captions and sentence-level alignment of paraphrases. This approach addresses the limitation that text encoders focus on individual words rather than their relationships when trained with standard contrastive objectives.", "novelty": "Unlike prior work that relies solely on hard negatives or supervises both image and text encoders jointly, READ specifically targets the text encoder with auxiliary objectives designed to capture compositional structure. The token-level reconstruction objective uniquely reconstructs alternative captions rather than the original, reducing overfitting to exact wording. The sentence-level alignment explicitly handles paraphrases, addressing the gap where previous methods focused on object-level alignment but neglected relational and semantic consistency across different linguistic expressions.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Compositional and Counterfactual Reasoning", "Prompt Learning and Optimization"], "score": 76.06, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4239206, "title": "Multi-step Visual Reasoning with Visual Tokens Scaling and Verification", "authors": "Tianyi Bai, Zengjie Hu, Fupeng Sun, Qiu Jiantao, Yizhen Jiang, Guangxin He, Bohan Zeng, Conghui He, Binhang Yuan, Wentao Zhang", "pdf_url": "https://openreview.net/pdf/a48656c1e2d8fb5cee742d21b8b328458f483707.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56938, "poster_number": 4813, "tag": "SD-5-4813 | Exhibit Hall C,D,E ", "relevance_score": 86.14, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces VTS-V, a framework that enables MLLMs to perform iterative, verifier-guided visual reasoning through dynamic visual token scaling at inference time. The method achieves significant improvements across visual reasoning benchmarks (e.g., +6.90% on BLINK for GPT-4o, +7.58% for Qwen2.5VL-7B), demonstrating that dynamic visual exploration guided by a multi-step DPO-trained verifier substantially outperforms static inference paradigms. The framework provides theoretical guarantees for bounded reasoning steps while maintaining interpretability through explicit tool-use trajectories.", "description": "The paper addresses the limitation of current MLLMs that encode images into fixed visual tokens upfront, preventing iterative refinement during reasoning. VTS-V formulates visual reasoning as a Markov Decision Process where a reasoner proposes visual actions (using tools like OCR, depth maps, grounding) and a verifier trained via multi-step Direct Preference Optimization evaluates these actions and determines termination. The authors introduce two datasets: VTS-SFT (315K supervised reasoning trajectories) and VTS-DPO (301K preference pairs) to train both components.", "key_contribution": "The main contribution is a theoretically grounded framework for inference-time visual token scaling that combines a reasoner capable of dynamic tool-based visual exploration with a step-wise verifier that provides real-time guidance and ensures bounded reasoning steps. This is supported by novel datasets (VTS-SFT and VTS-DPO) specifically designed for multi-step visual tool reasoning across diverse vision-language tasks.", "novelty": "Unlike existing approaches that use static visual encoding or limited single-step tool use, VTS-V enables dynamic, multi-step visual token scaling through an MDP formulation with formal termination guarantees. The step-wise verifier trained via multi-step DPO operates at each reasoning step (not just final answer ranking), providing real-time guidance that existing verifier methods lack. The framework generalizes to both open-source and closed-source models without task-specific adaptations, and introduces the first comprehensive datasets for training multi-turn visual tool reasoning with preference optimization.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Prompt Learning and Optimization"], "score": 86.14, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4423592, "title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering", "authors": "Yuyang Hong, Jiaqi Gu, Qi Yang, Lubin Fan, Yue Wu, Ying Wang, Kun Ding, Shiming Xiang, Jieping Ye", "pdf_url": "https://openreview.net/pdf/fc97a2f58898074b2cbe0521459da7f6551f2dcb.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56271, "poster_number": 1609, "tag": "SD-5-1609 | Exhibit Hall C,D,E ", "relevance_score": 73.92, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Wiki-PRF achieves state-of-the-art performance on knowledge-based VQA benchmarks (36.0 on E-VQA and 42.8 on InfoSeek) using only 4K training samples compared to methods requiring 934K-1M samples. The method demonstrates that reinforcement learning can effectively train VLMs to autonomously invoke visual tools and filter retrieved information for improved multimodal RAG, representing the first application of RL to multimodal retrieval-augmented generation.", "description": "This paper proposes Wiki-PRF, a three-stage framework (Processing-Retrieval-Filtering) for knowledge-based visual question answering that combines multimodal retrieval with reinforcement learning. The processing stage uses visual tools (captioning, grounding, flipping) to extract precise information, the retrieval stage performs multimodal knowledge retrieval, and the filtering stage uses a VLM trained via RL to remove irrelevant content and concentrate on task-relevant knowledge.", "key_contribution": "The main contribution is VLM-PRF, the first vision-language model trained via reinforcement learning for multimodal RAG tasks, which learns to flexibly invoke visual tools for fine-grained retrieval and filter retrieved information using answer accuracy and format consistency as reward signals, requiring minimal training data while achieving superior performance.", "novelty": "Unlike existing RAG methods that rely on full-image features and paragraph-level reranking, this work introduces tool-based fine-grained retrieval and question-specific filtering at the content level rather than passage level. The key innovation is using reinforcement learning (GRPO) instead of supervised fine-tuning to train the model's RAG capabilities, enabling it to learn optimal tool invocation and information filtering strategies from sparse reward signals without requiring annotated intermediate reasoning steps.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Prompt Learning and Optimization"], "score": 73.92, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4224371, "title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM", "authors": "Bowen Dong, Minheng Ni, Zitong Huang, Guanglei Yang, Wangmeng Zuo, Lei Zhang", "pdf_url": "https://openreview.net/pdf/5c8396fbd51956694870ead9c44afcff4e5253cb.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 57086, "poster_number": 4110, "tag": "SD-5-4110 | Exhibit Hall C,D,E ", "relevance_score": 72.89, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper reveals that model scale, data scale, and training stages significantly affect logical, fabrication, and factual hallucinations in MLLMs, but show limited improvement on spatial hallucinations, indicating weak visual reasoning capabilities. Strong correlations exist between question types and specific hallucination patterns, with logical hallucinations widespread across tasks while spatial errors are particularly prominent in logical reasoning questions. The proposed Logos method combining curriculum reinforcement fine-tuning with collaborative hint inference effectively reduces logical hallucinations by 15.4% and improves overall accuracy.", "description": "The paper introduces MIRAGE, a diagnostic benchmark with 1,329 questions specifically designed to isolate and evaluate reasoning-induced hallucinations in multimodal large language models, separate from perception errors. It provides multi-level annotations including final answers, intermediate reasoning steps, and ground-truth reasoning chains, along with three evaluation metrics: accuracy, factuality (step/claim-level), and LLMs Hallucination Score (LHS). The benchmark enables systematic analysis of five hallucination types: logical, spatial, factuality, context, and fabrication.", "key_contribution": "The main contribution is MIRAGE, the first benchmark specifically designed to isolate reasoning hallucinations from perception errors in MLLMs, featuring multi-granular evaluation metrics and comprehensive annotations. Additionally, the paper proposes Logos, a baseline method using curriculum reinforcement fine-tuning and collaborative hint inference to mitigate reasoning hallucinations.", "novelty": "Unlike existing benchmarks that conflate perception and reasoning errors, MIRAGE uniquely isolates reasoning hallucinations by selecting questions where visual inputs are correctly perceived but reasoning fails. The benchmark introduces fine-grained evaluation at answer, step, claim, and full chain levels, enabling precise diagnosis of error propagation. The proposed Logos method innovatively combines curriculum learning with online reward filtration in reinforcement learning, dynamically adjusting training difficulty while filtering low-impact samples, which proves more effective than standard supervised fine-tuning or training-free approaches.", "ai_categories": ["Hallucination Detection and Model Robustness", "Vision-Language Reasoning and Chain-of-Thought", "Multimodal Benchmarks and Evaluation"], "score": 72.89, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4246473, "title": "Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers", "authors": "Yixiao Huang, Hanlin Zhu, Tianyu Guo, Jiantao Jiao, Somayeh Sojoudi, Michael I. Jordan, Stuart Russell, Song Mei", "pdf_url": "https://openreview.net/pdf/ba09a69b9f3ea21dc345fef9e047ff5eb6a933f0.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56728, "poster_number": 1516, "tag": "SD-5-1516 | Exhibit Hall C,D,E ", "relevance_score": 72.69, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that both generalization and hallucination in LLMs after fine-tuning stem from the same mechanism: out-of-context reasoning (OCR). The authors prove theoretically that a one-layer transformer with factorized output-value matrices can perform OCR through implicit bias toward nuclear norm minimization, while non-factorized models cannot. The work reveals that OCR is highly sample-efficient, explaining why LLMs can both generalize well and hallucinate easily when learning new factual knowledge.", "description": "The paper investigates why large language models exhibit both strong generalization and hallucination tendencies when fine-tuned with new factual knowledge. Through experiments on five prominent LLMs and theoretical analysis of simplified transformer models, the authors show that out-of-context reasoning‚Äîthe ability to deduce implications by associating concepts‚Äîdrives both behaviors depending on whether the associations are causally related or spurious.", "key_contribution": "The main contribution is a rigorous theoretical framework showing that the implicit bias of gradient descent on factorized transformer parameters (separate output and value matrices) leads to nuclear norm minimization, enabling OCR capabilities. This explains both the strong generalization and hallucination tendencies of LLMs, and reveals that common reparameterizations used in theoretical analysis can fundamentally change model behavior.", "novelty": "Unlike prior work that studied OCR empirically or focused on specific failure modes like the reversal curse, this paper provides the first theoretical characterization of why OCR occurs through the lens of optimization dynamics and implicit bias. The work uniquely shows that matrix factorization structure is crucial for OCR‚Äîa finding that challenges common theoretical simplifications. It also unifies the understanding of generalization and hallucination as two sides of the same OCR mechanism, rather than treating them as separate phenomena.", "ai_categories": ["Hallucination Detection and Model Robustness", "In-Context Learning and Few-Shot Adaptation", "Compositional and Counterfactual Reasoning"], "score": 72.69, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4226118, "title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics", "authors": "Dongyoung Kim, Huiwon Jang, Sumin Park, Jaehyung Kim, Younggyo Seo, Jinwoo Shin", "pdf_url": "https://openreview.net/pdf/28f2bd82b713f7bb9565dbe9321b8b6fb4782dc3.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56435, "poster_number": 5308, "tag": "SD-5-5308 | Exhibit Hall C,D,E ", "relevance_score": 68.39, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "ROBOT-R1 demonstrates that reinforcement learning can significantly enhance embodied reasoning for robot control, achieving over 28% improvement on embodied reasoning tasks and outperforming GPT-4o despite having only 7B parameters. The framework shows strong generalization, with 31% improvement on EmbodiedBench Manipulation and ~40-60% improvement on SpatialRGPT benchmarks, while supervised fine-tuning approaches show limited improvement and performance degradation on out-of-distribution tasks.", "description": "This paper introduces ROBOT-R1, a framework that uses reinforcement learning to train Large Vision-Language Models (LVLMs) for enhanced embodied reasoning in robotics. The approach trains models to predict the next keypoint state required for task completion through explicit reasoning processes, reformulating the problem as multiple-choice question answering and optimizing via GRPO algorithm. The paper also introduces ROBOT-R1 Bench, a new benchmark for evaluating embodied reasoning capabilities across planning, high-level action, movement, and spatial reasoning tasks.", "key_contribution": "The main contribution is a novel RL-based framework (ROBOT-R1) that learns generalizable embodied reasoning for robot control by optimizing reasoning pathways for next-state prediction, along with a comprehensive benchmark (ROBOT-R1 Bench) for evaluating diverse embodied reasoning capabilities in robotic manipulation scenarios.", "novelty": "Unlike conventional supervised fine-tuning approaches that use heuristically constructed datasets not optimized for robot control and suffer from catastrophic forgetting, ROBOT-R1 applies reinforcement learning inspired by DeepSeek-R1 to learn and reinforce reasoning pathways specifically for robotic control. The work reformulates continuous next-state prediction as multiple-choice QA to enable efficient RL training, and demonstrates that effective embodied reasoning can emerge from training on low-level control tasks without explicit high-level supervision.", "ai_categories": ["Reinforcement Learning for Multimodal Models", "Embodied AI and Vision-Language-Action Models", "Vision-Language Reasoning and Chain-of-Thought"], "score": 68.39, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4209879, "title": "Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval", "authors": "Siting Li, Xiang Gao, Simon Shaolei Du", "pdf_url": "https://openreview.net/pdf/8197443737fe19c9a91c16f13d0dcf17e31a87d0.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56265, "poster_number": 4702, "tag": "SD-5-4702 | Exhibit Hall C,D,E ", "relevance_score": 62.42, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Current CLIP-like and MLLM-based text-to-image retrievers perform poorly on attribute-focused queries, with significant performance drops compared to general retrieval tasks. The paper introduces COCO-FACET, a benchmark with 9,112 queries across 8 attribute types, and demonstrates that promptable image embeddings can improve Recall@5 by 15% when prompts are predefined and 8% when using linear approximations at inference time.", "description": "This paper investigates text-to-image retrieval for attribute-focused queries (e.g., specific objects, weather, time of day) rather than global image semantics. The authors build the COCO-FACET benchmark to evaluate retrievers on fine-grained attributes and propose using promptable image embeddings enabled by MLLM-based universal embedders, which condition image representations on textual prompts to highlight relevant attributes. Two acceleration strategies are developed: pre-processing embeddings with predefined prompts and deriving linear approximations at test time.", "key_contribution": "The main contribution is the COCO-FACET benchmark for evaluating attribute-focused text-to-image retrieval and a generalizable pipeline for deriving promptable image embeddings that significantly improves retrieval performance on fine-grained attributes, with practical acceleration strategies for real-world deployment.", "novelty": "Unlike previous work that focuses on global image-text alignment, this paper systematically addresses the limitation of general image embeddings for attribute-specific queries by leveraging MLLM-based promptable embeddings. The work differs from region-based approaches (like cropping or bounding boxes) by supporting both regional and global attributes through flexible text prompts, and provides practical acceleration methods that make the approach deployable at scale.", "ai_categories": ["Prompt Learning and Optimization", "Visual Grounding and Spatial Reasoning", "Multimodal Benchmarks and Evaluation"], "score": 62.42, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4438224, "title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "authors": "Zhenyu Zhang, Tianyi Chen, Weiran Xu, Alex Pentland, Jiaxin Pei", "pdf_url": "https://openreview.net/pdf/311841e491eca81acc49d5288503e9b7819788d6.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56465, "poster_number": 4103, "tag": "SD-5-4103 | Exhibit Hall C,D,E ", "relevance_score": 63.04, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "ReCAP achieves substantial improvements on long-horizon reasoning tasks, with 32% gain on synchronous Robotouille and 29% on asynchronous Robotouille under strict pass@1 protocol. The framework demonstrates that structured context management and recursive decomposition significantly outperform sequential prompting methods (ReAct) on tasks requiring multi-step planning, while maintaining competitive performance on shorter tasks. The approach scales linearly with task depth rather than trajectory length, making it memory-efficient for complex reasoning.", "description": "This paper introduces ReCAP (Recursive Context-Aware Reasoning and Planning), a hierarchical prompting framework for LLM agents that addresses long-horizon task planning through three key mechanisms: plan-ahead decomposition that generates complete subtask lists before execution, structured re-injection of parent plans to maintain multi-level context during recursion, and memory-efficient execution with bounded prompt size. The framework is evaluated across embodied AI (Robotouille, ALFWorld), knowledge-intensive (FEVER), and code editing (SWE-bench) benchmarks.", "key_contribution": "The main contribution is a recursive prompting framework that maintains coherent multi-level context through structured injection and backtracking, enabling LLM agents to preserve high-level goals while executing low-level actions without training or fine-tuning. This approach solves the context drift problem in sequential methods and fragmentation issues in hierarchical methods through a shared context tree with dynamic plan refinement.", "novelty": "Unlike sequential prompting methods (ReAct, CoT) that suffer from context drift on long horizons, and hierarchical methods (THREAD, ADaPT) that fragment context across isolated subtask prompts, ReCAP maintains all reasoning within a single evolving LLM context with structured parent-plan re-injection during backtracking. This addresses the fundamental limitation of losing high-level intent in long trajectories while avoiding redundant context duplication. The plan-ahead strategy (generating full subtask lists then executing only the first) combined with observation-driven refinement enables adaptive replanning without the overhead of separate planner-executor loops.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation", "Embodied AI and Vision-Language-Action Models"], "score": 63.04, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4296977, "title": "Cross-modal Associations in Vision and Language Models: Revisiting the Bouba-Kiki Effect", "authors": "Tom Kouwenhoven, Kiana Shahrasbi, Tessa Verhoef", "pdf_url": "https://openreview.net/pdf/58f4768cf595d4f408c01015954cec5e3a3d83c6.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56200, "poster_number": 4700, "tag": "SD-5-4700 | Exhibit Hall C,D,E ", "relevance_score": 62.6, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that CLIP models (ResNet and ViT variants) do not exhibit human-like cross-modal associations as measured by the bouba-kiki effect, contradicting some prior claims. Using both probability-based evaluation and Grad-CAM interpretability analysis, the study shows that while models can match English adjectives to shapes, they fail to consistently associate pseudowords with corresponding visual features. Direct comparison with human data reveals model performance at chance level, suggesting VLMs lack cognitively grounded cross-modal representations despite their strong performance on other tasks.", "description": "The paper investigates whether vision-and-language models exhibit the bouba-kiki effect‚Äîa well-established human tendency to associate certain sounds (like 'bouba') with round shapes and others (like 'kiki') with jagged shapes. Using two complementary methods (probability-based matching and Grad-CAM visual attention analysis) closely modeled after human experiments, the authors comprehensively test two CLIP variants across multiple pseudoword sets and visual stimuli. The work provides direct comparisons with human experimental data to assess whether VLMs integrate cross-modal information in human-like ways.", "key_contribution": "The main contribution is a rigorous empirical demonstration that CLIP models lack human-like cross-modal associations, using both behavioral (probability-based) and mechanistic (Grad-CAM attention) evaluation methods. The paper introduces a novel application of interpretability tools (Grad-CAM) to probe cross-modal reasoning in VLMs, revealing that models attend to shape centers and backgrounds rather than shape-specific features relevant to sound-symbolic associations.", "novelty": "Unlike previous work that found evidence for the bouba-kiki effect in VLM embeddings, this study employs experimental paradigms directly mirroring human psycholinguistic tests with controlled image pairs and diverse pseudowords. The work introduces Grad-CAM as a novel methodology for testing cross-modal associations, examining not just whether models make correct predictions but whether they attend to the right visual features. This addresses limitations of embedding-based methods that may capture language-specific regularities rather than true sensory mappings, as evidenced by contradictory findings across languages.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Hallucination Detection and Model Robustness", "Vision-Language Reasoning and Chain-of-Thought"], "score": 62.6, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4441694, "title": "Jury-and-Judge Chain-of-Thought for Uncovering Toxic Data in 3D Visual Grounding", "authors": "Kaixiang Huang, Qifeng Zhang, Jin Wang, Jingru Yang, Yang Zhou, Huan Yu, Guodong Lu, Shengfeng He", "pdf_url": "https://openreview.net/pdf/25a92931070b5b801961cd0767a855a579110111.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56315, "poster_number": 107, "tag": "SD-5-107 | Exhibit Hall C,D,E ", "relevance_score": 60.62, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper identifies that 3D visual grounding datasets contain significant toxic data (paradoxical and ambiguous annotations) that degrade model training and evaluation. The proposed Refer-Judge framework achieves human-level performance in identifying toxic samples, and training on purified data consistently improves baseline model performance by 1-3% across multiple metrics.", "description": "The paper introduces Refer-Judge, a novel framework that uses Multimodal Large Language Models (MLLMs) to detect and filter toxic annotations in 3D visual grounding datasets. The framework employs a Jury-and-Judge Chain-of-Thought paradigm where multiple jurors assess samples from four perspectives (logic, consistency, distinguishability, ambiguity), followed by magistrate and district judges that refine and consolidate judgments through a Corroborative Refinement mechanism.", "key_contribution": "The main contribution is the Jury-and-Judge Chain-of-Thought framework with Corroborative Refinement that enables structured, multi-perspective reasoning for scene-level data quality assessment in 3D visual grounding, along with the ScanRefer-Justice benchmark for evaluating toxic data identification methods.", "novelty": "Unlike existing MLLM evaluators that focus on simple text-image pairs, Refer-Judge performs comprehensive scene-level reasoning through distributed multi-perspective analysis. The Corroborative Refinement mechanism goes beyond majority voting by adaptively reorganizing visual and textual evidence to correct uncertain reasoning caused by incomplete observations. This is the first work to systematically address toxic data identification in 3D visual grounding using MLLMs without task-specific fine-tuning.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Visual Grounding and Spatial Reasoning", "Hallucination Detection and Model Robustness"], "score": 60.62, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4182227, "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL", "authors": "Jiarui Yao, Yifan HAO, Hanning Zhang, Hanze Dong, Wei Xiong, Nan Jiang, Tong Zhang", "pdf_url": "https://openreview.net/pdf/c7e078578b86348d9d5cc90c5db280404242f16b.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56151, "poster_number": 1904, "tag": "SD-5-1904 | Exhibit Hall C,D,E ", "relevance_score": 57.4, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper identifies that the main bottleneck in chain-of-thought (CoT) training is inefficient stochastic gradient estimation due to static, uniform sampling strategies. The proposed GVM-RAFT method achieves 2-4√ó speedup in convergence and considerable accuracy improvements over vanilla RAFT by dynamically allocating computational resources based on prompt-specific acceptance rates and gradient norms to minimize gradient variance.", "description": "The paper reformulates CoT reasoning as a latent variable problem under an EM framework and proposes GVM-RAFT, a dynamic sample allocation strategy that adaptively assigns inference budgets across prompts. Unlike prior approaches that use uniform best-of-n sampling, this method monitors prompt acceptance rates and stochastic gradient norms to minimize gradient variance under computational budget constraints, with theoretical convergence guarantees and empirical validation on mathematical reasoning tasks.", "key_contribution": "The main contribution is a theoretically-grounded dynamic sampling strategy (GVM) that minimizes stochastic gradient variance by adaptively allocating computational resources based on prompt difficulty and gradient information, applicable to both RAFT and RL algorithms like GRPO.", "novelty": "Unlike existing RAFT-style approaches that uniformly allocate inference budgets across all prompts, this work introduces prompt-specific dynamic budget allocation based on acceptance rates and Lipschitz coefficients. It addresses the inefficiency of static sampling strategies that ignore differences in prompt difficulty and convergence behavior, providing both theoretical analysis (convergence guarantees under smoothness/convexity) and a practical implementation that generalizes beyond RAFT to other RL algorithms.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Prompt Learning and Optimization"], "score": 57.4, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4202146, "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly", "authors": "Zhaowei Wang, Wenhao Yu, Xiyu REN, Jipeng Zhang, Yu Zhao, Rohit Saxena, Liang Cheng, Ginny Wong, Simon See, ..., Mark Steedman", "pdf_url": "https://arxiv.org/pdf/2505.10610", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56816, "poster_number": 4507, "tag": "SD-5-4507 | Exhibit Hall C,D,E ", "relevance_score": 58.32, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "MMLongBench is the first comprehensive benchmark for evaluating long-context vision-language models (LCVLMs) across diverse tasks with standardized input lengths (8K-128K tokens). The evaluation of 46 models reveals that performance on single tasks poorly predicts overall long-context capability, both closed-source and open-source models struggle significantly, and models with stronger reasoning abilities demonstrate better long-context performance.", "description": "This paper introduces MMLongBench, a benchmark containing 13,331 examples across five task categories (Visual RAG, NIAH, Many-Shot ICL, Summarization, and Long-Document VQA) for evaluating LCVLMs. The benchmark uses unified cross-modal token counting and provides each example at five standardized input lengths. Through extensive evaluation of 46 models, the authors provide comprehensive analysis of current LCVLMs' long-context capabilities and identify key bottlenecks in OCR and cross-modal retrieval.", "key_contribution": "The main contribution is the creation of the first comprehensive benchmark for long-context vision-language understanding that covers diverse downstream tasks, uses unified token counting across modalities, provides standardized input lengths for controlled evaluation, and includes both natural and synthetic images. This enables systematic evaluation and comparison of LCVLMs' long-context abilities across multiple dimensions.", "novelty": "Unlike existing benchmarks that focus on single tasks (NIAH or DocVQA only), MMLongBench provides comprehensive coverage of five task categories with diverse image types. It introduces a unified cross-modal token counting scheme that combines vision patches and text tokens, and standardizes all examples at five input lengths (8K-128K tokens) for rigorous length-controlled evaluation. The work demonstrates that NIAH tasks are insufficient proxies for overall long-context capability and identifies long-document VQA as a more reliable indicator.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Video Understanding and Temporal Reasoning", "Vision-Language Reasoning and Chain-of-Thought"], "score": 58.32, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4292024, "title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding", "authors": "Jingli Lin, Chenming Zhu, Runsen Xu, Xiaohan Mao, Xihui Liu, Tai WANG, Jiangmiao Pang", "pdf_url": "https://arxiv.org/pdf/2507.07984", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56168, "poster_number": 4601, "tag": "SD-5-4601 | Exhibit Hall C,D,E ", "relevance_score": 61.89, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "OST-Bench reveals that current state-of-the-art MLLMs struggle significantly with online spatio-temporal reasoning, with even the best models lagging behind human performance by over 30%. The benchmark identifies two critical weaknesses: complex clue-based spatial reasoning and long-term memory retrieval, with models exhibiting a 'Spatio-temporal Reasoning Shortcut' phenomenon where they avoid retrieving key information and rely on shallow inferences. Performance degrades substantially as exploration horizons extend and memory grows, particularly for tasks requiring multi-step spatial connections.", "description": "This paper introduces OST-Bench, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) in Online Spatio-Temporal scene understanding from an embodied agent's perspective. The benchmark consists of 1.4k real-world indoor scenes from ScanNet, Matterport3D, and ARKitScenes with 10k question-answer pairs across three main categories: Agent State, Agent Visible Info, and Agent-Object Spatial Relationships. Unlike existing offline benchmarks, OST-Bench requires models to process incrementally acquired observations and integrate current visual inputs with historical memory for dynamic spatial reasoning.", "key_contribution": "The main contribution is the first benchmark specifically designed to evaluate online spatio-temporal understanding in MLLMs from an agent-centric perspective, requiring models to reason over incrementally received observations rather than fixed pre-recorded inputs. The benchmark provides comprehensive evaluation across 15 fine-grained question subtypes and identifies critical failure modes in current models through extensive analysis.", "novelty": "Unlike existing spatial reasoning benchmarks that operate in offline settings with fixed-length videos or pre-recorded scenes, OST-Bench uniquely emphasizes online processing where models must continuously update understanding as new observations arrive. The benchmark addresses the gap between static evaluation and real-world embodied tasks by requiring dynamic spatial reasoning that integrates multi-view 2D observations into coherent 3D representations over time. It introduces novel task formulations that explicitly test both temporal memory retrieval and multi-step spatial connection abilities, revealing that current models take reasoning shortcuts rather than performing genuine spatio-temporal integration.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Video Understanding and Temporal Reasoning", "Visual Grounding and Spatial Reasoning"], "score": 61.89, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4145510, "title": "Seeking and Updating with Live Visual Knowledge", "authors": "Mingyang Fu, Yuyang Peng, Dongping Chen, Zetong Zhou, Benlin Liu, Yao Wan, Zhou Zhao, Philip S Yu, Ranjay Krishna", "pdf_url": "https://arxiv.org/pdf/2504.05288", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56468, "poster_number": 4600, "tag": "SD-5-4600 | Exhibit Hall C,D,E ", "relevance_score": 57.87, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces LIVEVQA, the first large-scale dataset with 107,143 questions and 28,488 images from April 2024-May 2025, specifically designed to evaluate MLLMs on up-to-date visual knowledge. Experiments reveal that current state-of-the-art MLLMs struggle significantly with recent visual content beyond their training cutoff, but multimodal search tools provide a 327% average improvement. The study also demonstrates that visual knowledge can be efficiently updated through parameter-efficient fine-tuning methods, though this comes at the cost of visual perception capabilities.", "description": "The paper addresses the challenge that Multimodal Large Language Models quickly become outdated as their knowledge is limited to training data cutoff dates. LIVEVQA is constructed through an automated LLM/MLLM-in-the-loop pipeline that collects fresh visual content from news articles, YouTube videos, and academic papers, generating two levels of questions: basic visual entity recognition (Level 1) and complex multi-hop cross-modal reasoning (Level 2). The work investigates both how well current MLLMs can seek out live visual knowledge using external tools and whether they can be efficiently updated with new visual information through fine-tuning.", "key_contribution": "The main contribution is LIVEVQA, the first synthetic dataset and benchmark specifically designed for evaluating and improving MLLMs' ability to seek and update live visual knowledge, along with comprehensive analysis of 17 state-of-the-art models and exploration of parameter-efficient updating methods that balance new knowledge acquisition with retention of existing capabilities.", "novelty": "Unlike traditional VQA datasets that rely on fixed knowledge boundaries, LIVEVQA is the first to focus specifically on fresh, non-contaminated visual content post-dating model training cutoffs, with an automated collection engine for continuous updates. The work uniquely addresses both the seeking dimension (through multimodal search augmentation) and the updating dimension (through PEFT methods like LoRA/DoRA), revealing critical trade-offs between knowledge updating and visual perception preservation. The dataset's multi-source approach (news, videos, academic papers) and two-level question design (entity recognition vs. multi-hop reasoning) provides a more comprehensive evaluation framework than existing benchmarks.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought", "Hallucination Detection and Model Robustness"], "score": 57.87, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4442628, "title": "Robust Cross-modal Alignment Learning for Cross-Scene Spatial Reasoning and Grounding", "authors": "Yanglin Feng, Hongyuan Zhu, Dezhong Peng, Xi Peng, Xiaomin Song, Peng Hu", "pdf_url": "https://openreview.net/pdf/17c45f7182d9da6b92fcf8467d83199936ca20aa.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56375, "poster_number": 4812, "tag": "SD-5-4812 | Exhibit Hall C,D,E ", "relevance_score": 55.92, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces Cross-Scene Spatial Reasoning and Grounding (CSSRG), a novel task that extends 3D visual grounding to locate objects across hundreds of scenes rather than within predetermined scenes. The proposed CoRe framework achieves superior performance while reducing computational overhead by 250x compared to existing 3DVG methods. A new CrossScene-RETR benchmark dataset with rich, discriminative descriptions significantly improves cross-modal spatial alignment performance.", "description": "The paper addresses the limitation of existing 3D visual grounding methods that assume known scene-text correspondence by proposing CSSRG, which requires finding described objects anywhere across large scene collections. The authors develop CoRe, a two-stage matching-then-grounding framework with a Robust Text-Scene Aligning module (RTSA) for efficient scene retrieval and a Tailored Word-Object Associating module (TWOA) for precise object localization. They also introduce the CrossScene-RETR dataset with comprehensive object descriptions to better evaluate this challenging task.", "key_contribution": "The main contribution is the formulation of the CSSRG task and the CoRe framework that efficiently handles cross-scene grounding through robust text-scene matching and fine-grained word-object association. The framework addresses partial alignment challenges between texts and scenes while dramatically reducing computational costs through a matching-then-grounding pipeline.", "novelty": "Unlike existing 3DVG methods that assume predetermined scene-text correspondence, this work tackles the more general problem of finding objects across arbitrary scene collections without known correspondence. It addresses the partial alignment problem where object descriptions focus on targets rather than entire scenes, introducing a complementary learning paradigm for robust matching and a novel Screening Attention mechanism for non-redundant word-object association. The CrossScene-RETR dataset provides significantly richer descriptions (77.7 avg length vs 17.9 in ScanRefer) with better cross-scene discrimination.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought"], "score": 55.92, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4220152, "title": "Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs", "authors": "Xudong Li, Mengdan Zhang, Peixian Chen, Xiawu Zheng, Yan Zhang, Jingyuan Zheng, Yunhang Shen, Ke Li, Chaoyou Fu, ..., Rongrong Ji", "pdf_url": "https://openreview.net/pdf/bec6a67c764747509298fcd29045d23332b42142.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56637, "poster_number": 4701, "tag": "SD-5-4701 | Exhibit Hall C,D,E ", "relevance_score": 56.48, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "The paper identifies three fundamental hallucination types in multi-image MLLMs (context omission, conflation, and detail misinterpretation) and demonstrates that hierarchical preference optimization from context to fine-grained details significantly reduces these errors. CcDPO achieves +4.6 average improvement on multi-image benchmarks and +12.8 points on ultra-long context tasks (MM-NIAH) while maintaining single-image performance. The work introduces MultiScope-42k, a scalable, automatically-generated dataset with hierarchical preference pairs that enables low-cost training without manual annotation.", "description": "This paper proposes Context-to-Cue Direct Preference Optimization (CcDPO), a two-level preference optimization framework for improving multi-image understanding in MLLMs. The approach consists of context-level optimization using structured multi-image captioning with perturbations (truncation, swapping) to address global coherence, and needle-level optimization using visual prompts and contrastive learning to enhance fine-grained detail perception. The method is supported by MultiScope-42k, an automatically constructed dataset with 42k hierarchical preference pairs.", "key_contribution": "The main contribution is a hierarchical preference optimization framework that systematically addresses multi-image hallucinations through two sequential stages: context-level DPO for global sequence understanding and needle-level DPO for local visual detail grounding, along with an automated pipeline for generating large-scale multi-level preference data.", "novelty": "Unlike prior work like MIA-DPO that relies on predefined image references and text-only preferences, CcDPO explicitly models both holistic context and fine-grained visual cues through hierarchical optimization. The approach introduces novel perturbation strategies (sequence truncation, content swapping) for context-level learning and integrates region-specific visual prompts with vision contrastive preference optimization for needle-level learning. This hierarchical design mirrors human reasoning patterns (global context first, then local details) and addresses limitations in existing methods that neglect comprehensive cross-image integration.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Hallucination Detection and Model Robustness"], "score": 56.48, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4215098, "title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains", "authors": "Chun Wang, Xiaojun Ye, Xiaoran Pan, Zihao Pan, Haofan Wang, Yiren Song", "pdf_url": "https://openreview.net/pdf/37967ca9eb618eb49ac5f64b75660d4ad6a47109.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56755, "poster_number": 4606, "tag": "SD-5-4606 | Exhibit Hall C,D,E ", "relevance_score": 54.99, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "The paper introduces GRE Suite, achieving state-of-the-art performance on geo-localization benchmarks (Im2GPS3k and GWS15k) by using only 5% of training data compared to previous methods. The approach demonstrates that structured reasoning chains with multi-stage inference significantly outperform traditional alignment-based methods, with improvements of +4.2% to +4.2% across various distance thresholds. The work establishes that reinforcement learning can effectively enhance visual reasoning capabilities in multimodal models for complex geographic inference tasks.", "description": "This paper presents a comprehensive framework for worldwide image geo-localization that augments Vision-Language Models with structured reasoning chains. The approach consists of three components: GRE30K (a high-quality reasoning dataset), the GRE model (using multi-stage reasoning with cold-start supervised fine-tuning and two-stage GRPO-based reinforcement learning), and GREval-Bench (a comprehensive evaluation benchmark assessing both reasoning quality and localization accuracy across multiple granularities).", "key_contribution": "The main contribution is a novel reasoning-augmented framework that combines supervised fine-tuning with two-stage reinforcement learning (GRPO) to enable VLMs to extract and reason over multi-granular geographic indicators (both explicit landmarks and implicit cues like architectural styles) for accurate, explainable worldwide geo-localization without relying on candidate databases or fixed grid classifications.", "novelty": "Unlike existing alignment-based approaches that rely on large-scale image-GPS pair databases or grid-based classification, this work introduces a reasoning-first paradigm that progressively infers location through chain-of-thought analysis of visual geographic indicators. The paper addresses limitations of current methods by enabling open-ended coordinate prediction without candidate databases and introduces the first comprehensive evaluation of reasoning quality (not just final accuracy) for geo-localization tasks. The two-stage GRPO training strategy with rule-based rewards represents a novel application of RL to enhance visual reasoning in multimodal models.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Visual Grounding and Spatial Reasoning"], "score": 54.99, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4210549, "title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains", "authors": "Wenhui Tan, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Ruihua Song, Jian Luan", "pdf_url": "https://openreview.net/pdf/9a1c1a85e08c73fd0ae736b069afe387fbe0fb59.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 57025, "poster_number": 212, "tag": "SD-5-212 | Exhibit Hall C,D,E ", "relevance_score": 52.02, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "CoLaR achieves 14.1% higher accuracy than latent-based baseline methods at comparable compression ratios, while reducing reasoning chain length by 53.3% with only 4.8% performance degradation compared to explicit Chain-of-Thought methods. When enhanced with reinforcement learning on challenging mathematical tasks, CoLaR demonstrates performance gains of up to 5.4% while dramatically reducing latent reasoning chain length by 82.8%, showing that exploration-exploitation capabilities in latent space can significantly improve both efficiency and accuracy.", "description": "This paper introduces Compressed Latent Reasoning (CoLaR), a framework that compresses LLM reasoning processes into dense latent space through dynamic-speed reasoning. The method uses a two-stage training approach: first, supervised fine-tuning with an auxiliary next compressed embedding prediction objective that merges consecutive token embeddings with variable compression factors; second, reinforcement learning (GRPO) that leverages a probabilistic latent head to explore diverse reasoning paths and exploit more compact ones.", "key_contribution": "The main contribution is a novel framework that enables dynamic-speed latent reasoning through auto-regressive prediction of compressed embeddings, combined with a probabilistic latent head that allows reinforcement learning to balance exploration of correct reasoning paths with exploitation of shorter, more efficient solutions.", "novelty": "Unlike existing latent reasoning methods (Coconut, CODI) that use fixed-length deterministic reasoning chains, CoLaR introduces dynamic compression with variable-length chains controlled by a compression factor that can be adjusted at inference time via prompting. The probabilistic latent head enables exploration-exploitation capabilities through reinforcement learning, which previous deterministic approaches could not leverage. The method addresses the limitation of sparse supervision signals by training on compressed reasoning tokens, not just final answers.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Prompt Learning and Optimization"], "score": 52.02, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4058372, "title": "MIR-Bench: Can Your LLM Recognize Complicated Patterns via Many-Shot In-Context Reasoning?", "authors": "Kai Yan, Zhan Ling, Kang Liu, Yifan Yang, Ting-Han Fan, Lingfeng Shen, Zhengyin Du, Jiecao Chen", "pdf_url": "https://arxiv.org/pdf/2502.09933", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56334, "poster_number": 5201, "tag": "SD-5-5201 | Exhibit Hall C,D,E ", "relevance_score": 52.12, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "MIR-Bench reveals that state-of-the-art LLMs struggle with many-shot pattern recognition, with even the best models (o1-mini) achieving less than 70% accuracy. The study finds that LLMs perform better at transductive reasoning (direct prediction) than inductive reasoning (explicit rule extraction via CoT), and that performance saturates around 256-512 shots due to attention dispersion rather than information retrieval limitations. LLMs demonstrate surprising robustness against erroneous examples, maintaining decent performance even with 75% error rates.", "description": "This paper introduces MIR-Bench, the first large-scale benchmark for evaluating many-shot in-context pattern recognition in LLMs, containing 6,930 problems derived from coding functions with diverse input-output formats. The benchmark tests LLMs' ability to recognize underlying patterns from 4 to 2048 examples and predict outputs for new inputs. The authors conduct extensive empirical studies on 21 LLMs examining scaling effects, robustness, inductive vs. transductive reasoning, RAG, coding approaches, and cross-domain generalization.", "key_contribution": "The main contribution is MIR-Bench, a novel benchmark that fills the gap between few-shot pattern recognition tasks and many-shot ICL evaluations by providing a diverse, automatically generated dataset that tests LLMs' ability to aggregate information from hundreds to thousands of examples. The paper also provides comprehensive empirical insights into many-shot reasoning capabilities and limitations of current LLMs.", "novelty": "Unlike existing pattern recognition benchmarks that focus on few-shot settings (<10 examples) or many-shot benchmarks limited to classification tasks, MIR-Bench evaluates general pattern recognition with up to 2048 shots across diverse input-output types. The work introduces an automatic pipeline for generating new tasks from coding benchmarks without data leakage, and reveals that CoT breaks structural coherence in many-shot scenarios, explaining why transductive reasoning outperforms inductive reasoning. It also demonstrates that straightforward solutions like RAG and code-generation paradigms do not solve many-shot saturation issues.", "ai_categories": ["In-Context Learning and Few-Shot Adaptation", "Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought"], "score": 52.12, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4159922, "title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation", "authors": "Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, Michael Qizhe Shieh", "pdf_url": "https://openreview.net/pdf/533c2b23b105c6b1687de467a26c6eb3d0dfc213.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56732, "poster_number": 414, "tag": "SD-5-414 | Exhibit Hall C,D,E ", "relevance_score": 51.75, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "NoisyRollout achieves state-of-the-art performance among open-source RL-tuned vision-language models with minimal training data (2.1K samples), outperforming models trained on 35K+ samples. The method demonstrates that mixing rollouts from clean and moderately distorted images during RL training significantly improves both visual reasoning (53.2% on MathVerse) and perception capabilities (72.1% on HallusionBench). The approach requires no additional training cost and generalizes across model sizes (7B to 32B) and data scales.", "description": "This paper introduces NoisyRollout, a data augmentation technique for reinforcement learning training of vision-language models that enhances visual reasoning by mixing trajectories from both clean and distorted images. The method applies image distortions (e.g., Gaussian noise) during rollout generation while conditioning policy updates only on clean inputs, combined with a noise annealing schedule that gradually reduces distortion strength during training. This approach promotes better policy exploration through vision-oriented inductive biases without modifying the underlying RL objective.", "key_contribution": "The main contribution is a simple yet effective data augmentation method that improves RL-based visual reasoning in VLMs by introducing perceptual diversity through noisy rollouts, enabling more robust reasoning strategies and implicit contrastive signals that refine visual perception during reasoning, all without additional computational cost.", "novelty": "Unlike prior work that directly adapts LLM RL methods to VLMs without addressing perceptual challenges, NoisyRollout specifically targets the unique visual perception limitations of VLMs through vision-oriented data augmentation. The method differs from traditional temperature-based diversity approaches by introducing more targeted and effective exploration through perceptual variance rather than superficial decoding variability. The hybrid rollout strategy with noise annealing provides a principled way to balance exploration and exploitation specifically for multimodal reasoning tasks.", "ai_categories": ["Reinforcement Learning for Multimodal Models", "Vision-Language Reasoning and Chain-of-Thought", "Hallucination Detection and Model Robustness"], "score": 51.75, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4391333, "title": "C$^2$Prompt: Class-aware Client Knowledge Interaction for Federated Continual Learning", "authors": "Kunlun Xu, yibo feng, Jiangmeng Li, Yongsheng Qi, Jiahuan Zhou", "pdf_url": "https://openreview.net/pdf/f8b45261958643144e6e7e3f1a61d4363528d66e.pdf", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56284, "poster_number": 4903, "tag": "SD-5-4903 | Exhibit Hall C,D,E ", "relevance_score": 50.37, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "C2Prompt addresses the critical challenge of class-wise knowledge coherence in federated continual learning by introducing mechanisms that reduce intra-class distribution gaps across clients and mitigate inter-class knowledge confusion during prompt aggregation. The method achieves state-of-the-art performance with 2.51% and 2.90% improvements over existing methods on ImageNet-R and DomainNet benchmarks, while maintaining minimal communication overhead. The approach effectively handles both temporal forgetting (across sequential tasks) and spatial forgetting (across heterogeneous clients) simultaneously through class-aware knowledge interaction.", "description": "This paper proposes C2Prompt, a novel federated continual learning method that enhances class-wise knowledge coherence during prompt communication across distributed clients. The approach introduces two key mechanisms: Local Class Distribution Compensation (LCDC) to align local class distributions with estimated global distributions, and Class-aware Prompt Aggregation (CPA) to selectively strengthen class-relevant knowledge aggregation while reducing conflicts. The method operates on frozen pre-trained vision transformers using learnable prompts, avoiding the need for data synthesis or full model fine-tuning.", "key_contribution": "The main contribution is a class-aware client knowledge interaction framework that explicitly addresses knowledge coherence issues in prompt-based federated continual learning through global distribution estimation, local distribution compensation, and class-aware prompt aggregation, achieving superior performance while maintaining privacy and efficiency.", "novelty": "Unlike existing prompt-based FCL methods that suffer from knowledge conflicts during prompt aggregation, C2Prompt explicitly models and addresses class-wise knowledge coherence across two dimensions: reducing intra-class distribution disparities across clients and mitigating inter-class confusion through selective aggregation. The method introduces a theoretically grounded distribution compensation mechanism that transfers local semantics to the global domain and a class-affinity-based aggregation scheme that dynamically weights prompts based on their class relevance. This addresses the fundamental limitation of prior work where prompts from different clients with divergent class-wise knowledge are naively aggregated, leading to degraded performance and increased forgetting.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation"], "score": 50.37, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4213573, "title": "Knot So Simple: A Minimalistic Environment for Spatial Reasoning", "authors": "Zizhao Chen, Yoav Artzi", "pdf_url": "https://arxiv.org/pdf/2505.18028", "session_id": 553, "session_name": "San Diego Poster Session 5", "poster_id": 56854, "poster_number": 4505, "tag": "SD-5-4505 | Exhibit Hall C,D,E ", "relevance_score": 51.29, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "KNOTGYM introduces a minimalistic yet challenging environment for visual spatial reasoning through knot manipulation, revealing that state-of-the-art RL methods and VLMs struggle significantly with complex spatial reasoning tasks. The environment demonstrates a clear generalization ladder based on the number of knot crossings, where methods trained on simpler knots (#X=2) fail to generalize to more complex configurations (#X=3,4). DreamerV3 achieves >90% success on unknot tasks but all methods perform near-random on tie and convert tasks with #X‚â•3.", "description": "The paper presents KNOTGYM, an interactive environment for training and evaluating visual reasoning agents on goal-oriented rope manipulation tasks. Agents observe knot configurations as images and must apply forces to transform knots to match target topological structures (Gauss codes) across three tasks: unknot (untangle to simple loop), tie (create complex knot from loop), and convert (transform one knot to another). The environment provides a mathematically grounded complexity measure through the number of crossings, enabling systematic evaluation of generalization capabilities.", "key_contribution": "The main contribution is a novel benchmark environment that combines acute visual perception, continuous spatial reasoning, and grounded manipulation in a mathematically principled framework with a clear generalization ladder. KNOTGYM provides an accessible yet challenging testbed for evaluating visual reasoning agents, with sparse topological rewards rather than coordinate-based dense rewards, requiring abstract reasoning over equivalence classes of states.", "novelty": "Unlike existing deformable object manipulation benchmarks (e.g., SoftGym) that use dense coordinate-based rewards, KNOTGYM requires abstract topological reasoning with sparse rewards based on Gauss code equivalence. The environment introduces a mathematically grounded complexity axis (number of crossings) that creates a natural generalization ladder, distinguishing it from prior spatial reasoning benchmarks that focus on static scenes or discrete spaces. It uniquely combines the challenges of acute perception (pixel-level crossing detection), continuous action spaces, and abstract goal specification in a single unified framework.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Multimodal Benchmarks and Evaluation", "Reinforcement Learning for Multimodal Models"], "score": 51.29, "session_type": "San Diego Poster Session 5"}, {"paper_id": 4307546, "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning", "authors": "Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, Fu-En Yang", "pdf_url": "https://openreview.net/pdf/b35b0fc70612e191baced400f754db8ff1fae711.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57774, "poster_number": 2203, "tag": "SD-6-2203 | Exhibit Hall C,D,E ", "relevance_score": 92.93, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "ThinkAct demonstrates that reinforcement learning with action-aligned visual rewards (goal completion and trajectory alignment) enables multimodal LLMs to perform effective long-horizon planning for robot manipulation. The framework achieves state-of-the-art results on LIBERO (84.4%) and SimplerEnv benchmarks while enabling emergent capabilities like few-shot adaptation and self-correction. The dual-system architecture allows asynchronous operation where reasoning happens at slower intervals while action execution operates at higher frequency.", "description": "ThinkAct is a vision-language-action reasoning framework that bridges high-level embodied reasoning with low-level action execution through reinforced visual latent planning. The system trains a multimodal LLM to generate reasoning plans using GRPO optimization guided by action-aligned visual rewards, then compresses these plans into visual latents that condition a downstream diffusion-based action model for execution in target environments.", "key_contribution": "The main contribution is a novel dual-system VLA framework that uses action-aligned visual feedback (goal completion and trajectory alignment rewards) to incentivize embodied reasoning in MLLMs via reinforcement learning, connecting high-level planning to low-level control through visual latent planning that enables few-shot adaptation and self-correction in robot manipulation tasks.", "novelty": "Unlike existing VLA models that predict actions end-to-end or rely on supervised chain-of-thought annotations, ThinkAct uses reinforcement learning with action-aligned visual rewards derived from trajectory matching and goal completion rather than QA-style rewards. This approach grounds reasoning in physical execution without requiring expensive step-by-step CoT supervision, and the visual latent planning mechanism enables asynchronous slow thinking and fast control while supporting emergent self-correction behaviors.", "ai_categories": ["Embodied AI and Vision-Language-Action Models", "Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models"], "score": 92.93, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4210192, "title": "VLM-R¬≥: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought", "authors": "Chaoya Jiang, Yongrui Heng, Wei Ye, Haiyang Xu, Ming Yan, Ji Zhang, Fei Huang, Shikun Zhang", "pdf_url": "https://openreview.net/pdf/5e7d2aa638471c63b356d40e8943dbfd513adca1.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57440, "poster_number": 4504, "tag": "SD-6-4504 | Exhibit Hall C,D,E ", "relevance_score": 97.22, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "VLM-R¬≥ introduces a novel framework that enables multimodal large language models to dynamically interact with visual regions during chain-of-thought reasoning, achieving state-of-the-art results on benchmarks like MathVista (70.4%), ScienceQA (87.9%), and MathVision (30.2%). The paper demonstrates that interleaving visual region grounding with textual reasoning significantly outperforms traditional text-only reasoning approaches, with particularly strong gains on tasks requiring fine-grained spatial reasoning and visual cue extraction. The Region-Conditioned Reinforcement Policy Optimization (R-GRPO) training paradigm successfully teaches models when and where to look within images during multi-step reasoning.", "description": "This paper presents VLM-R¬≥, a framework that equips multimodal large language models with the ability to perform dynamic, iterative visual grounding during chain-of-thought reasoning. The approach combines a curated Visuo-Lingual Interleaved Rationale (VLIR) dataset for supervised fine-tuning with a novel Region-Conditioned Reinforcement Policy Optimization (R-GRPO) training strategy. The model learns to decide when additional visual evidence is needed, determine where to ground within images through bounding box selection and transformations (crop, zoom), and seamlessly integrate the resulting visual context into interleaved reasoning chains.", "key_contribution": "The main contributions are: (1) the VLIR dataset, the first benchmark for training and evaluating MLLMs on interleaved visual-textual chain-of-thought reasoning with explicit region-level interactions, and (2) the VLM-R¬≥ framework with R-GRPO training strategy that enables dynamic visual region localization and evidence integration during the reasoning process, allowing models to actively revisit and focus on specific visual regions throughout multi-step inference.", "novelty": "Unlike existing MLLMs that perform reasoning predominantly in the textual domain with only initial static visual grounding, VLM-R¬≥ enables dynamic, iterative interaction with specific visual regions throughout the reasoning chain. The work addresses the limitation that current reasoning-based MLLMs struggle with complex tasks requiring sequential verification of hypotheses against image details or tracking object states across visual cues. The R-GRPO training paradigm is novel in that it provides credit assignment for region selection decisions, rewarding the model for selecting informative regions and integrating them appropriately into reasoning steps, going beyond simple final-answer supervision.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Visual Grounding and Spatial Reasoning", "Reinforcement Learning for Multimodal Models"], "score": 97.22, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4101176, "title": "MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?", "authors": "Zhe Xu, Daoyuan Chen, Zhenqing Ling, Yaliang Li, Ying Shen", "pdf_url": "https://arxiv.org/pdf/2503.09499", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57826, "poster_number": 113, "tag": "SD-6-113 | Exhibit Hall C,D,E ", "relevance_score": 92.44, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "MindGYM demonstrates that cognitively-guided synthetic data achieves 16.7% higher quality and 67.91% lower variance compared to baselines, with only 400 samples yielding up to 16% performance gains on reasoning benchmarks. The framework reveals that both high mean quality and low variance are critical for effective thinking-oriented fine-tuning. Remarkably, text-only synthetic data successfully transfers reasoning capabilities to multimodal tasks, improving performance across both language and vision domains.", "description": "MindGYM is a thinking-centric data synthesis framework for enhancing reasoning capabilities in large foundation models through self-generated, cognitively guided training data. The framework operates through three stages: injecting structured cognitive thinking processes, synthesizing diverse single-hop seed questions across eight meta-topics, and composing challenging multi-hop questions that require deeper reasoning. The approach enables models to self-evolve through internally generated data without relying on rigid templates or extensive human annotation.", "key_contribution": "The main contribution is a scalable, self-synthesis framework that embeds structured cognitive priors (breadth, depth, progression) into question generation, enabling models to create high-quality, reasoning-intensive training data from their own knowledge. This thinking-centric paradigm achieves superior performance with minimal data (400 samples) while generalizing across different model architectures and sizes.", "novelty": "Unlike prior approaches that rely on rigid templates, crowd-sourced annotations, or computationally expensive RL methods, MindGYM introduces cognitive thinking injection inspired by human reasoning processes to guide data synthesis. The framework addresses limitations of existing methods by reducing quality variance alongside improving mean quality, and demonstrates successful cross-modal transfer where text-only synthetic data enhances multimodal reasoning. The progressive training pathway (guided answering ‚Üí reason reconstruction ‚Üí paired reasoning ‚Üí autonomous solving) mirrors human cognitive development and proves more effective than uniform supervision.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation"], "score": 92.44, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4244714, "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning", "authors": "Xiangning Yu, Zhuohan Wang, Linyi Yang, Haoxuan Li, Anjie Liu, Xiao Xue, Jun Wang, Mengyue Yang", "pdf_url": "https://openreview.net/pdf/722d843c0b72fca7838e93dad027bbb061500956.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57531, "poster_number": 2512, "tag": "SD-6-2512 | Exhibit Hall C,D,E ", "relevance_score": 84.07, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces a causal framework using Probability of Necessity and Sufficiency (PNS) to optimize Chain-of-Thought reasoning by identifying and retaining only causally essential steps. Experiments show substantial improvements in reasoning efficiency (50-80% reduction in tokens/steps) while maintaining or improving accuracy across mathematical and commonsense reasoning benchmarks. The optimized CoT traces enable both effective in-context learning for non-reasoning models and supervised fine-tuning for reasoning-capable models.", "description": "The paper addresses two fundamental challenges in Chain-of-Thought reasoning: sufficiency (ensuring steps comprehensively support conclusions) and necessity (identifying indispensable steps). It proposes a causal framework based on PNS that uses counterfactual interventions to evaluate each reasoning step's causal contribution, enabling automated pruning of redundant steps and addition of missing ones. The resulting compact CoT traces are used as demonstrations for in-context learning and fine-tuning.", "key_contribution": "The main contribution is a novel bi-level optimization framework that integrates causal Probability of Necessity and Sufficiency into CoT reasoning, providing both theoretical formalization and a practical algorithm for constructing minimal yet causally sufficient reasoning chains through counterfactual rollouts and interventions.", "novelty": "Unlike existing methods that rely on correlation-based metrics (attention weights, likelihood scores) or heuristic compression, this work applies rigorous causal analysis at the step level to evaluate logical contributions. It extends Pearl's PNS framework specifically to CoT reasoning, using counterfactual interventions to distinguish genuinely necessary steps from redundant ones, addressing the 'overthinking' problem while ensuring logical soundness. Previous work focused on either sufficiency or necessity separately, but not both simultaneously through a causal lens.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation", "Compositional and Counterfactual Reasoning"], "score": 84.07, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4442837, "title": "GoT: Unleashing Reasoning Capability of MLLM for Visual Generation and Editing", "authors": "Rongyao Fang, Chengqi Duan, Kun Wang, Linjiang Huang, Hao Li, Hao Tian, Shilin Yan, Weihao Yu, Xingyu Zeng, ..., Hongsheng Li", "pdf_url": "https://openreview.net/pdf/88332ed6ca1dea81759b8d07d12b66ab6da63abf.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57864, "poster_number": 4101, "tag": "SD-6-4101 | Exhibit Hall C,D,E ", "relevance_score": 86.36, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "GoT introduces explicit semantic-spatial reasoning chains with precise coordinates before image generation, achieving state-of-the-art performance on GenEval (0.64 overall score) and image editing benchmarks. The framework enables interactive generation where users can modify reasoning steps to control outputs, and demonstrates that integrating MLLM reasoning capabilities significantly improves compositional generation and editing accuracy. The work includes the first large-scale dataset (9M+ samples) with detailed reasoning chains for visual generation tasks.", "description": "This paper presents Generation Chain-of-Thought (GoT), a novel paradigm that uses Multimodal Large Language Models to generate explicit reasoning chains containing semantic relationships and spatial coordinates before synthesizing images. The framework unifies text-to-image generation and image editing through a Semantic-Spatial Guidance Module that integrates MLLM reasoning (Qwen2.5-VL) with diffusion models (SDXL-based). The approach transforms visual generation from direct prompt-to-image mapping into a reasoning-guided process with interpretable intermediate steps.", "key_contribution": "The main contribution is the GoT paradigm that bridges MLLM reasoning capabilities with visual generation through explicit semantic-spatial reasoning chains, supported by a unified end-to-end framework with a novel Semantic-Spatial Guidance Module and large-scale datasets containing over 9M annotated samples with detailed reasoning chains.", "novelty": "Unlike prior work that treats layout planning and generation as separate stages or merely combines LLMs with diffusion models without true integration, GoT generates explicit reasoning chains with both semantic descriptions and precise spatial coordinates that directly guide the synthesis process end-to-end. The approach addresses limitations of current methods that lack reasoning capabilities by enabling step-by-step decomposition of complex instructions with interpretable intermediate representations. GoT also enables interactive generation through direct modification of reasoning chains, providing unprecedented user control compared to black-box generation systems.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Visual Grounding and Spatial Reasoning", "Prompt Learning and Optimization"], "score": 86.36, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4287517, "title": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning", "authors": "Yana Wei, Liang Zhao, Jianjian Sun, Kangheng Lin, jisheng yin, Jingcheng Hu, Yinmin Zhang, En Yu, Haoran Lv, ..., Vishal M. Patel", "pdf_url": "https://openreview.net/pdf/aa64c75f269da233428bef98108f402866b88aff.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57475, "poster_number": 1907, "tag": "SD-6-1907 | Exhibit Hall C,D,E ", "relevance_score": 86.23, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that linguistic cognitive behaviors (like backtracking, verification, subgoal setting) can be effectively transferred to multimodal vision-language models through a two-stage training approach. The work reveals three fundamental insights: (1) behavior transfer emerges surprisingly early during cold start due to linguistic mental imagery, (2) cold start broadly memorizes visual behaviors while RL critically discerns and scales effective patterns, and (3) transfer strategically favors high-utility behaviors like visual reflection. The resulting Open-Vision-Reasoner (OVR) achieves state-of-the-art performance on reasoning benchmarks, including 95.3% on MATH500 and 51.8% on MathVision.", "description": "The paper introduces Open-Vision-Reasoner (OVR), a multimodal large language model built on Qwen2.5-VL-7B using a two-stage training paradigm: massive linguistic cold-start fine-tuning on 2M+ examples followed by nearly 1,000 steps of multimodal reinforcement learning on 300k+ mixed-modality examples. The work systematically investigates how cognitive behaviors like visual reflection, divide-and-conquer, visual verification, and goal-driven visual tracing emerge and transfer from language to vision domains through this training process.", "key_contribution": "The main contribution is establishing the largest open-source RL practice on Qwen2.5-VL-7B that demonstrates effective transfer of linguistic cognitive behaviors to visual reasoning tasks, achieving superior performance across both linguistic and multimodal benchmarks. The work provides in-depth analysis of visual cognitive behavior emergence and evolution, revealing strategic patterns in how behaviors transfer from language to vision modalities.", "novelty": "Unlike previous MLLM approaches that rely on RLHF with learned reward models or complex pipeline-based behavior synthesis, this work pioneers the use of large-scale RLVR (Reinforcement Learning from Verifiable Reward) with simple rule-based rewards for multimodal reasoning. The paper uniquely analyzes the transfer mechanisms of cognitive behaviors from language to vision, revealing that mental imagery in linguistic reasoning enables early emergence of visual behaviors. It addresses the limitation of understanding how and why reasoning behaviors emerge in MLLMs by providing systematic analysis of behavior transfer rates and evolution dynamics across training stages.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Multimodal Benchmarks and Evaluation"], "score": 86.23, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4222169, "title": "Grounded Reinforcement Learning for Visual Reasoning", "authors": "Gabriel Herbert Sarch, Snigdha Saha, Naitik Khandelwal, Ayush Jain, Michael J. Tarr, Aviral Kumar, Katerina Fragkiadaki", "pdf_url": "https://openreview.net/pdf/51495ccee37191c401ab184d34f4e5dc92f3ef58.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57834, "poster_number": 4417, "tag": "SD-6-4417 | Exhibit Hall C,D,E ", "relevance_score": 87.51, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "ViGoRL demonstrates that explicitly grounding each reasoning step in spatial coordinates (x,y locations) significantly improves visual reasoning performance, achieving 12.9 point improvement on SAT-2 and 86.4% on V*Bench. The approach amplifies crucial visual reasoning behaviors including region exploration (3.5 vs 1.44 regions), grounded subgoal setting (1.1 vs 0.07), and visual backtracking (0.47 vs 0.00) compared to standard approaches. Human evaluations confirm that 72.8% of predicted coordinates accurately refer to intended regions and meaningfully improve interpretability.", "description": "This paper introduces ViGoRL (Visually Grounded Reinforcement Learning), a vision-language model that learns to anchor each reasoning step to specific image coordinates through a combination of MCTS-based warm-start supervision and reinforcement learning. The approach uses Monte Carlo Tree Search to generate diverse grounded reasoning trajectories, followed by supervised fine-tuning and Group Relative Policy Optimization (GRPO) to reinforce correct grounded reasoning chains. A multi-turn variant enables dynamic zooming into predicted regions for fine-grained visual inspection.", "key_contribution": "The main contribution is demonstrating that explicit spatial grounding of reasoning steps, trained via MCTS-generated trajectories and RL, serves as a critical cognitive scaffold that amplifies visual reasoning behaviors (exploration, verification, backtracking) and significantly improves performance across diverse visual reasoning benchmarks without requiring external supervision for grounding.", "novelty": "Unlike prior work that applies RL directly to base VLMs or uses prompt-based grounding, ViGoRL explicitly trains models to produce spatially-grounded reasoning chains where each thought is anchored to (x,y) coordinates. The paper reveals that standard RL on VLMs leads to abstract, ungrounded reasoning, and addresses this by using MCTS to bootstrap grounded behaviors before RL training. The multi-turn RL framework with visual feedback (zooming) is novel for enabling iterative visual refinement during reasoning.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Visual Grounding and Spatial Reasoning"], "score": 87.51, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4183496, "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning", "authors": "Yibin Wang, Zhimin Li, Yuhang Zang, Chunyu Wang, Qinglin Lu, Cheng Jin, Jiaqi Wang", "pdf_url": "https://openreview.net/pdf/e4dafb5f56d85a3ea3c98693dd6bf3a57ca6f9c7.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57865, "poster_number": 4604, "tag": "SD-6-4604 | Exhibit Hall C,D,E ", "relevance_score": 77.61, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that incorporating explicit long Chain-of-Thought (CoT) reasoning into multimodal reward models significantly enhances the accuracy and reliability of reward signals across both visual understanding and generation tasks. Remarkably, once the model masters CoT reasoning through reinforcement learning, it also exhibits strong implicit reasoning capabilities, achieving superior performance even without explicit reasoning traces. The proposed three-stage training pipeline (cold start, rejection sampling, and GRPO) successfully activates latent reasoning abilities in VLMs using only limited high-quality CoT data.", "description": "The paper introduces UnifiedReward-Think, the first unified multimodal Chain-of-Thought reward model capable of multi-dimensional, step-by-step reasoning for both visual understanding and generation tasks across images and videos. The approach uses an exploration-driven reinforcement fine-tuning strategy with three stages: (1) cold start using GPT-4o distilled CoT data to learn reasoning format, (2) rejection sampling on large-scale multimodal preference data to generalize CoT reasoning, and (3) Group Relative Policy Optimization (GRPO) to refine reasoning through trial-and-error learning with verifiable rewards.", "key_contribution": "The main contribution is the first unified multimodal CoT-based reward model that performs multi-dimensional long-chain reasoning across diverse vision tasks, trained via a novel three-stage reinforcement learning pipeline that elicits and enhances VLMs' latent reasoning capabilities without requiring extensive CoT supervision data.", "novelty": "Unlike existing reward models that provide direct responses or shallow reasoning, this work introduces explicit long CoT reasoning with multi-dimensional scoring to improve interpretability and accuracy. It addresses the limitation of scarce CoT reward data by using reinforcement learning (GRPO) to activate VLMs' latent reasoning abilities rather than relying solely on supervised fine-tuning. The multidimensional CoT scoring strategy ensures consistency between reasoning processes and final decisions, mitigating the common issue of implausible reasoning despite correct predictions.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Multimodal Benchmarks and Evaluation"], "score": 77.61, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4393192, "title": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception", "authors": "Ziang Yan, Yinan He, Xinhao Li, Zhengrong Yue, Xiangyu Zeng, Yali Wang, Yu Qiao, Limin Wang, Yi Wang", "pdf_url": "https://openreview.net/pdf/7cf4479fc77f583f1cdede042be97389be035f9a.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57798, "poster_number": 5200, "tag": "SD-6-5200 | Exhibit Hall C,D,E ", "relevance_score": 79.52, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "VideoChat-R1.5 introduces Visual Test-Time Scaling (VTTS), achieving over 5% average improvement across 15+ benchmarks by enabling MLLMs to iteratively refine visual perception during inference. The method uses reinforcement learning with spatio-temporal supervision to guide progressive attention to high-confidence regions, mimicking human hierarchical perception. The approach demonstrates that scaling perceptual compute (not just reasoning) significantly enhances multimodal understanding.", "description": "This paper presents VTTS, a novel test-time scaling approach for multimodal large language models that enhances reasoning through iterative visual perception. Unlike existing methods that perceive once and then reason, VTTS employs an Iterative Perception (ITP) mechanism that progressively refines focus on relevant spatio-temporal regions guided by evolving textual predictions. The method is trained using reinforcement learning (GRPO) on the newly introduced VTTS-80K dataset with spatio-temporal annotations.", "key_contribution": "The main contribution is the Visual Test-Time Scaling framework that explicitly builds visual dependencies through iterative perception, enabling MLLMs to dynamically refine their understanding by repeatedly focusing on high-confidence spatio-temporal regions. This is supported by a reinforcement learning training paradigm and the VTTS-80K dataset designed specifically for learning iterative perception capabilities.", "novelty": "Unlike existing test-time scaling methods that only extend reasoning in language space with static visual perception, VTTS introduces dynamic iterative visual perception that refines understanding across multiple steps. The work addresses the limitation of single-pass visual processing by enabling models to progressively narrow attention to relevant regions, similar to human coarse-to-fine perception. It uses reinforcement learning rather than supervised fine-tuning to learn accurate spatio-temporal localization, overcoming the limitations of autoregressive prediction for continuous spatial-temporal coordinates.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Video Understanding and Temporal Reasoning"], "score": 79.52, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4210848, "title": "Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models", "authors": "Jiaqi WANG, Kevin Qinghong Lin, James Cheng, Mike Zheng Shou", "pdf_url": "https://openreview.net/pdf/62bc8f6585ecc381363cf32ea8c833c4ef08d00e.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57281, "poster_number": 400, "tag": "SD-6-400 | Exhibit Hall C,D,E ", "relevance_score": 77.66, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "TON demonstrates that vision-language models can learn to selectively bypass reasoning for simple questions while engaging in detailed reasoning for complex ones, reducing token usage by up to 90% without sacrificing performance. The paper shows that the ability to decide 'when to think' must be explicitly trained through format-following in supervised fine-tuning, as models default to generating full reasoning traces even when prompted to skip unnecessary steps. Across multiple benchmarks (CLEVR, GeoQA, AITZ), TON achieves substantial efficiency gains and in some cases improves accuracy by up to 17% compared to vanilla GRPO.", "description": "This paper introduces TON (Think-or-Not), a two-stage training framework that enables vision-language models to adaptively decide when reasoning is necessary. The approach combines (1) a supervised fine-tuning stage with 'thought dropout' that randomly replaces reasoning traces with empty thoughts, and (2) a GRPO reinforcement learning stage that allows the model to freely explore when to think while maximizing task-aware rewards. The method is evaluated across language, vision-language, and agentic tasks using 3B and 7B models.", "key_contribution": "The main contribution is a simple yet effective 'thought dropout' mechanism during SFT combined with GRPO training that teaches models to selectively engage in reasoning based on task difficulty, mirroring human-like cognitive effort modulation. This is the first work to systematically study 'when to think' rather than 'how to think' in vision-language models.", "novelty": "Unlike existing GRPO approaches that always generate full reasoning traces, TON introduces selective reasoning as a trainable skill through thought dropout during SFT. The paper demonstrates that prompting alone is insufficient‚Äîmodels must be explicitly trained to skip reasoning through format-following. This addresses the inefficiency of current RL methods that produce unnecessarily long and redundant reasoning processes, offering a more natural and scalable solution without requiring external control mechanisms or rule-based reward penalties.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Prompt Learning and Optimization"], "score": 77.66, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4255952, "title": "PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning", "authors": "Yizhen Zhang, Yang Ding, Shuoshuo Zhang, Xinchen Zhang, Haoling Li, Zhong-Zhi Li, Peijie Wang, Jie Wu, Lei Ji, ..., Yujiu Yang", "pdf_url": "https://openreview.net/pdf/2a9e8b0109094071a532fedeb9aee41d18c08710.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57200, "poster_number": 301, "tag": "SD-6-301 | Exhibit Hall C,D,E ", "relevance_score": 77.51, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "PeRL achieves state-of-the-art performance on multi-image benchmarks (76.39 on Mantis-Eval, 58.53 on BLINK) while maintaining competitive single-image performance, using substantially less training data than supervised fine-tuning methods. The approach demonstrates that permuting image sequences during reinforcement learning effectively mitigates positional bias in vision-language models, with theoretical guarantees showing reduced sensitivity to input permutations compared to standard GRPO. The method generalizes well to out-of-distribution multi-image mathematical reasoning tasks despite being trained only on single-image math and multi-image natural scene understanding.", "description": "This paper introduces PeRL (Permutation-Enhanced Reinforcement Learning), a reinforcement learning framework designed to improve vision-language models' ability to reason across multiple interleaved images. The approach addresses positional bias by permuting image sequences during training while maintaining semantic consistency through text rephrasing, combined with a rollout filtering mechanism that resamples training data to balance difficulty distribution and improve learning efficiency.", "key_contribution": "The main contribution is a multi-stage RL training strategy that combines image sequence permutation with semantic-preserving text modification to create positional diversity, paired with a rollout filtering mechanism that focuses learning on informative trajectories. This enables VLMs to learn order-invariant representations for robust multi-image reasoning.", "novelty": "Unlike existing multimodal RL approaches (R1-VL, MM-Eureka) that focus on single-image spatial reasoning or standard GRPO training, PeRL specifically targets positional bias in multi-image scenarios through controlled permutation augmentation. The work differs from NoisyRollout by updating policy based on augmented inputs rather than original images, and provides theoretical analysis proving reduced positional sensitivity. It addresses the limitation that current VLMs fail to maintain consistent reasoning when image order changes, as demonstrated by performance degradation on permuted inputs.", "ai_categories": ["Reinforcement Learning for Multimodal Models", "Vision-Language Reasoning and Chain-of-Thought", "Visual Grounding and Spatial Reasoning"], "score": 77.51, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4433922, "title": "PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments", "authors": "Weijie Zhou, Xuantang Xiong, Yi Peng, Manli Tao, Chaoyang Zhao, Honghui Dong, Ming Tang, Jinqiao Wang", "pdf_url": "https://openreview.net/pdf/89b1018647a9f5681e3ad0ed184e80229e3997ee.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57168, "poster_number": 3601, "tag": "SD-6-3601 | Exhibit Hall C,D,E ", "relevance_score": 74.84, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces Active Visual Reasoning (AVR), a paradigm that enables multimodal large language models to actively gather information through sequential physical actions in partially observable environments. The work reveals that current embodied MLLMs can detect information incompleteness but struggle to strategically acquire and integrate new information through interaction. PhysVLM-AVR achieves state-of-the-art performance on the CLEVR-AVR benchmark while maintaining strong results on embodied reasoning tasks (OpenEQA, RoboVQA) and passive visual reasoning benchmarks.", "description": "The paper presents a comprehensive framework for active visual reasoning in partially observable environments, including the CLEVR-AVR benchmark for evaluation and the AVR-152k dataset with rich Chain-of-Thought annotations. The dataset models AVR as a higher-order Markov Decision Process, providing supervision for uncertainty identification, action-conditioned information gain prediction, and information-maximizing action selection. The PhysVLM-AVR model demonstrates that training on this structured data enables agents to perform closed-loop perception-reasoning-action cycles for complex visual reasoning tasks.", "key_contribution": "The main contribution is the formalization of Active Visual Reasoning as a task requiring agents to actively gather information through sequential actions, along with the CLEVR-AVR benchmark and AVR-152k dataset featuring detailed Chain-of-Thought annotations that explicitly model the iterative reasoning process for uncertainty-driven information seeking in partially observable environments.", "novelty": "Unlike existing visual reasoning tasks that assume complete visual inputs or embodied QA tasks that rely on passive observation of pre-captured data, this work uniquely integrates reasoning with strategic, sequential action selection for active information gathering. The paper addresses the fundamental gap in current MLLMs by providing explicit supervision for the complete perception-reasoning-action loop through structured CoT annotations that detail uncertainty identification, information gain prediction, and action selection. The higher-order MDP formulation with multi-step interactive reasoning sequences represents a novel approach to training agents for dynamic decision-making in partially observable settings.", "ai_categories": ["Embodied AI and Vision-Language-Action Models", "Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models"], "score": 74.84, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4204669, "title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning", "authors": "Qi Wang, Yanrui Yu, Ye Yuan, Rui Mao, Tianfei Zhou", "pdf_url": "https://openreview.net/pdf/cfd06225250bc51c74d2efae79070dacdee1e780.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57410, "poster_number": 904, "tag": "SD-6-904 | Exhibit Hall C,D,E ", "relevance_score": 72.57, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "VideoRFT achieves state-of-the-art performance on six video reasoning benchmarks by introducing a novel reinforcement fine-tuning framework specifically designed for video understanding. The paper introduces two large-scale datasets (VideoRFT-CoT-102K and VideoRFT-RL-310K) generated through a cognitively-inspired pipeline that combines reasoning LLMs with MLLMs to produce high-quality chain-of-thought annotations. A semantic-consistency reward mechanism explicitly promotes visual grounding in reasoning outputs, reducing hallucinations and improving cross-modal alignment.", "description": "This paper presents VideoRFT, a reinforcement fine-tuning framework that extends RFT paradigms to video understanding in multimodal large language models. The approach addresses the scarcity of video CoT data through a multi-expert pipeline that generates structured video representations, uses reasoning LLMs for initial CoT generation, and refines outputs through cross-modal verification with MLLMs. The framework employs a two-stage training process (SFT followed by RL) with a novel semantic-consistency reward that ensures reasoning traces are grounded in actual visual content.", "key_contribution": "The main contribution is a comprehensive framework for video reasoning that includes: (1) a scalable, cognitively-inspired pipeline for generating high-quality video CoT datasets, (2) two large-scale datasets supporting video RFT, and (3) a novel semantic-consistency reward mechanism that explicitly enforces visual grounding during reinforcement learning by measuring alignment between video description tokens and visual features.", "novelty": "Unlike previous video reasoning approaches that use rigid templates or rely on sparse visual context, VideoRFT combines the reasoning capabilities of LLMs with the multimodal understanding of MLLMs through a cross-modal refinement process. The work addresses the fundamental bottleneck of lacking large-scale, high-quality video CoT datasets through a multi-stage generation pipeline. The semantic-consistency reward is novel in that it selectively targets only the video-describing portion of reasoning traces for visual alignment, avoiding penalization of abstract reasoning steps that appropriately extend beyond visual scope.", "ai_categories": ["Video Understanding and Temporal Reasoning", "Reinforcement Learning for Multimodal Models", "Vision-Language Reasoning and Chain-of-Thought"], "score": 72.57, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4441643, "title": "Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval", "authors": "Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, Neel Nanda", "pdf_url": "https://openreview.net/pdf/fdeb4ce0127121481dc4bc44af9c24a30eb5a6d3.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57283, "poster_number": 4615, "tag": "SD-6-4615 | Exhibit Hall C,D,E ", "relevance_score": 68.07, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper reveals that 11 out of 14 tested VLMs exhibit factual recall degradation compared to their LLM backbones, with degradation reaching up to 43.9% in some models. The study identifies a fundamental 'two-hop problem': VLMs must first form entity representations from visual inputs (hop 1) before accessing factual knowledge (hop 2), but degraded VLMs resolve entity representations too late in the computation to utilize early-layer MLPs responsible for factual recall. High-performing VLMs like Gemma-3 and Qwen2.5-VL resolve entity representations early enough to reuse existing LLM factual recall mechanisms.", "description": "This paper investigates why vision-language models struggle with factual recall compared to their text-only LLM backbones. Using mechanistic interpretability techniques including attribution patching, activation patching, and linear probing across 14 VLMs, the authors demonstrate that visual representations in adapter-based VLMs are misaligned with the LLM token space and emerge too late to engage early-layer factual recall circuits. The study benchmarks models on 15,000 multimodal factual recall questions and proposes mitigation strategies including chain-of-thought prompting.", "key_contribution": "The paper provides the first mechanistic explanation for factual recall degradation in VLMs, demonstrating through causal interventions that misalignment between visual and textual representations prevents VLMs from utilizing their backbone LLM's factual recall circuits. The work introduces a novel benchmark and shows that patching early-layer MLP outputs from LLM backbones can recover up to 35% of lost performance.", "novelty": "Unlike prior work that observed factual recall failures in VLMs, this paper provides a mechanistic explanation by identifying the specific computational bottleneck: late emergence of entity representations that bypass early-layer MLPs. The work goes beyond correlation to establish causality through systematic patching experiments, and uniquely compares adapter-based versus native VLMs to show that massive multimodal fine-tuning or native pretraining can mitigate the problem by enabling early entity resolution.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Hallucination Detection and Model Robustness", "Multimodal Benchmarks and Evaluation"], "score": 68.07, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4216248, "title": "Two Causally Related Needles in a Video Haystack", "authors": "Miaoyu Li, Qin Chao, Boyang Li", "pdf_url": "https://arxiv.org/pdf/2505.19853", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57153, "poster_number": 4414, "tag": "SD-6-4414 | Exhibit Hall C,D,E ", "relevance_score": 67.36, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Current Video-Language Models (VLMs) struggle significantly with causal two-needle questions, showing substantial performance drops compared to one-needle questions. Model performance is negatively correlated with the distance between the two needles in long videos, revealing critical limitations in joint understanding of causally related events. Even state-of-the-art models like ChatGPT-4o achieve only 11.4% accuracy on two-needle questions requiring joint understanding, compared to 39.2% on single-needle causal questions.", "description": "This paper introduces CAUSAL2NEEDLES, a benchmark for evaluating long-context video understanding that requires models to extract and jointly reason about information from two separate locations (needles) in videos. The benchmark contains 4,100 questions testing both single-needle and two-needle retrieval with causal reasoning about human behaviors, using movie summary videos with narration text. Two complementary question formats (visual grounding and image description) are used to mitigate textual bias.", "key_contribution": "The paper presents the first diagnostic benchmark specifically designed to evaluate VLMs' ability to jointly understand two causally related events at arbitrary distances in long videos, using a novel bridge entity mechanism that forces models to resolve ambiguous references by retrieving both cause and effect events.", "novelty": "Unlike existing benchmarks that focus on single-needle retrieval or multiple needles with independent understanding, CAUSAL2NEEDLES requires joint understanding where comprehension of one needle (cause) depends on correctly interpreting another (effect) through bridge entities. The benchmark addresses the gap in evaluating world models for human behavior causality rather than just object motion, and introduces complementary question formats to balance between avoiding textual bias and preventing out-of-distribution underestimation of model capabilities.", "ai_categories": ["Video Understanding and Temporal Reasoning", "Multimodal Benchmarks and Evaluation", "Compositional and Counterfactual Reasoning"], "score": 67.36, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4183046, "title": "RESAnything: Attribute Prompting for Arbitrary Referring Segmentation", "authors": "Ruiqi Wang, Hao Zhang", "pdf_url": "https://openreview.net/pdf/3ec88c013e57b56be455d70c321c6a870176ef99.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57972, "poster_number": 4517, "tag": "SD-6-4517 | Exhibit Hall C,D,E ", "relevance_score": 66.41, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "RESAnything achieves state-of-the-art zero-shot performance on referring expression segmentation, significantly outperforming existing zero-shot methods (e.g., 68.5% vs 26.2% on RefCOCO) and even approaching supervised methods. The method excels particularly on challenging tasks involving implicit queries, part-level reasoning, and complex relationships, achieving 74.6% gIoU on ReasonSeg and 78.2% gIoU on the new ABO-Image-ARES benchmark without any task-specific training.", "description": "This paper presents RESAnything, a training-free approach for arbitrary referring expression segmentation that handles both explicit semantic labels and implicit references (e.g., material properties, functionality, design features). The method uses a two-stage framework combining attribute prompting with MLLMs to generate detailed descriptions of visual attributes (shape, color, location) for SAM-generated mask proposals, followed by multi-metric selection using both MLLM reasoning and CLIP similarity scores to identify the correct segmentation.", "key_contribution": "The core innovation is attribute prompting for Chain-of-Thought reasoning, which systematically generates detailed text descriptions of object/part attributes to bridge abstract expressions with concrete visual features. This enables zero-shot handling of implicit queries and complex part-level relationships without requiring any training or fine-tuning on specialized datasets.", "novelty": "Unlike existing methods that require fine-tuning on curated datasets (e.g., LISA, GLaMM) or focus only on semantic labels, RESAnything operates in a completely training-free manner while supporting arbitrary expressions including implicit references to functionality, materials, and design. The attribute prompting technique addresses MLLMs' weakness in directly connecting text descriptions to image regions by creating intermediate text-text pairs that enable more robust comparison, and combines MLLM binary decisions with CLIP scalar scores to overcome individual model limitations.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Prompt Learning and Optimization", "Vision-Language Reasoning and Chain-of-Thought"], "score": 66.41, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4202950, "title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs", "authors": "Xiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen, Ziji Zhang, Yingying Zhuang, Narayanan Sadagopan, Anurag Beniwal", "pdf_url": "https://openreview.net/pdf/057301f0efb5252bc107372a0fda3e9e277ad744.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57693, "poster_number": 3503, "tag": "SD-6-3503 | Exhibit Hall C,D,E ", "relevance_score": 65.83, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper reveals a surprising phenomenon: explicit Chain-of-Thought (CoT) reasoning can significantly degrade instruction-following accuracy in large language models. Across 20+ models evaluated on IFEval and ComplexBench, CoT prompting consistently causes performance drops, with models often neglecting simple constraints or introducing unnecessary content that violates instructions. The authors demonstrate that classifier-selective reasoning strategies can substantially recover lost performance, achieving improvements of up to 17 percentage points.", "description": "The paper systematically investigates how explicit reasoning affects instruction-following capabilities in LLMs by evaluating models on two benchmarks with verifiable constraints. Through large-scale case studies and attention-based analysis, the authors identify patterns where reasoning helps (formatting, lexical precision) versus hurts (neglecting simple constraints, adding violating content), and propose a constraint attention metric showing that CoT diverts focus from instruction-relevant tokens. Four mitigation strategies are introduced and evaluated: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning.", "key_contribution": "The paper is the first to systematically expose and analyze reasoning-induced failures in instruction-following tasks, providing both mechanistic insights through attention analysis and practical mitigation strategies. The classifier-selective reasoning approach demonstrates consistent effectiveness across models and benchmarks, offering a principled solution to determine when reasoning should be applied.", "novelty": "Unlike prior work that assumes reasoning universally improves performance, this paper uncovers and characterizes scenarios where explicit CoT reasoning degrades instruction adherence. It introduces a novel constraint attention metric to quantify how reasoning shifts model focus away from constraints, and proposes the first systematic mitigation strategies specifically designed to address reasoning-induced instruction-following failures. The work challenges the prevailing assumption that more reasoning is always beneficial.", "ai_categories": ["Prompt Learning and Optimization", "Hallucination Detection and Model Robustness"], "score": 65.83, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4442635, "title": "ChatVLA-2: Vision-Language-Action Model with Open-World Reasoning", "authors": "Zhongyi Zhou, Yichen Zhu, Xiaoyu Liu, Zhibin Tang, Junjie Wen, Yaxin Peng, Chaomin Shen, Yi Xu", "pdf_url": "https://openreview.net/pdf/c88d737915ea445cb600d21cb0c7125912b7053b.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57271, "poster_number": 2201, "tag": "SD-6-2201 | Exhibit Hall C,D,E ", "relevance_score": 62.48, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "ChatVLA-2 demonstrates that Vision-Language-Action models can retain and leverage pre-trained VLM knowledge for open-world robotic manipulation. The model successfully performs tasks requiring mathematical reasoning, OCR, and spatial understanding that were never explicitly trained, achieving 82.7% success on novel math equations and 81.4% on unseen object placement tasks. This represents a significant advancement over existing VLA models that lose pre-trained capabilities during fine-tuning.", "description": "This paper introduces ChatVLA-2, a vision-language-action model that preserves pre-trained VLM knowledge while enabling robotic control. The approach uses a dynamic mixture-of-experts architecture to disentangle multimodal understanding from robotic control features, combined with a two-stage training strategy that first establishes connections between reasoning and actions, then freezes the VLM to train only the action expert for reasoning-following behavior.", "key_contribution": "The main contribution is a novel VLA architecture with dynamic MoE that successfully retains pre-trained VLM capabilities (reasoning, OCR, object recognition) while enabling end-to-end robotic control, demonstrated through open-world tasks like solving unseen math equations and placing novel objects based on spatial instructions.", "novelty": "Unlike existing VLA models that suffer from catastrophic forgetting of pre-trained knowledge during fine-tuning, ChatVLA-2 explicitly preserves VLM capabilities through dynamic mixture-of-experts that separate task-specific features while maintaining shared representations. The two-stage training strategy and reasoning-following enhancement module ensure actions align with internal reasoning, enabling generalization to tasks absent from training data. This addresses the fundamental limitation that current VLAs cannot leverage their pre-trained knowledge for open-world manipulation.", "ai_categories": ["Embodied AI and Vision-Language-Action Models", "Vision-Language Reasoning and Chain-of-Thought", "Visual Grounding and Spatial Reasoning"], "score": 62.48, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4399927, "title": "MuSLR: Multimodal Symbolic Logical Reasoning", "authors": "Jundong Xu, Hao Fei, Yuhui Zhang, Liangming Pan, Qijun Huang, Qian Liu, Preslav Nakov, Min-Yen Kan, William Yang Wang, ..., Wynne Hsu", "pdf_url": "https://openreview.net/pdf/c3fc5a8b459eec88aa0b5113f6e1f32a2d2743e4.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57202, "poster_number": 103, "tag": "SD-6-103 | Exhibit Hall C,D,E ", "relevance_score": 64.36, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Current state-of-the-art vision-language models struggle significantly with multimodal symbolic logical reasoning, with the best model (GPT-4.1) achieving only 46.8% accuracy. The proposed LogiCAM framework improves GPT-4.1's performance by 14.13% overall, with even larger gains (48.93%) on complex first-order logic tasks. Error analysis reveals that approximately 70% of failures stem from logical misalignment between visual and textual modalities, highlighting a critical gap in current VLMs.", "description": "This paper introduces MuSLR (Multimodal Symbolic Logical Reasoning), the first task and benchmark for evaluating vision-language models on formal logical reasoning that requires integrating both visual and textual inputs. The benchmark comprises 1,093 instances across 7 domains with reasoning depths from 2 to 9, covering propositional logic, first-order logic, and non-monotonic logic. The authors propose LogiCAM, a modular Chain-of-Thought framework that decomposes multimodal symbolic reasoning into premise selection, reasoning type identification, and formal inference steps.", "key_contribution": "The main contribution is the introduction of MuSLR as the first multimodal symbolic logical reasoning task grounded in formal logic rules, accompanied by MuSLR-Bench (a rigorously annotated benchmark dataset) and LogiCAM (a modular framework that systematically applies formal logical rules to multimodal inputs, significantly outperforming existing approaches).", "novelty": "Unlike existing benchmarks that focus on either purely textual symbolic reasoning or multimodal reasoning without formal logic, MuSLR uniquely requires models to perform rigorous symbolic deduction (e.g., Modus Ponens, Hypothetical Syllogism) over combined visual and textual inputs. The work addresses the limitation that previous multimodal benchmarks lack explicit formal logical rules, while textual symbolic reasoning benchmarks cannot handle visual information. LogiCAM's modular design explicitly separates multimodal premise selection, reasoning type identification, and symbolic inference, enabling more systematic and verifiable reasoning compared to end-to-end approaches.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Multimodal Benchmarks and Evaluation", "Compositional and Counterfactual Reasoning"], "score": 64.36, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4204312, "title": "Do different prompting methods yield a common task representation in language models?", "authors": "Guy Davidson, Todd M. Gureckis, Brenden Lake, Adina Williams", "pdf_url": "https://openreview.net/pdf/3b10744b26832eee4512c27477a8af62d2dea11f.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57165, "poster_number": 1016, "tag": "SD-6-1016 | Exhibit Hall C,D,E ", "relevance_score": 61.24, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Different prompting methods (demonstrations vs. instructions) do not induce a common task representation in language models through function vectors, but rather activate mostly distinct, partly overlapping mechanisms. Instruction-based function vectors can be successfully extracted and promote zero-shot task accuracy, though they engage different attention heads than demonstration-based function vectors. Combining both types of function vectors provides complementary benefits, supporting the practice of using both instructions and demonstrations together.", "description": "This paper extends the function vector (FV) extraction method from in-context demonstrations to textual instructions, investigating whether different task presentations elicit common task representations in large language models. The authors analyze the effectiveness, attention head overlap, and functional properties of demonstration-based versus instruction-based function vectors across multiple model families, examining how different prompting approaches influence internal task representations.", "key_contribution": "The paper generalizes function vectors beyond demonstrations to arbitrary task presentations (particularly instructions), demonstrating that different prompting methods activate distinct but complementary mechanisms in LLMs, with instruction FVs identifying mostly different attention heads than demonstration FVs while still providing additive benefits when combined.", "novelty": "Unlike prior work that studied either demonstration-based ICL or instruction-following in isolation, this work directly compares task representations arising from both approaches using a unified framework. The paper addresses the limitation of understanding whether different task presentations share common mechanisms by extending FV extraction to instructions and showing an asymmetry where instruction-identified heads are more useful for demonstrations than vice versa, suggesting instruction-following leverages a distinct mechanism developed during post-training.", "ai_categories": ["Prompt Learning and Optimization", "In-Context Learning and Few-Shot Adaptation"], "score": 61.24, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4384656, "title": "ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding", "authors": "Jialiang Kang, Han Shu, Wenshuo Li, Yingjie Zhai, Xinghao Chen", "pdf_url": "https://openreview.net/pdf/5cc1cc1fbfadc72f4c5f96b94d29671e352d2f1f.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57384, "poster_number": 4717, "tag": "SD-6-4717 | Exhibit Hall C,D,E ", "relevance_score": 62.76, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "ViSpec achieves the first substantial speedup (up to 3.22√ó) in vision-language model inference through speculative decoding, significantly outperforming existing methods like Medusa (1.67√ó) and EAGLE-2 (2.17√ó). The framework addresses the fundamental challenge that small draft models struggle to process redundant visual information while maintaining textual coherence, a limitation that has prevented effective speculative decoding in VLMs until now.", "description": "This paper introduces Vision-Aware Speculative Decoding (ViSpec), a novel framework designed specifically for accelerating vision-language models. ViSpec employs a lightweight vision adaptor to compress image tokens into compact representations and injects global visual features into text tokens to maintain multimodal coherence. The authors develop a specialized training strategy using synthetic long-response datasets generated from existing multimodal data to overcome the scarcity of suitable training data.", "key_contribution": "The main contribution is ViSpec, the first speculative decoding framework to achieve meaningful acceleration for VLMs by introducing vision-aware mechanisms including image embedding compression, global visual feature injection, and a novel training strategy with multi-token prediction that prevents shortcut learning.", "novelty": "Unlike prior work that achieved only marginal speedups (<1.5√ó) by directly applying text-only speculative decoding methods to VLMs, ViSpec addresses the fundamental issue that shallow draft models cannot effectively process long, redundant image sequences. The framework introduces dual integration mechanisms (attention-based compression and feature augmentation) specifically designed for multimodal inputs, and develops a training approach that leverages synthetic long-response generation with multi-token prediction to prevent the draft model from exploiting direct access to target model hidden states.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Multimodal Benchmarks and Evaluation"], "score": 62.76, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4232450, "title": "Multimodal Tabular Reasoning with Privileged Structured Information", "authors": "Jun-Peng Jiang, Yu Xia, Hai-Long Sun, Shiyin Lu, Qing-Guo Chen, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, Han-Jia Ye", "pdf_url": "https://openreview.net/pdf/c140cd5b9ba2e6981d9024c35dbfc29dc881e63b.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57295, "poster_number": 3602, "tag": "SD-6-3602 | Exhibit Hall C,D,E ", "relevance_score": 58.01, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "TURBO achieves state-of-the-art performance on multimodal tabular reasoning tasks with only 9k training examples, showing +7.2% improvement over previous SOTA. The framework successfully transfers reasoning capabilities from structured tables (privileged information available only during training) to enable accurate reasoning directly from table images. Despite being 85x smaller than DeepSeek-R1 (8B vs 671B parameters), TURBO achieves competitive performance even when the larger model has access to clean structured tables.", "description": "This paper introduces TURBO, a framework for multimodal tabular reasoning that leverages structured tables as privileged information during training to enhance MLLMs' ability to reason over table images. The approach uses a two-stage training pipeline: first generating high-quality structure-aware reasoning traces using DeepSeek-R1, then applying supervised fine-tuning followed by reinforcement learning (GRPO) to progressively improve reasoning capabilities. The framework addresses the practical challenge where structured tables are unavailable at inference time in real-world scenarios.", "key_contribution": "The main contribution is a novel framework that bridges the modality gap between structured tables and table images through modality-agnostic reasoning traces, combined with a two-stage training approach (SFT + GRPO) that effectively transfers reasoning capabilities from LLMs to MLLMs using privileged structured information available only during training.", "novelty": "Unlike previous work that focuses on reasoning over structured tables or requires structured input at inference time, TURBO introduces the concept of using structured tables as privileged information exclusively during training. The framework generates structure-aware reasoning traces that are modality-independent, enabling knowledge transfer from text-based LLMs to vision-language models. This approach addresses the gap between training conditions and real-world deployment where only table images are available, while previous methods like HIPPO require both modalities at inference time.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Multimodal Benchmarks and Evaluation", "Prompt Learning and Optimization"], "score": 58.01, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4202583, "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning", "authors": "Yue Liu, Shengfang Zhai, Mingzhe Du, Yulin Chen, Tri Cao, Hongcheng Gao, Cheng Wang, Xinfeng Li, Kun Wang, ..., Bryan Hooi", "pdf_url": "https://openreview.net/pdf/3c2def3de954b67f399af3bd84c4e317c6d3b893.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57438, "poster_number": 5006, "tag": "SD-6-5006 | Exhibit Hall C,D,E ", "relevance_score": 59.03, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "GuardReasoner-VL achieves state-of-the-art performance on multimodal safety benchmarks, surpassing the runner-up by 19.27% F1 score on average. The model demonstrates that incentivizing deliberative reasoning before moderation decisions significantly improves performance, explainability, and generalization across text, image, and text-image modalities. The approach successfully balances safety performance with token efficiency through a novel length-aware reward mechanism.", "description": "This paper introduces GuardReasoner-VL, a reasoning-based vision-language model guard that safeguards VLMs by first reasoning and then making moderation decisions. The authors construct GuardReasoner-VLTrain, a reasoning corpus with 123K samples and 631K reasoning steps spanning multiple modalities, and train the model using supervised fine-tuning followed by online reinforcement learning with safety-aware data augmentation, dynamic clipping, and length-aware rewards.", "key_contribution": "The main contribution is a novel reasoning-based VLM guard model that deliberatively reasons before moderating, trained via online RL with three key innovations: safety-aware data concatenation for augmentation, dynamic clipping parameters for exploration-exploitation balance, and a length-aware safety reward that integrates accuracy, format compliance, and token efficiency.", "novelty": "Unlike existing VLM guard models that only output classification results, this work introduces reasoning capabilities to improve interpretability and generalization to new harmful categories. The paper addresses limitations of prior work through online RL training (vs. offline-only approaches), multimodal reasoning data covering text, image, and text-image inputs, and a novel safety-aware data concatenation strategy that trains models to detect harmful content hidden among benign content. The length-aware reward mechanism uniquely balances performance with token efficiency.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Hallucination Detection and Model Robustness"], "score": 59.03, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4441305, "title": "Learning Robust Vision-Language Models from Natural Latent Spaces", "authors": "Zhangyun Wang, Ni Ding, Aniket Mahanti", "pdf_url": "https://openreview.net/pdf/e8595a31cf77a217d9396e59495cf5cba4c01d8e.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57664, "poster_number": 3808, "tag": "SD-6-3808 | Exhibit Hall C,D,E ", "relevance_score": 58.14, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "CoAPT achieves an excellent trade-off among natural generalization, adversarial robustness, and task-specific adaptation, improving natural and adversarial robustness performance by an average of 9.83% and 24.16% respectively across 15 datasets and four benchmarks. The method introduces a novel paradigm that learns robust vision-language models from the natural latent space of pre-trained CLIP, using an adaptive total variation regularization to suppress high-frequency adversarial perturbations while preserving edge structures. The approach demonstrates strong cross-task transferability and maintains performance without requiring benchmark-specific hyperparameter tuning.", "description": "This paper proposes Collaborative Adversarial Prompt Tuning (CoAPT), a method to enhance adversarial robustness of vision-language models like CLIP while maintaining natural generalization. The approach uses an improved real-time total variation algorithm to suppress high-frequency details from images that contain adversarial perturbations, then restores corrupted natural features under guidance from the pre-trained CLIP's latent space representations. The method employs deep multimodal adversarial prompts, low-rank residual modules, and R√©nyi divergence regularization to minimize discrepancies between natural and adversarial distributions.", "key_contribution": "The main contribution is a novel adversarial prompt tuning paradigm that collaborates pre-trained natural CLIP with target robust CLIP, using adaptive fast gradient projection for high-frequency suppression and latent space reconstruction to preserve generalization while achieving robustness. This is combined with a spatially adaptive regularization strategy and R√©nyi divergence to explicitly regulate distributional discrepancies between natural and adversarial examples.", "novelty": "Unlike existing adversarial prompt tuning methods that struggle to balance natural generalization and adversarial robustness, CoAPT introduces a collaborative framework where corrupted natural features are restored in the latent space guided by natural CLIP. The work addresses the limitation of masked image modeling approaches that focus on high-frequency components where adversarial perturbations concentrate, instead using pixel-level corruption with adaptive total variation regularization. The method achieves superior performance without dataset-specific hyperparameter tuning, demonstrating better generalization across in-distribution, out-of-distribution, and zero-shot scenarios.", "ai_categories": ["Hallucination Detection and Model Robustness", "Prompt Learning and Optimization", "Multimodal Benchmarks and Evaluation"], "score": 58.14, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4207717, "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "authors": "Haolei Xu, Yuchen Yan, Yongliang Shen, Wenqi Zhang, Guiyang Hou, Shengpei Jiang, Kaitao Song, Weiming Lu, Jun Xiao, Yueting Zhuang", "pdf_url": "https://openreview.net/pdf/b63a80f12eba96f7a1d80c464c6cd302c8b2c6b6.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 58020, "poster_number": 3502, "tag": "SD-6-3502 | Exhibit Hall C,D,E ", "relevance_score": 56.47, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper identifies and formalizes the 'Thought Leap' phenomenon in Chain-of-Thought (CoT) reasoning, where intermediate reasoning steps are omitted, negatively impacting model learning. Models fine-tuned on bridged datasets (with thought leaps filled) achieve significant performance improvements of up to +5.87% on mathematical benchmarks and +2.99% on out-of-domain logical reasoning tasks. The approach functions as a plug-and-play module that enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%).", "description": "The paper introduces the CoT Thought Leap Bridge Task, which automatically detects gaps in reasoning chains where intermediate steps are missing and generates the necessary bridging content. The authors construct ScaleQM+, a specialized training dataset based on ScaleQuestMath, and train CoT-Bridge (based on Qwen2.5-Math-7B) to identify and fill these thought leaps. They apply this approach to existing mathematical reasoning datasets (MetaMathQA and NuminaMath) to enhance their completeness and coherence.", "key_contribution": "The main contribution is the systematic identification and formalization of the Thought Leap phenomenon in CoT reasoning, along with a specialized model (CoT-Bridge) and dataset (ScaleQM+) that can automatically detect and bridge these reasoning gaps to improve the quality of training data and downstream model performance.", "novelty": "Unlike prior work focusing on factual errors or answer accuracy, this is the first to systematically address the structural completeness of reasoning chains by identifying and filling omitted intermediate steps. The approach differs from existing methods by treating reasoning completeness as a distinct problem requiring specialized detection and repair mechanisms, rather than relying on general prompt engineering or data augmentation. It addresses the limitation that expert-generated CoT demonstrations often contain implicit knowledge gaps that hinder effective model learning.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization"], "score": 56.47, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4239818, "title": "Learning to Focus: Causal Attention Distillation via Gradient‚ÄêGuided Token Pruning", "authors": "Yiju Guo, Wenkai Yang, Zexu Sun, Ning Ding, Zhiyuan Liu, Yankai Lin", "pdf_url": "https://openreview.net/pdf/ccfb52a7538dd17ba250574015c53a7ab5a47315.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57532, "poster_number": 4008, "tag": "SD-6-4008 | Exhibit Hall C,D,E ", "relevance_score": 55.56, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "The paper demonstrates that large language models struggle to focus on critical information during long-context reasoning, with certain distracting patterns misdirecting attention. By identifying and pruning these confounding tokens using gradient-based comparisons between teacher and student models, the method achieves over 20% improvement on math benchmarks and over 10% on code generation tasks. The approach not only improves accuracy but also makes models more interpretable by aligning attention with causally relevant information.", "description": "This paper introduces Learning to Focus (LeaF), a two-stage framework that uses causal intervention to improve LLM reasoning by identifying and removing confounding tokens. The method employs gradient-based comparisons between an advanced teacher model and student model to detect distracting patterns in training data, then performs causal attention distillation using both original and counterfactual samples (with confounders pruned) to align the student's attention with truly critical context tokens.", "key_contribution": "The main contribution is a causal framework for knowledge distillation that automatically identifies confounding tokens via gradient-based teacher-student comparisons and uses counterfactual distillation to teach models to focus on causally relevant information rather than spurious correlations, improving both reasoning accuracy and model interpretability.", "novelty": "Unlike existing knowledge distillation methods that focus on output imitation or token importance based on self-assessment, this work takes a causal perspective to identify confounding tokens by comparing gradient sensitivities between teacher and student models. It addresses the limitation that models attend to distracting patterns rather than critical information by introducing counterfactual distillation that explicitly removes spurious correlations, going beyond prior work on token pruning or reasoning consistency that doesn't leverage teacher guidance for identifying confounders.", "ai_categories": ["Prompt Learning and Optimization", "Hallucination Detection and Model Robustness", "In-Context Learning and Few-Shot Adaptation"], "score": 55.56, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4216272, "title": "Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought", "authors": "Chao Huang, Benfeng Wang, Wei Wang, Jie Wen, Chengliang Liu, Li Shen, Xiaochun Cao", "pdf_url": "https://openreview.net/pdf/374b81cce984cf48963806b2557f39ee9972a826.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57998, "poster_number": 4515, "tag": "SD-6-4515 | Exhibit Hall C,D,E ", "relevance_score": 55.0, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces Video Anomaly Reasoning (VAR), a new task that extends video anomaly detection from surface-level recognition to deep cognitive understanding. The proposed Vad-R1 framework achieves superior performance over both open-source and proprietary models, outperforming GPT-4o and other reasoning MLLMs on anomaly reasoning tasks. The work demonstrates that structured Chain-of-Thought reasoning combined with reinforcement learning significantly improves anomaly detection and explanation capabilities.", "description": "The paper presents Vad-R1, an end-to-end MLLM-based framework for video anomaly reasoning that uses a Perception-to-Cognition Chain-of-Thought (P2C-CoT) to guide models through structured reasoning about anomalies. The approach includes a new Vad-Reasoning dataset with fine-grained CoT annotations and a two-stage training pipeline combining supervised fine-tuning with an improved reinforcement learning algorithm (AVA-GRPO) that uses self-verification mechanisms to enhance reasoning capability.", "key_contribution": "The main contribution is the introduction of Video Anomaly Reasoning as a new task, along with the Vad-R1 framework featuring a structured Perception-to-Cognition Chain-of-Thought paradigm, a specialized Vad-Reasoning dataset with 8,641 videos, and the AVA-GRPO algorithm that incentivizes reasoning capability through anomaly verification rewards with limited annotations.", "novelty": "Unlike existing MLLM-based VAD methods that only provide shallow anomaly descriptions, this work enables deep causal reasoning by simulating human cognitive processes from perception to cognition. The paper addresses the limitation of lacking structured reasoning annotations by designing a multi-stage annotation pipeline and introduces a novel self-verification mechanism in reinforcement learning that evaluates reasoning quality through temporal video trimming and re-prediction. The AVA-GRPO algorithm extends standard GRPO with anomaly-specific verification rewards, enabling effective training with only video-level weak labels.", "ai_categories": ["Vision-Language Reasoning and Chain-of-Thought", "Video Understanding and Temporal Reasoning", "Reinforcement Learning for Multimodal Models"], "score": 55.0, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4442788, "title": "Struct2D: A Perception-Guided Framework for Spatial Reasoning in MLLMs", "authors": "Fangrui Zhu, Hanhui Wang, Yiming Xie, Jing Gu, Tianye Ding, Jianwei Yang, Huaizu Jiang", "pdf_url": "https://openreview.net/pdf/45604c251617139c1947693fb938f69e20844fb1.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57616, "poster_number": 4616, "tag": "SD-6-4616 | Exhibit Hall C,D,E ", "relevance_score": 54.2, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that MLLMs can perform robust 3D spatial reasoning using only structured 2D inputs (BEV images with object marks and metadata) without requiring explicit 3D representations at inference. Zero-shot analysis on GPT-o3 reveals surprisingly strong spatial reasoning capabilities when provided with properly structured 2D projections, achieving 96.5% on relative distance and 94.4% on relative direction tasks. The fine-tuned open-source model (Qwen2.5VL) achieves competitive performance across multiple benchmarks while using significantly fewer visual frames than prior methods.", "description": "The paper introduces Struct2D, a perception-guided prompting framework that enables spatial reasoning in MLLMs by transforming 3D perception outputs into structured 2D inputs consisting of bird's-eye-view (BEV) images with object marks, object-centric metadata, and optional egocentric keyframes. The authors construct Struct2D-Set, a large-scale instruction tuning dataset with 200K QA pairs across eight spatial reasoning categories, and demonstrate that fine-tuning open-source MLLMs on this dataset achieves strong performance on 3D question answering, dense captioning, and object grounding tasks.", "key_contribution": "The main contribution is a perception-guided 2D prompting strategy that bridges 3D perception and language reasoning without requiring explicit 3D features as model input, along with Struct2D-Set, a large-scale automatically generated instruction tuning dataset covering diverse spatial reasoning tasks grounded in realistic 3D indoor scenes.", "novelty": "Unlike prior work that relies on explicit 3D point cloud inputs or uniform video frame sampling, Struct2D uses structured 2D representations (BEV images with filtered object marks and metadata) derived from 3D perception as an intermediate step. This approach addresses the limitations of incomplete perception and lack of global context in video-based methods while avoiding the infrastructure requirements of point cloud-based LLMs. The work introduces filtered object marks tailored to queries, rotation-aligned BEV images, and depth-aware keyframe selection, making spatial reasoning more efficient and interpretable.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Vision-Language Reasoning and Chain-of-Thought", "Prompt Learning and Optimization"], "score": 54.2, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4135152, "title": "From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D", "authors": "Jiahui Zhang, Yurui Chen, Yueming Xu, Ze Huang, Jilin Mei, Chunhui Chen, Yanpeng Zhou, Yu-Jie Yuan, Xinyue Cai, ..., Li Zhang", "pdf_url": "https://arxiv.org/pdf/2503.22976", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57722, "poster_number": 116, "tag": "SD-6-116 | Exhibit Hall C,D,E ", "relevance_score": 52.25, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper demonstrates that VLMs can achieve strong 3D spatial understanding using only 2D multi-view images, without requiring explicit 3D representations like point clouds. The SPAR-7M dataset with 7M+ QA pairs across 33 spatial tasks enables models to achieve state-of-the-art performance on spatial benchmarks (CV-Bench 3D, VSI-Bench) and competitive results on 3D-specific tasks. The work shows that proper 2D spatial supervision derived from 3D ground truth can unlock spatial reasoning capabilities comparable to methods using direct 3D inputs.", "description": "The paper introduces SPAR-7M, a large-scale dataset for teaching vision-language models spatial perception and reasoning using 2D images derived from 3D scenes. The authors develop a novel data generation pipeline that leverages 3D ground truth from indoor scene datasets to create diverse spatial tasks ranging from basic depth estimation to complex multi-view spatial imagination. They also present SPAR-Bench, a comprehensive benchmark with 7,207 manually validated questions for evaluating spatial understanding across single-view and multi-view settings.", "key_contribution": "The main contribution is a scalable 2D spatial data generation pipeline that creates 7M+ training samples and a comprehensive benchmark (SPAR-Bench) spanning 33 spatial tasks from low-level perception to high-level reasoning. This enables VLMs to learn robust 3D spatial understanding from 2D observations alone, achieving competitive performance without architectural modifications or explicit 3D input modalities.", "novelty": "Unlike existing approaches that inject 3D representations (point clouds, voxels) into VLMs, this work explores learning spatial understanding purely from spatially-relevant 2D images. The paper addresses limitations of prior datasets that either lack precise 3D annotations, focus only on high-level reasoning while neglecting fundamental perception, or don't support multi-view inputs effectively. The novel pipeline generates both numerical (depth, distance) and semantic (spatial relations) tasks with precise 3D ground truth, creating a more comprehensive spatial understanding dataset than previous work.", "ai_categories": ["Visual Grounding and Spatial Reasoning", "Multimodal Benchmarks and Evaluation", "Embodied AI and Vision-Language-Action Models"], "score": 52.25, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4440371, "title": "Instance-Level Composed Image Retrieval", "authors": "Bill Psomas, George Retsinas, Nikos Efthymiadis, Panagiotis Filntisis, Yannis Avrithis, Petros Maragos, Ondrej Chum, Giorgos Tolias", "pdf_url": "https://openreview.net/pdf/adc14ffb898a463ae55a846a3e9f69bd1d5584c0.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57577, "poster_number": 4514, "tag": "SD-6-4514 | Exhibit Hall C,D,E ", "relevance_score": 51.98, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "This paper introduces i-CIR, the first instance-level composed image retrieval benchmark with explicit hard negatives across visual, textual, and composed dimensions. The proposed training-free method BASIC achieves state-of-the-art performance on i-CIR and existing CIR datasets by leveraging pre-trained VLMs through semantic projection, contextualization, and Harris-inspired fusion. The dataset demonstrates genuine compositional requirements with +490% relative gain over uni-modal baselines, compared to only +11-26% on existing datasets.", "description": "The paper addresses composed image retrieval (CIR) where queries combine an image and text to retrieve images showing the same object instance under specified modifications. It introduces i-CIR, a carefully curated evaluation dataset of 202 instances with 1,883 composed queries and 750K images including challenging hard negatives. The proposed BASIC method is training-free, using separate image-to-image and text-to-image similarities enhanced through semantic projection and contextualization, then fused via a multiplicative operation with Harris criterion regularization.", "key_contribution": "The main contributions are: (1) i-CIR dataset - the first instance-level CIR benchmark with explicit hard negatives and diverse object types/modifications, and (2) BASIC - a training-free approach that sets new state-of-the-art across multiple CIR benchmarks by effectively combining visual and textual modalities through semantic projection, query contextualization, and principled fusion.", "novelty": "Unlike existing CIR datasets that use semantic-level class definitions and automated construction leading to poor quality and text-only shortcuts, i-CIR focuses on instance-level retrieval with manually curated hard negatives. The work addresses the lack of appropriate training data by proposing a training-free method that fully leverages VLM capabilities without requiring triplet supervision. BASIC introduces novel components including text-space semantic projection for image features, query-time contextualization for text, and Harris-inspired fusion that requires joint activation of both modalities rather than dominance by one.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Vision-Language Reasoning and Chain-of-Thought"], "score": 51.98, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4431220, "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning", "authors": "Gunshi Gupta, Karmesh Yadav, Zsolt Kira, Yarin Gal, Rahaf Aljundi", "pdf_url": "https://openreview.net/pdf/4877c8c9446b72d733a5677610304c04fd55f44d.pdf", "session_id": 557, "session_name": "San Diego Poster Session 6", "poster_id": 57397, "poster_number": 316, "tag": "SD-6-316 | Exhibit Hall C,D,E ", "relevance_score": 51.44, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "Memo introduces a transformer-based architecture that learns to compress long-horizon experiences through periodic summarization tokens, achieving 8-10√ó memory reduction while outperforming full-context transformers by 7.5% in success rate on navigation tasks. The method demonstrates superior robustness in streaming settings and better context length extrapolation, maintaining performance when historical context must be truncated due to inference constraints.", "description": "This paper presents Memo, a memory-efficient reinforcement learning framework for embodied agents that addresses the context length limitations of transformers in long-horizon tasks. The approach interleaves learnable summary tokens with visual inputs during training, enabling the model to compress and retrieve task-relevant memories while propagating gradients across all summaries. The method is evaluated on grid-world meta-RL benchmarks and photo-realistic 3D navigation tasks using both on-policy (ReLIC) and off-policy (AMAGO) RL algorithms.", "key_contribution": "The main contribution is a learned context summarization mechanism that enables transformers to periodically compress past experiences into summary tokens, which are accumulated and attended to in future timesteps. This allows efficient memory utilization through gradient propagation across all summaries while maintaining a compact KV cache, unlike prior methods that either use fixed-size recurrent memory or require full-context access.", "novelty": "Unlike Autocompressors which fine-tune pretrained models with truncated backpropagation, Memo trains from scratch with full gradient propagation through all summary tokens, enabling more effective task-driven memory formation. The work extends context summarization from language modeling to reinforcement learning, addressing unique challenges like sparse rewards and credit assignment over long horizons. Memo's summary accumulation mechanism provides residual-like gradient shortcuts that outperform both fixed-size recurrent memory (RMT) and full-context transformers in long-horizon decision-making tasks.", "ai_categories": ["Embodied AI and Vision-Language-Action Models", "Reinforcement Learning for Multimodal Models", "Visual Grounding and Spatial Reasoning"], "score": 51.44, "session_type": "San Diego Poster Session 6"}, {"paper_id": 4442425, "title": "AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models", "authors": "Xinyi Wang, Xun Yang, Yanlong Xu, Yuchen Wu, Zhen Li, Na Zhao", "pdf_url": "https://openreview.net/pdf/686a233441037675c3bfb6cea63ba14355cad978.pdf", "session_id": 558, "session_name": "Mexico City Poster Session 6", "poster_id": 58037, "poster_number": 999, "tag": "MC-6-999 | Foyer ", "relevance_score": 81.44, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "AffordBot achieves state-of-the-art performance on fine-grained 3D embodied reasoning by unifying affordance grounding and motion estimation into a single instruction-conditioned framework. The method demonstrates that MLLMs can perform physically grounded reasoning on 3D scenes using only point cloud input through a tailored chain-of-thought paradigm, without requiring video sequences. The active perception mechanism and surround-view synthesis significantly improve performance over existing methods, with AP25 reaching 23.3% compared to 18.7% for the previous best approach.", "description": "This paper introduces a new task called Fine-grained 3D Embodied Reasoning that requires agents to predict structured triplets (3D mask, motion type, motion axis direction) for affordance elements in 3D scenes based on natural language instructions. AffordBot addresses this by bridging 3D point clouds with 2D-native MLLMs through surround-view image synthesis and 3D-to-2D projection, then employing a chain-of-thought reasoning process that includes active viewpoint selection, affordance grounding, and motion estimation stages.", "key_contribution": "The main contribution is a unified framework that integrates 3D perception with MLLM reasoning through holistic multimodal representation construction and a task-specific chain-of-thought paradigm. This enables instruction-conditioned joint prediction of affordance locations and interaction motions directly from 3D point clouds, eliminating the need for video processing while achieving superior performance.", "novelty": "Unlike prior work that treats affordance grounding and motion estimation as separate, instruction-agnostic tasks or relies on computationally expensive video processing, AffordBot unifies these into a coherent instruction-conditioned framework. The novel active perception stage allows the MLLM to autonomously select the most informative viewpoint from synthesized surround views, while the tailored chain-of-thought paradigm enables physically grounded reasoning. This approach overcomes limitations of video-based methods including limited field of view, redundant frame processing, and incomplete target coverage.", "ai_categories": ["Embodied AI and Vision-Language-Action Models", "Vision-Language Reasoning and Chain-of-Thought", "Visual Grounding and Spatial Reasoning"], "score": 81.44, "session_type": "Mexico City Poster Session 6"}, {"paper_id": 3984136, "title": "OCRBench v2: An Improved Benchmark for Evaluating Large Multimodal Models on Visual Text Localization and Reasoning", "authors": "Ling Fu, Zhebin Kuang, Jiajun Song, Mingxin Huang, Biao Yang, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, ..., Xiang Bai", "pdf_url": "https://arxiv.org/pdf/2501.00321", "session_id": 558, "session_name": "Mexico City Poster Session 6", "poster_id": 58035, "poster_number": 999, "tag": "MC-6-999 | Foyer ", "relevance_score": 56.45, "award": false, "bookmarked": false, "liked": false, "disliked": false, "pinned": false, "key_findings": "OCRBench v2 reveals that state-of-the-art Large Multimodal Models (LMMs) score below 50 on average across comprehensive OCR tasks, with significant weaknesses in text localization, handwritten content extraction, complex element parsing, and logical reasoning. The benchmark demonstrates consistent evaluation trends across both public and private test sets, validating its reliability. Most critically, models struggle with less frequently encountered texts, fine-grained spatial perception, layout understanding, and structured output generation despite strong performance on basic text recognition.", "description": "This paper introduces OCRBench v2, a comprehensive benchmark for evaluating OCR capabilities of LMMs across 23 tasks spanning 8 core abilities (text recognition, referring, spotting, relation extraction, element parsing, mathematical calculation, visual text understanding, and knowledge reasoning). The benchmark contains 10,000 human-verified question-answering pairs covering 31 diverse scenarios in both English and Chinese, plus a private test set of 1,500 manually annotated images to prevent training contamination and ensure robust evaluation.", "key_contribution": "OCRBench v2 provides the most comprehensive OCR evaluation framework for LMMs to date, with 4√ó more tasks than previous benchmarks, systematic coverage of 31 scenarios, six types of evaluation metrics, and a private test set for reliable generalization assessment. It reveals critical capability gaps in current LMMs across text localization, complex element parsing, and reasoning tasks that were not adequately captured by existing benchmarks.", "novelty": "Unlike previous OCR benchmarks that focus on narrow domains or basic text recognition, OCRBench v2 addresses the gap between high performance on existing benchmarks and poor real-world OCR capabilities by introducing challenging tasks like text spotting with coordinate output, handwritten content extraction, complex element parsing (tables, charts, documents, formulas), and reasoning-based VQA. The benchmark uniquely combines structured output requirements (Markdown, HTML, JSON, LaTeX), fine-grained localization tasks, and a private evaluation set to prevent overfitting, revealing that models achieving 88-96% on previous benchmarks still struggle significantly on comprehensive OCR tasks.", "ai_categories": ["Multimodal Benchmarks and Evaluation", "Visual Grounding and Spatial Reasoning", "Vision-Language Reasoning and Chain-of-Thought"], "score": 56.45, "session_type": "Mexico City Poster Session 6"}];

        // Embedded author data
        const authors = [{"name": "Chaoya Jiang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 97.2, "max_score": 97.22, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VLM-R¬≥: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought", "score": 97.22, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/5e7d2aa638471c63b356d40e8943dbfd513adca1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Peking University", "role": "PhD Student", "photo_url": "https://assets.underline.io/profile/213006/square_avatar/medium-582b2dc35bba12fced659ade07f76db5.jpg", "profile_url": "https://underline.io/speakers/213006-chaoya-jiang"}, {"name": "Yongrui Heng", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 97.2, "max_score": 97.22, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VLM-R¬≥: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought", "score": 97.22, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/5e7d2aa638471c63b356d40e8943dbfd513adca1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Peking University", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shikun Zhang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 97.2, "max_score": 97.22, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VLM-R¬≥: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought", "score": 97.22, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/5e7d2aa638471c63b356d40e8943dbfd513adca1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Peking University", "role": "Professor", "photo_url": "https://se.pku.edu.cn/images/content/2017-11/20171118203119707418.jpg", "profile_url": "https://se.pku.edu.cn/ky/kyry/index.htm"}, {"name": "junyan ye", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 94.9, "max_score": 94.91, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "BLINK-Twice: You see, but do you observe?  A Reasoning Benchmark on Visual Perception", "score": 94.91, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2510.09361", "relevant": 0, "reads": 0}], "affiliation": "Sun Yat-sen University", "role": "Master's Student", "photo_url": "https://avatars.githubusercontent.com/u/83576010?v=4", "profile_url": "https://yejy53.github.io/"}, {"name": "DONGZHI JIANG", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 94.9, "max_score": 94.91, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "BLINK-Twice: You see, but do you observe?  A Reasoning Benchmark on Visual Perception", "score": 94.91, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2510.09361", "relevant": 0, "reads": 0}], "affiliation": "The Chinese University of Hong Kong", "role": "PhD Student", "photo_url": "https://github.com/CaraJ7.png", "profile_url": "https://caraj7.github.io/"}, {"name": "Weijia Li", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 94.9, "max_score": 94.91, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "BLINK-Twice: You see, but do you observe?  A Reasoning Benchmark on Visual Perception", "score": 94.91, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2510.09361", "relevant": 0, "reads": 0}], "affiliation": "Sun Yat-Sen University", "role": "Associate Professor", "photo_url": "https://liweijia.github.io/assets/img/liweijia-2.png", "profile_url": "https://liweijia.github.io/"}, {"name": "Xi Chen", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 93.7, "max_score": 93.71, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning", "score": 93.71, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/96a88a23f63cd49accb6442bf71166bb7c2723de.pdf", "relevant": 0, "reads": 0}], "affiliation": "The University of Hong Kong", "role": "PhD Student", "photo_url": null, "profile_url": "https://xavierchen34.github.io/"}, {"name": "Mingkang Zhu", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 93.7, "max_score": 93.71, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning", "score": 93.71, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/96a88a23f63cd49accb6442bf71166bb7c2723de.pdf", "relevant": 0, "reads": 0}], "affiliation": "The Chinese University of Hong Kong", "role": "PhD Student", "photo_url": "https://static.wixstatic.com/media/5777a3_e8d050cc2a624e74b9b6e3d70cc57901~mv2.png/v1/crop/x_0%2Cy_33%2Cw_600%2Ch_735/fill/w_165%2Ch_202%2Cal_c%2Cq_85%2Cusm_0.66_1.00_0.01%2Cenc_avif%2Cquality_auto/mingkang_zhu.png", "profile_url": "https://mingkangz.github.io/logosticker/"}, {"name": "Hengshuang Zhao", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 93.7, "max_score": 93.71, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning", "score": 93.71, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/96a88a23f63cd49accb6442bf71166bb7c2723de.pdf", "relevant": 0, "reads": 0}], "affiliation": "The University of Hong Kong", "role": "Assistant Professor", "photo_url": "https://www.ai.hku.hk/images/Staff/hszhao03_full.jpg", "profile_url": "https://hszhao.github.io/"}, {"name": "Candace Ross", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 93.6, "max_score": 93.57, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "What‚Äôs in Common? Multimodal Models Hallucinate When Reasoning Across Scenes", "score": 93.57, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2511.03768", "relevant": 0, "reads": 0}], "affiliation": "Meta", "role": "Research Scientist", "photo_url": null, "profile_url": "https://candaceross.io/"}, {"name": "Florian Bordes", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 93.6, "max_score": 93.57, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "What‚Äôs in Common? Multimodal Models Hallucinate When Reasoning Across Scenes", "score": 93.57, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2511.03768", "relevant": 0, "reads": 0}], "affiliation": "Meta", "role": "Researcher", "photo_url": null, "profile_url": "https://mila.quebec/en/directory/florian-bordes"}, {"name": "Mark Ibrahim", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 93.6, "max_score": 93.57, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "What‚Äôs in Common? Multimodal Models Hallucinate When Reasoning Across Scenes", "score": 93.57, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2511.03768", "relevant": 0, "reads": 0}], "affiliation": "Meta", "role": "Researcher", "photo_url": "https://huggingface.co/avatars/b39c5637e00291410620afb7c770d587.svg", "profile_url": "https://markibrahim.me/"}, {"name": "Tianle Li", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 93.4, "max_score": 93.42, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model", "score": 93.42, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/64ee56c0512b904e0aebcd799494fb96e7bdedba.pdf", "relevant": 0, "reads": 0}], "affiliation": "xAI", "role": "Member of Technical Staff", "photo_url": null, "profile_url": "https://codingwithtim.github.io/"}, {"name": "Jihai Zhang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 93.4, "max_score": 93.42, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model", "score": 93.42, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/64ee56c0512b904e0aebcd799494fb96e7bdedba.pdf", "relevant": 0, "reads": 0}], "affiliation": "The Chinese University of Hong Kong", "role": "PhD Student", "photo_url": "https://avatars.githubusercontent.com/u/48614839?v=4", "profile_url": "https://majordavidzhang.github.io/"}, {"name": "Yu Cheng", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 93.4, "max_score": 93.42, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model", "score": 93.42, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/64ee56c0512b904e0aebcd799494fb96e7bdedba.pdf", "relevant": 0, "reads": 0}], "affiliation": "The Chinese University of Hong Kong", "role": "Associate Professor", "photo_url": "https://ych133.github.io/images/profile0.png", "profile_url": "https://ych133.github.io/"}, {"name": "Chi-Pin Huang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.9, "max_score": 92.93, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning", "score": 92.93, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/b35b0fc70612e191baced400f754db8ff1fae711.pdf", "relevant": 0, "reads": 0}], "affiliation": "National Taiwan University", "role": "PhD Student", "photo_url": null, "profile_url": "https://jasper0314-huang.github.io/"}, {"name": "Yueh-Hua Wu", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.9, "max_score": 92.93, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning", "score": 92.93, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/b35b0fc70612e191baced400f754db8ff1fae711.pdf", "relevant": 0, "reads": 0}], "affiliation": "NVIDIA", "role": "Research Scientist", "photo_url": "https://kristery.github.io/photos/new_selfie.jpg", "profile_url": "https://kristery.github.io/"}, {"name": "Fu-En Yang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.9, "max_score": 92.93, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning", "score": 92.93, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/b35b0fc70612e191baced400f754db8ff1fae711.pdf", "relevant": 0, "reads": 0}], "affiliation": "NVIDIA Research", "role": "Research Scientist", "photo_url": null, "profile_url": "https://fuenyang1127.github.io/"}, {"name": "Huajie Tan", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.8, "max_score": 92.8, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning of Vision Language Models", "score": 92.8, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/65dc0346cca4c6d3ceb0f9fcbf6938b5bb15a527.pdf", "relevant": 0, "reads": 0}], "affiliation": "Peking University", "role": "Master's Student", "photo_url": "https://pku-hmi-lab.github.io/HMI-Web/people/huajie.jpg", "profile_url": "https://github.com/tanhuajie"}, {"name": "Yuheng Ji", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.8, "max_score": 92.8, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning of Vision Language Models", "score": 92.8, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/65dc0346cca4c6d3ceb0f9fcbf6938b5bb15a527.pdf", "relevant": 0, "reads": 0}], "affiliation": "Chinese Academy of Sciences", "role": "PhD Student", "photo_url": "https://avatars.githubusercontent.com/u/66873890?v=4", "profile_url": "https://yuheng2000.github.io/"}, {"name": "Shanghang Zhang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.8, "max_score": 92.8, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning of Vision Language Models", "score": 92.8, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/65dc0346cca4c6d3ceb0f9fcbf6938b5bb15a527.pdf", "relevant": 0, "reads": 0}], "affiliation": "Peking University", "role": "Assistant Professor", "photo_url": "https://static.wixstatic.com/media/4ce291_d6f37238efe947779e8b471e881d89af~mv2.jpeg/v1/fill/w_450%2Ch_559%2Cal_c%2Cq_80%2Cusm_0.66_1.00_0.01%2Cenc_avif%2Cquality_auto/WechatIMG7593.jpeg", "profile_url": "https://www.shanghangzhang.com/"}, {"name": "Zhe Xu", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.4, "max_score": 92.44, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?", "score": 92.44, "session": "San Diego Poster Session 6", "pdf_url": "https://arxiv.org/pdf/2503.09499", "relevant": 0, "reads": 0}], "affiliation": "Sun Yat-sen University", "role": "PhD Student", "photo_url": null, "profile_url": "https://openreview.net/profile?id=~Zhe_Xu15"}, {"name": "Daoyuan Chen", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.4, "max_score": 92.44, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?", "score": 92.44, "session": "San Diego Poster Session 6", "pdf_url": "https://arxiv.org/pdf/2503.09499", "relevant": 0, "reads": 0}], "affiliation": "Alibaba Group", "role": "Researcher", "photo_url": "https://avatars.githubusercontent.com/u/67475544?v=4", "profile_url": "https://yxdyc.github.io/"}, {"name": "Ying Shen", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 92.4, "max_score": 92.44, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MindGYM: What Matters in Question Synthesis for Thinking-Centric Fine-Tuning?", "score": 92.44, "session": "San Diego Poster Session 6", "pdf_url": "https://arxiv.org/pdf/2503.09499", "relevant": 0, "reads": 0}], "affiliation": "Sun Yat-Sen University", "role": "Associate Professor", "photo_url": "https://www.sysu-hcp.net/userfiles/images/2024/09/02/36ea9878d52a306f.png", "profile_url": "https://ise.sysu.edu.cn/teacher/teacher02/1371452.htm"}, {"name": "Meng-Hao Guo", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 91.5, "max_score": 91.55, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "RBench-V: A Primary Assessment for Visual Reasoning Models with Multimodal Outputs", "score": 91.55, "session": "San Diego Poster Session 4", "pdf_url": "https://arxiv.org/pdf/2505.16770", "relevant": 0, "reads": 0}], "affiliation": "Tsinghua University", "role": "Postdoctoral Researcher", "photo_url": "https://avatars.githubusercontent.com/u/38318671?v=4", "profile_url": "https://menghaoguo.github.io/"}, {"name": "Xuanyu Chu", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 91.5, "max_score": 91.55, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "RBench-V: A Primary Assessment for Visual Reasoning Models with Multimodal Outputs", "score": 91.55, "session": "San Diego Poster Session 4", "pdf_url": "https://arxiv.org/pdf/2505.16770", "relevant": 0, "reads": 0}], "affiliation": "Tsinghua University", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shi-min Hu", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 91.5, "max_score": 91.55, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "RBench-V: A Primary Assessment for Visual Reasoning Models with Multimodal Outputs", "score": 91.55, "session": "San Diego Poster Session 4", "pdf_url": "https://arxiv.org/pdf/2505.16770", "relevant": 0, "reads": 0}], "affiliation": "Tsinghua University", "role": "Professor", "photo_url": null, "profile_url": "http://cg.cs.tsinghua.edu.cn/~shimin/"}, {"name": "Jinguo Luo", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 91.3, "max_score": 91.35, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "InstructHOI: Context-Aware Instruction for Multi-Modal Reasoning in Human-Object Interaction Detection", "score": 91.35, "session": "Mexico City Poster Session 3", "pdf_url": "https://openreview.net/pdf/9256d13b1c0d87dee1265c615f5a083bea101d9c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Harbin Institute of Technology", "role": "MS Student", "photo_url": null, "profile_url": "https://openreview.net/profile?id=~Jinguo_Luo1"}, {"name": "Weihong Ren", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 91.3, "max_score": 91.35, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "InstructHOI: Context-Aware Instruction for Multi-Modal Reasoning in Human-Object Interaction Detection", "score": 91.35, "session": "Mexico City Poster Session 3", "pdf_url": "https://openreview.net/pdf/9256d13b1c0d87dee1265c615f5a083bea101d9c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Harbin Institute of Technology, Shenzhen", "role": "Assistant Professor", "photo_url": null, "profile_url": "https://scholar.google.com/citations?user=yu56w28AAAAJ"}, {"name": "Honghai LIU", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 91.3, "max_score": 91.35, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "InstructHOI: Context-Aware Instruction for Multi-Modal Reasoning in Human-Object Interaction Detection", "score": 91.35, "session": "Mexico City Poster Session 3", "pdf_url": "https://openreview.net/pdf/9256d13b1c0d87dee1265c615f5a083bea101d9c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Harbin Institute of Technology, Shenzhen", "role": "Professor", "photo_url": null, "profile_url": "https://faculty.hitsz.edu.cn/liuhonghai"}, {"name": "Alex Su", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 91.3, "max_score": 91.35, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Pixel Reasoner: Incentivizing Pixel Space Reasoning via Curiosity-Driven Reinforcement Learning", "score": 91.35, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/1cff63c2ec284033d02664f1b348bc05165216ea.pdf", "relevant": 0, "reads": 0}], "affiliation": "University of Science and Technology of China", "role": "Undergraduate Student", "photo_url": null, "profile_url": "https://openreview.net/profile?id=~Alex_Su1"}, {"name": "Zihui Cheng", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 90.2, "max_score": 90.24, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought", "score": 90.24, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/0822568f152da6a15e1119aef56e6106b5054824.pdf", "relevant": 0, "reads": 0}], "affiliation": "Central South University", "role": "MS Student", "photo_url": "https://cdn-avatars.huggingface.co/v1/production/uploads/663ede8d8732bcd07634783b/QHBu9WJY_oMlB1go6xkUm.jpeg", "profile_url": "https://huggingface.co/czh-up"}, {"name": "Qiguang Chen", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 90.2, "max_score": 90.24, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought", "score": 90.24, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/0822568f152da6a15e1119aef56e6106b5054824.pdf", "relevant": 0, "reads": 0}], "affiliation": "Harbin Institute of Technology", "role": "PhD Student", "photo_url": "https://lightchen233.github.io/images/android-chrome-512x512.png", "profile_url": "https://lightchen233.github.io/"}, {"name": "Libo Qin", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 90.2, "max_score": 90.24, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought", "score": 90.24, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/0822568f152da6a15e1119aef56e6106b5054824.pdf", "relevant": 0, "reads": 0}], "affiliation": "Central South University", "role": "Professor", "photo_url": null, "profile_url": "https://faculty.csu.edu.cn/qinlibo/en/index.htm"}, {"name": "Philip Schroeder", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 88.9, "max_score": 88.88, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks", "score": 88.88, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/194ef43930192ab08c960e21945eb03ce919797a.pdf", "relevant": 0, "reads": 0}], "affiliation": "MIT", "role": "PhD Student", "photo_url": null, "profile_url": "https://sls.csail.mit.edu/archives/root/people/PhilipSchroeder.shtml"}, {"name": "Ondrej Biza", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 88.9, "max_score": 88.88, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks", "score": 88.88, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/194ef43930192ab08c960e21945eb03ce919797a.pdf", "relevant": 0, "reads": 0}], "affiliation": "Boston Dynamics AI Institute", "role": "Researcher", "photo_url": "https://ondrejbiza.com/static/images/prof_pic.png", "profile_url": "https://ondrejbiza.com/"}, {"name": "James R. Glass", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 88.9, "max_score": 88.88, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks", "score": 88.88, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/194ef43930192ab08c960e21945eb03ce919797a.pdf", "relevant": 0, "reads": 0}], "affiliation": "Massachusetts Institute of Technology", "role": "Senior Research Scientist", "photo_url": null, "profile_url": "https://people.csail.mit.edu/jrg/"}, {"name": "Amirmohammad Izadi", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 88.5, "max_score": 88.49, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Visual Structures Help Visual Reasoning:  Addressing the Binding Problem in LVLMs", "score": 88.49, "session": "Mexico City Poster Session 2", "pdf_url": "https://openreview.net/pdf/9aa6856102d8ec332673e3b3497a2c653885bfee.pdf", "relevant": 0, "reads": 0}], "affiliation": "Sharif University of Technology", "role": "Unknown", "photo_url": null, "profile_url": "https://sharif-ml-lab.github.io/VISER/"}, {"name": "Mohammadali Banayeeanzade", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 88.5, "max_score": 88.49, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Visual Structures Help Visual Reasoning:  Addressing the Binding Problem in LVLMs", "score": 88.49, "session": "Mexico City Poster Session 2", "pdf_url": "https://openreview.net/pdf/9aa6856102d8ec332673e3b3497a2c653885bfee.pdf", "relevant": 0, "reads": 0}], "affiliation": "Sharif University of Technology", "role": "Undergraduate Student", "photo_url": null, "profile_url": "https://alibanayeean.github.io/"}, {"name": "Mahdieh Soleymani Baghshah", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 88.5, "max_score": 88.49, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Visual Structures Help Visual Reasoning:  Addressing the Binding Problem in LVLMs", "score": 88.49, "session": "Mexico City Poster Session 2", "pdf_url": "https://openreview.net/pdf/9aa6856102d8ec332673e3b3497a2c653885bfee.pdf", "relevant": 0, "reads": 0}], "affiliation": "Sharif University of Technology", "role": "Associate Professor", "photo_url": null, "profile_url": "https://dblp.org/pid/21/473.html"}, {"name": "Yue Fan", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 87.6, "max_score": 87.58, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GRIT: Teaching MLLMs to Think with Images", "score": 87.58, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/87fbb0dd229346697981e700a4a0dd02546ee154.pdf", "relevant": 0, "reads": 0}], "affiliation": "University of California, Santa Cruz", "role": "PhD Student", "photo_url": "https://adoberesearch.ctlprojects.com/wp-content/uploads/2025/08/Yue-Fan-4-3.png", "profile_url": "https://yfan.site/"}, {"name": "Xuehai He", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 87.6, "max_score": 87.58, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GRIT: Teaching MLLMs to Think with Images", "score": 87.58, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/87fbb0dd229346697981e700a4a0dd02546ee154.pdf", "relevant": 0, "reads": 0}], "affiliation": "Microsoft", "role": "Researcher", "photo_url": "https://sheehan1230.github.io/images/profile.png", "profile_url": "https://sheehan1230.github.io/"}, {"name": "Xin Eric Wang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 87.6, "max_score": 87.58, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GRIT: Teaching MLLMs to Think with Images", "score": 87.58, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/87fbb0dd229346697981e700a4a0dd02546ee154.pdf", "relevant": 0, "reads": 0}], "affiliation": "UC Santa Barbara", "role": "Assistant Professor", "photo_url": "https://engineering.ucsb.edu/sites/default/files/styles/news_left/public/images/news/Wang_Eric_CS.png?itok=6vfoZVz1", "profile_url": "https://eric-xw.github.io/"}, {"name": "Gabriel Herbert Sarch", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 87.5, "max_score": 87.51, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Grounded Reinforcement Learning for Visual Reasoning", "score": 87.51, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/51495ccee37191c401ab184d34f4e5dc92f3ef58.pdf", "relevant": 0, "reads": 0}], "affiliation": "Princeton University", "role": "Postdoc", "photo_url": "https://gabesarch.me/images/main_photo.jpg", "profile_url": "https://gabesarch.me/"}, {"name": "Snigdha Saha", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 87.5, "max_score": 87.51, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Grounded Reinforcement Learning for Visual Reasoning", "score": 87.51, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/51495ccee37191c401ab184d34f4e5dc92f3ef58.pdf", "relevant": 0, "reads": 0}], "affiliation": "Carnegie Mellon University", "role": "Master's Student", "photo_url": "https://images.squarespace-cdn.com/content/v1/586ec16bb3db2b558ebfec60/bf51e952-827e-43df-84f5-a060fff83f3c/Snigdha-SoCaltech-Final.jpg", "profile_url": "https://magazine.caltech.edu/post/socaltech-meet-the-class-of-2024"}, {"name": "Katerina Fragkiadaki", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 87.5, "max_score": 87.51, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Grounded Reinforcement Learning for Visual Reasoning", "score": 87.51, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/51495ccee37191c401ab184d34f4e5dc92f3ef58.pdf", "relevant": 0, "reads": 0}], "affiliation": "Carnegie Mellon University", "role": "Associate Professor", "photo_url": "https://www.cs.cmu.edu/~katef/images/KaterinaFragkiadaki.png", "profile_url": "https://www.cs.cmu.edu/~katef/"}, {"name": "Anurag Arnab", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 87.4, "max_score": 87.42, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames", "score": 87.42, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/ecef04e6004300bdfe10dc8ca9560dcffe5ecf5d.pdf", "relevant": 0, "reads": 0}], "affiliation": "Google DeepMind", "role": "Research Scientist", "photo_url": "https://anuragarnab.github.io/assets/anurag.jpeg", "profile_url": "https://anuragarnab.github.io"}, {"name": "Ahmet Iscen", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 87.4, "max_score": 87.42, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames", "score": 87.42, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/ecef04e6004300bdfe10dc8ca9560dcffe5ecf5d.pdf", "relevant": 0, "reads": 0}], "affiliation": "Google", "role": "Staff Research Scientist", "photo_url": null, "profile_url": "https://cmp.felk.cvut.cz/~iscenahm/"}, {"name": "Cordelia Schmid", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 87.4, "max_score": 87.42, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames", "score": 87.42, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/ecef04e6004300bdfe10dc8ca9560dcffe5ecf5d.pdf", "relevant": 0, "reads": 0}], "affiliation": "INRIA", "role": "Research Director", "photo_url": "https://cordeliaschmid.github.io/images/profil_cd.jpg", "profile_url": "https://cordeliaschmid.github.io/"}, {"name": "Honglin Lin", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.5, "max_score": 86.53, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning", "score": 86.53, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/b5f91855f5922ea89b810ed1560821c78f774fff.pdf", "relevant": 0, "reads": 0}], "affiliation": "Beijing University of Posts and Telecommunications", "role": "Undergraduate Student", "photo_url": "https://lhl3341.github.io/images/honglin_avatar.jpg", "profile_url": "https://lhl3341.github.io/"}, {"name": "Qizhi Pei", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.5, "max_score": 86.53, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning", "score": 86.53, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/b5f91855f5922ea89b810ed1560821c78f774fff.pdf", "relevant": 0, "reads": 0}], "affiliation": "Renmin University of China", "role": "PhD Student", "photo_url": "https://qizhipei.github.io/images/profile_pqz.jpeg", "profile_url": "https://qizhipei.github.io/"}, {"name": "Lijun Wu", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.5, "max_score": 86.53, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning", "score": 86.53, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/b5f91855f5922ea89b810ed1560821c78f774fff.pdf", "relevant": 0, "reads": 0}], "affiliation": "Shanghai Artificial Intelligence Laboratory", "role": "Research Scientist", "photo_url": "https://apeterswu.github.io/images/head_2022_11_circle.png", "profile_url": "https://apeterswu.github.io/"}, {"name": "Rongyao Fang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.4, "max_score": 86.36, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GoT: Unleashing Reasoning Capability of MLLM for Visual Generation and Editing", "score": 86.36, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/88332ed6ca1dea81759b8d07d12b66ab6da63abf.pdf", "relevant": 0, "reads": 0}], "affiliation": "The Chinese University of Hong Kong", "role": "PhD Student", "photo_url": "https://rongyaofang.github.io/images/avatar.jpg", "profile_url": "https://rongyaofang.github.io/"}, {"name": "Chengqi Duan", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.4, "max_score": 86.36, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GoT: Unleashing Reasoning Capability of MLLM for Visual Generation and Editing", "score": 86.36, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/88332ed6ca1dea81759b8d07d12b66ab6da63abf.pdf", "relevant": 0, "reads": 0}], "affiliation": "The University of Hong Kong", "role": "PhD Student", "photo_url": "https://datascience.hku.hk/wp-content/uploads/2024/11/photo-rpg-stuent-chengqi-duan-150x150.jpg", "profile_url": "https://github.com/gogoduan"}, {"name": "Xinyu Zhang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.2, "max_score": 86.17, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CoFFT: Chain of Foresight-Focus Thought for Visual Language Models", "score": 86.17, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/cde471b7df444cf85c986002216423b046c56f8d.pdf", "relevant": 0, "reads": 0}], "affiliation": "Xi'an Jiaotong University", "role": "PhD Student", "photo_url": "https://avatars.githubusercontent.com/u/42887064?v=4", "profile_url": "https://dxzxy12138.github.io/PhysReason/"}, {"name": "Yuxuan Dong", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.2, "max_score": 86.17, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CoFFT: Chain of Foresight-Focus Thought for Visual Language Models", "score": 86.17, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/cde471b7df444cf85c986002216423b046c56f8d.pdf", "relevant": 0, "reads": 0}], "affiliation": "Xi'an Jiaotong University", "role": "Unknown", "photo_url": null, "profile_url": "https://aclanthology.org/people/yuxuan-dong/"}, {"name": "Yana Wei", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.2, "max_score": 86.23, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning", "score": 86.23, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/aa64c75f269da233428bef98108f402866b88aff.pdf", "relevant": 0, "reads": 0}], "affiliation": "Johns Hopkins University", "role": "PhD Student", "photo_url": "https://weiyana.github.io/asset/image/me.JPG", "profile_url": "https://weiyana.github.io/"}, {"name": "Liang Zhao", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.2, "max_score": 86.23, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning", "score": 86.23, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/aa64c75f269da233428bef98108f402866b88aff.pdf", "relevant": 0, "reads": 0}], "affiliation": "StepFun", "role": "Researcher", "photo_url": null, "profile_url": "https://scholar.google.com/citations?hl=en&user=uJJ5zskAAAAJ"}, {"name": "Vishal M. Patel", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.2, "max_score": 86.23, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning", "score": 86.23, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/aa64c75f269da233428bef98108f402866b88aff.pdf", "relevant": 0, "reads": 0}], "affiliation": "Johns Hopkins University", "role": "Associate Professor", "photo_url": "https://engineering.jhu.edu/vpatel36/wp-content/uploads/2017/07/vishal-1-e1548137289434.jpg", "profile_url": "https://engineering.jhu.edu/vpatel36/vishal-patel/"}, {"name": "Tianyi Bai", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.1, "max_score": 86.14, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Multi-step Visual Reasoning with Visual Tokens Scaling and Verification", "score": 86.14, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/a48656c1e2d8fb5cee742d21b8b328458f483707.pdf", "relevant": 0, "reads": 0}], "affiliation": "Hong Kong University of Science and Technology", "role": "PhD Student", "photo_url": "https://beccabai.github.io/images/photo3.jpg", "profile_url": "https://beccabai.github.io/"}, {"name": "Zengjie Hu", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.1, "max_score": 86.14, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Multi-step Visual Reasoning with Visual Tokens Scaling and Verification", "score": 86.14, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/a48656c1e2d8fb5cee742d21b8b328458f483707.pdf", "relevant": 0, "reads": 0}], "affiliation": "Peking University", "role": "Master's Student", "photo_url": "https://flosophorae.github.io//images/huzengjie_4.jpg", "profile_url": "https://flosophorae.github.io/"}, {"name": "Wentao Zhang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 86.1, "max_score": 86.14, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Multi-step Visual Reasoning with Visual Tokens Scaling and Verification", "score": 86.14, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/a48656c1e2d8fb5cee742d21b8b328458f483707.pdf", "relevant": 0, "reads": 0}], "affiliation": "Peking University", "role": "Assistant Professor", "photo_url": "https://cs.pku.edu.cn/virtual_attach_file.vsb?afc=xnN-J4U4l4UmMVM0N-ZUzn7nmfVoRCa4nznfozGYUmGYLRv0gihFp2hmCIa0U1ybokhRLky8UmNDLRUiM4LZLl-sL4NZo7QVMz7aUzNsUz6FLz7boR-DM4MFMznkoSb%2Fv2veo4OeqjM%2F_dTZgDTJQty0Lz-DLYyYoR9ZgtA8pUPcc&e=.jpg&nid=2888&oid=1934453449&tid=1263", "profile_url": "https://zwt233.github.io/"}, {"name": "Silin Cheng", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.9, "max_score": 85.89, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VaMP: Variational Multi-Modal Prompt Learning for Vision-Language Models", "score": 85.89, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/3ec902e63d7caa0a34c58304a519e30a791073d1.pdf", "relevant": 0, "reads": 0}], "affiliation": "The University of Hong Kong", "role": "PhD Student", "photo_url": "https://saasresearch.hku.hk/share/student/pgs2022/chengsilin.jpg", "profile_url": "https://visailab.github.io/people.html"}, {"name": "Kai Han", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.9, "max_score": 85.89, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VaMP: Variational Multi-Modal Prompt Learning for Vision-Language Models", "score": 85.89, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/3ec902e63d7caa0a34c58304a519e30a791073d1.pdf", "relevant": 0, "reads": 0}], "affiliation": "The University of Hong Kong", "role": "Assistant Professor", "photo_url": "https://www.kaihan.org/images/kaihan.jpg", "profile_url": "https://www.kaihan.org/"}, {"name": "Yicheng Xiao", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.8, "max_score": 85.84, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO", "score": 85.84, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/89e0ca00b9f6b5998d606e3a0c50147368b03e0a.pdf", "relevant": 0, "reads": 0}], "affiliation": "Tsinghua University", "role": "Master Student", "photo_url": "https://easonxiao-888.github.io/images/yichengxiao.jpg", "profile_url": "https://easonxiao-888.github.io/"}, {"name": "Lin Song", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.8, "max_score": 85.84, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO", "score": 85.84, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/89e0ca00b9f6b5998d606e3a0c50147368b03e0a.pdf", "relevant": 0, "reads": 0}], "affiliation": "Tencent", "role": "Researcher", "photo_url": null, "profile_url": "https://linsong.info/"}, {"name": "Ying Shan", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.8, "max_score": 85.84, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO", "score": 85.84, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/89e0ca00b9f6b5998d606e3a0c50147368b03e0a.pdf", "relevant": 0, "reads": 0}], "affiliation": "Tencent", "role": "Distinguished Scientist", "photo_url": null, "profile_url": "https://scholar.google.com/citations?user=4oXBp9UAAAAJ"}, {"name": "Wenyi WU", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.3, "max_score": 85.28, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards General Continuous Memory for Vision-Language Models", "score": 85.28, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/eb3b13d0293bfe8c1b60b497e2634c96c134f57c.pdf", "relevant": 0, "reads": 0}], "affiliation": "University of California, San Diego", "role": "Master Student", "photo_url": "https://wenyiwu0111.github.io/images/PastedGraphic11.png", "profile_url": "https://wenyiwu0111.github.io/"}, {"name": "Zixuan Song", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.3, "max_score": 85.28, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards General Continuous Memory for Vision-Language Models", "score": 85.28, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/eb3b13d0293bfe8c1b60b497e2634c96c134f57c.pdf", "relevant": 0, "reads": 0}], "affiliation": "University of California San Diego", "role": "MS Student", "photo_url": null, "profile_url": "https://openreview.net/profile?id=~Zixuan_Song1"}, {"name": "Jusheng Zhang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.1, "max_score": 85.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CF-VLMÔºöCounterFactual Vision-Language Fine-tuning", "score": 85.12, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/7e5f14766c2d8126be08ddbafcf8266e3b134a68.pdf", "relevant": 0, "reads": 0}], "affiliation": "Sun Yat-sen University", "role": "PhD Student", "photo_url": null, "profile_url": "https://openreview.net/profile?id=~Jusheng_Zhang3"}, {"name": "Kaitong Cai", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.1, "max_score": 85.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CF-VLMÔºöCounterFactual Vision-Language Fine-tuning", "score": 85.12, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/7e5f14766c2d8126be08ddbafcf8266e3b134a68.pdf", "relevant": 0, "reads": 0}], "affiliation": "Sun Yat-sen University", "role": "Undergraduate Student", "photo_url": "https://avatars.githubusercontent.com/u/196105464?v=4", "profile_url": "https://github.com/CaiKaitong"}, {"name": "Keze Wang", "paper_count": 1, "highly_relevant_count": 1, "avg_score": 85.1, "max_score": 85.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CF-VLMÔºöCounterFactual Vision-Language Fine-tuning", "score": 85.12, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/7e5f14766c2d8126be08ddbafcf8266e3b134a68.pdf", "relevant": 0, "reads": 0}], "affiliation": "Sun Yat-sen University", "role": "Associate Professor", "photo_url": "https://kezewang.com/image/avatar.png", "profile_url": "https://kezewang.com/"}, {"name": "Zhengyuan Yang", "paper_count": 2, "highly_relevant_count": 1, "avg_score": 82.3, "max_score": 88.28, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Point-RFT: Improving Multimodal Reasoning with Visually Grounded Reinforcement Finetuning", "score": 88.28, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/5084e959f05b99951639c06f88dbb5747c300a40.pdf", "relevant": 0, "reads": 0}, {"title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement", "score": 76.24, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/758205547dc940c338296a99ef384666df7ca2ac.pdf", "relevant": 0, "reads": 0}], "affiliation": "Microsoft", "role": "Principal Researcher", "photo_url": "https://www.cs.rochester.edu/people/graduate/assets/images/yang_zhengyuan.jpg", "profile_url": "https://zyang-ur.github.io/"}, {"name": "Lijuan Wang", "paper_count": 2, "highly_relevant_count": 1, "avg_score": 82.3, "max_score": 88.28, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Point-RFT: Improving Multimodal Reasoning with Visually Grounded Reinforcement Finetuning", "score": 88.28, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/5084e959f05b99951639c06f88dbb5747c300a40.pdf", "relevant": 0, "reads": 0}, {"title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement", "score": 76.24, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/758205547dc940c338296a99ef384666df7ca2ac.pdf", "relevant": 0, "reads": 0}], "affiliation": "Microsoft Research", "role": "Principal Research Manager", "photo_url": "https://www.microsoft.com/en-us/research/wp-content/uploads/2020/06/Me.jpg", "profile_url": "https://www.microsoft.com/en-us/research/people/lijuanw/"}, {"name": "Hongsheng Li", "paper_count": 3, "highly_relevant_count": 1, "avg_score": 82.1, "max_score": 86.36, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GoT: Unleashing Reasoning Capability of MLLM for Visual Generation and Editing", "score": 86.36, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/88332ed6ca1dea81759b8d07d12b66ab6da63abf.pdf", "relevant": 0, "reads": 0}, {"title": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning", "score": 81.04, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/82857fa25f2da29b5e82ec3b3093429dd593aab4.pdf", "relevant": 0, "reads": 0}, {"title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT", "score": 78.81, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/ffb79a572869428a5be310d74404eb5faac1291f.pdf", "relevant": 0, "reads": 0}], "affiliation": "The Chinese University of Hong Kong", "role": "Associate Professor", "photo_url": null, "profile_url": "https://www.ee.cuhk.edu.hk/~hsli/"}, {"name": "Mike Zheng Shou", "paper_count": 2, "highly_relevant_count": 1, "avg_score": 81.9, "max_score": 86.17, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CoFFT: Chain of Foresight-Focus Thought for Visual Language Models", "score": 86.17, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/cde471b7df444cf85c986002216423b046c56f8d.pdf", "relevant": 0, "reads": 0}, {"title": "Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models", "score": 77.66, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/62bc8f6585ecc381363cf32ea8c833c4ef08d00e.pdf", "relevant": 0, "reads": 0}], "affiliation": "National University of Singapore", "role": "Assistant Professor", "photo_url": "https://lh3.googleusercontent.com/sitesv/AAzXCkew9gq81fjCpgsw1HuNpxOTORmq5Dwl4naVw0mTHA-lxIArUjFLdF69qDcBfHBK3SqENrJBAWryGrgxZTJ0Wfmr40iftyCKI0fB-fSMB_BOJzwHV-WvvGoBgcqcLhId66kGfUZo3xWy9mvcxcupubE5rHwQKFLBd5WZ4MatFen4vku-v5ohk9PuXFiTcDZxvnkDriRHaxRV6pOSIxIWceKIbT57ZChRVKHR=w1280", "profile_url": "https://sites.google.com/view/showlab/home"}, {"name": "Minheng Ni", "paper_count": 2, "highly_relevant_count": 1, "avg_score": 80.6, "max_score": 88.28, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Point-RFT: Improving Multimodal Reasoning with Visually Grounded Reinforcement Finetuning", "score": 88.28, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/5084e959f05b99951639c06f88dbb5747c300a40.pdf", "relevant": 0, "reads": 0}, {"title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM", "score": 72.89, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/5c8396fbd51956694870ead9c44afcff4e5253cb.pdf", "relevant": 0, "reads": 0}], "affiliation": "The Hong Kong Polytechnic University", "role": "PhD Student", "photo_url": "https://github.com/kodenii.png", "profile_url": "https://kodenii.github.io/"}, {"name": "Haozhe Wang", "paper_count": 2, "highly_relevant_count": 1, "avg_score": 76.9, "max_score": 91.35, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Pixel Reasoner: Incentivizing Pixel Space Reasoning via Curiosity-Driven Reinforcement Learning", "score": 91.35, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/1cff63c2ec284033d02664f1b348bc05165216ea.pdf", "relevant": 0, "reads": 0}, {"title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning", "score": 62.37, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/4caa878eb3dbd25718a328d937d8d6cc4e0d3100.pdf", "relevant": 0, "reads": 0}], "affiliation": "Hong Kong University of Science and Technology", "role": "PhD Student", "photo_url": null, "profile_url": "https://haozheh3.github.io/"}, {"name": "Wenhu Chen", "paper_count": 2, "highly_relevant_count": 1, "avg_score": 76.9, "max_score": 91.35, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Pixel Reasoner: Incentivizing Pixel Space Reasoning via Curiosity-Driven Reinforcement Learning", "score": 91.35, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/1cff63c2ec284033d02664f1b348bc05165216ea.pdf", "relevant": 0, "reads": 0}, {"title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning", "score": 62.37, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/4caa878eb3dbd25718a328d937d8d6cc4e0d3100.pdf", "relevant": 0, "reads": 0}], "affiliation": "University of Waterloo", "role": "Assistant Professor", "photo_url": "https://wenhuchen.github.io/images/headshot.jpg", "profile_url": "https://wenhuchen.github.io/"}, {"name": "Biwei Huang", "paper_count": 2, "highly_relevant_count": 1, "avg_score": 69.8, "max_score": 85.28, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards General Continuous Memory for Vision-Language Models", "score": 85.28, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/eb3b13d0293bfe8c1b60b497e2634c96c134f57c.pdf", "relevant": 0, "reads": 0}, {"title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models", "score": 54.39, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/2b62626f959a636707dfb61efe094e4dcd7bd32f.pdf", "relevant": 0, "reads": 0}], "affiliation": "UC San Diego", "role": "Assistant Professor", "photo_url": "https://cabalcmu.wordpress.com/wp-content/uploads/2017/01/huang_biewei_150x176.jpg?w=739", "profile_url": "https://biweihuang.com/"}, {"name": "Xiangning Yu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 84.1, "max_score": 84.07, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning", "score": 84.07, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/722d843c0b72fca7838e93dad027bbb061500956.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhuohan Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 84.1, "max_score": 84.07, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning", "score": 84.07, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/722d843c0b72fca7838e93dad027bbb061500956.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mengyue Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 84.1, "max_score": 84.07, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning", "score": 84.07, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/722d843c0b72fca7838e93dad027bbb061500956.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zihan Weng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.2, "max_score": 83.22, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Caption This, Reason That: VLMs Caught in the Middle", "score": 83.22, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/756a05a421ed180f3aedfcaa7a55a73f50e83d30.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lucas Gomez", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.2, "max_score": 83.22, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Caption This, Reason That: VLMs Caught in the Middle", "score": 83.22, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/756a05a421ed180f3aedfcaa7a55a73f50e83d30.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pouya Bashivan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.2, "max_score": 83.22, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Caption This, Reason That: VLMs Caught in the Middle", "score": 83.22, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/756a05a421ed180f3aedfcaa7a55a73f50e83d30.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Honghao Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.1, "max_score": 83.09, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards", "score": 83.09, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/b75967c26b83c8cc4921f37f32977c5e087b1f44.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xingzhou Lou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.1, "max_score": 83.09, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards", "score": 83.09, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/b75967c26b83c8cc4921f37f32977c5e087b1f44.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xinlong Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.1, "max_score": 83.09, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards", "score": 83.09, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/b75967c26b83c8cc4921f37f32977c5e087b1f44.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chanhyeong Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.0, "max_score": 83.02, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection", "score": 83.02, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/50b2801b8c0fac609e33847c31b795508b0bb09f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Taehoon song", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.0, "max_score": 83.02, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection", "score": 83.02, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/50b2801b8c0fac609e33847c31b795508b0bb09f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hyunwoo J. Kim", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 83.0, "max_score": 83.02, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection", "score": 83.02, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/50b2801b8c0fac609e33847c31b795508b0bb09f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junfei Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 82.8, "max_score": 82.84, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "score": 82.84, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/0da039d9be67a74526e2ce4408db4cde8bdca6f5.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jian Guan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 82.8, "max_score": 82.84, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "score": 82.84, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/0da039d9be67a74526e2ce4408db4cde8bdca6f5.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tieniu Tan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 82.8, "max_score": 82.84, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "score": 82.84, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/0da039d9be67a74526e2ce4408db4cde8bdca6f5.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shuo Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 82.1, "max_score": 82.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation", "score": 82.12, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/4eb2cec937108c8a5e1785cf53b814a0999b4eb2.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yongcai Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 82.1, "max_score": 82.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation", "score": 82.12, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/4eb2cec937108c8a5e1785cf53b814a0999b4eb2.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhaoxin Fan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 82.1, "max_score": 82.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation", "score": 82.12, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/4eb2cec937108c8a5e1785cf53b814a0999b4eb2.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yang Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.5, "max_score": 81.46, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning", "score": 81.46, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/25e4a0d5d0d7bb0f7006a8d400a662a0c31c22d7.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ming Ma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.5, "max_score": 81.46, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning", "score": 81.46, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/25e4a0d5d0d7bb0f7006a8d400a662a0c31c22d7.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Donglin Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.5, "max_score": 81.46, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning", "score": 81.46, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/25e4a0d5d0d7bb0f7006a8d400a662a0c31c22d7.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xinyi Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.4, "max_score": 81.44, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models", "score": 81.44, "session": "Mexico City Poster Session 6", "pdf_url": "https://openreview.net/pdf/686a233441037675c3bfb6cea63ba14355cad978.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xun Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.4, "max_score": 81.44, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models", "score": 81.44, "session": "Mexico City Poster Session 6", "pdf_url": "https://openreview.net/pdf/686a233441037675c3bfb6cea63ba14355cad978.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Na Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.4, "max_score": 81.44, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models", "score": 81.44, "session": "Mexico City Poster Session 6", "pdf_url": "https://openreview.net/pdf/686a233441037675c3bfb6cea63ba14355cad978.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xinyan Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81.04, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning", "score": 81.04, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/82857fa25f2da29b5e82ec3b3093429dd593aab4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Renrui Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 81.0, "max_score": 81.04, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning", "score": 81.04, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/82857fa25f2da29b5e82ec3b3093429dd593aab4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pengteng Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.9, "max_score": 80.89, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model", "score": 80.89, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/bcef03fe8da8c459269e00eeb6626d6584a0ba4c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pinhao Song", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.9, "max_score": 80.89, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model", "score": 80.89, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/bcef03fe8da8c459269e00eeb6626d6584a0ba4c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hui Xiong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.9, "max_score": 80.89, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model", "score": 80.89, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/bcef03fe8da8c459269e00eeb6626d6584a0ba4c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haoyu Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.8, "max_score": 80.77, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Spatial Understanding from Videos: Structured Prompts Meet Simulation Data", "score": 80.77, "session": "Mexico City Poster Session 1", "pdf_url": "https://openreview.net/pdf/ae7873ba9e6cdcfd29717a979c05e0533e2d1f47.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Meng Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.8, "max_score": 80.77, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Spatial Understanding from Videos: Structured Prompts Meet Simulation Data", "score": 80.77, "session": "Mexico City Poster Session 1", "pdf_url": "https://openreview.net/pdf/ae7873ba9e6cdcfd29717a979c05e0533e2d1f47.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jongwoo Ko", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.5, "max_score": 80.46, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Flex-Judge: Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators", "score": 80.46, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/0895cf6fc3c5ff656996f38666b8fb528c975fb4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sungnyun Kim", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.5, "max_score": 80.46, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Flex-Judge: Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators", "score": 80.46, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/0895cf6fc3c5ff656996f38666b8fb528c975fb4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Se-Young Yun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 80.5, "max_score": 80.46, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Flex-Judge: Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators", "score": 80.46, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/0895cf6fc3c5ff656996f38666b8fb528c975fb4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ziang Yan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 79.5, "max_score": 79.52, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception", "score": 79.52, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/7cf4479fc77f583f1cdede042be97389be035f9a.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yinan He", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 79.5, "max_score": 79.52, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception", "score": 79.52, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/7cf4479fc77f583f1cdede042be97389be035f9a.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yi Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 79.5, "max_score": 79.52, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception", "score": 79.52, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/7cf4479fc77f583f1cdede042be97389be035f9a.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dongzhi Jiang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.8, "max_score": 78.81, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT", "score": 78.81, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/ffb79a572869428a5be310d74404eb5faac1291f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ziyu Guo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.8, "max_score": 78.81, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT", "score": 78.81, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/ffb79a572869428a5be310d74404eb5faac1291f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhongxing Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.5, "max_score": 78.54, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "score": 78.54, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/3843716274c768fa2caeeb1593f7865631cc0905.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chengzhi Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.5, "max_score": 78.54, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "score": 78.54, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/3843716274c768fa2caeeb1593f7865631cc0905.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sheng Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.5, "max_score": 78.54, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "score": 78.54, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/3843716274c768fa2caeeb1593f7865631cc0905.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yang Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.1, "max_score": 78.07, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation", "score": 78.07, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/3069d87176e110b08bf8a1a84618385fcd354a5b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pu Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.1, "max_score": 78.07, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation", "score": 78.07, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/3069d87176e110b08bf8a1a84618385fcd354a5b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hao Frank Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 78.1, "max_score": 78.07, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation", "score": 78.07, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/3069d87176e110b08bf8a1a84618385fcd354a5b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaqi WANG", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.7, "max_score": 77.66, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models", "score": 77.66, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/62bc8f6585ecc381363cf32ea8c833c4ef08d00e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kevin Qinghong Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.7, "max_score": 77.66, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models", "score": 77.66, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/62bc8f6585ecc381363cf32ea8c833c4ef08d00e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yibin Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.6, "max_score": 77.61, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning", "score": 77.61, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/e4dafb5f56d85a3ea3c98693dd6bf3a57ca6f9c7.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhimin Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.6, "max_score": 77.61, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning", "score": 77.61, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/e4dafb5f56d85a3ea3c98693dd6bf3a57ca6f9c7.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaqi Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.6, "max_score": 77.61, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning", "score": 77.61, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/e4dafb5f56d85a3ea3c98693dd6bf3a57ca6f9c7.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiangqi Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.5, "max_score": 77.46, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking", "score": 77.46, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/dd1128b101a8e529cfa219b76424da7861923e32.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yue Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.5, "max_score": 77.46, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking", "score": 77.46, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/dd1128b101a8e529cfa219b76424da7861923e32.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiangliang Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.5, "max_score": 77.46, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking", "score": 77.46, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/dd1128b101a8e529cfa219b76424da7861923e32.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Van Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.5, "max_score": 77.52, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning", "score": 77.52, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/3aa73c81186850fe236acaa4a81d2af3f1e5c910.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zirui Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.5, "max_score": 77.52, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning", "score": 77.52, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/3aa73c81186850fe236acaa4a81d2af3f1e5c910.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaotian Han", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.5, "max_score": 77.52, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning", "score": 77.52, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/3aa73c81186850fe236acaa4a81d2af3f1e5c910.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yizhen Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.5, "max_score": 77.51, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning", "score": 77.51, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/2a9e8b0109094071a532fedeb9aee41d18c08710.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yang Ding", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.5, "max_score": 77.51, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning", "score": 77.51, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/2a9e8b0109094071a532fedeb9aee41d18c08710.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yujiu Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.5, "max_score": 77.51, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning", "score": 77.51, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/2a9e8b0109094071a532fedeb9aee41d18c08710.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianchi Xie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.1, "max_score": 77.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts", "score": 77.12, "session": "San Diego Poster Session 4", "pdf_url": "https://arxiv.org/pdf/2505.19028", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Minzhi Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.1, "max_score": 77.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts", "score": 77.12, "session": "San Diego Poster Session 4", "pdf_url": "https://arxiv.org/pdf/2505.19028", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shixia Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 77.1, "max_score": 77.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts", "score": 77.12, "session": "San Diego Poster Session 4", "pdf_url": "https://arxiv.org/pdf/2505.19028", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Rim Assouel", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 76.3, "max_score": 76.28, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Object-centric binding in Contrastive Language-Image Pretraining", "score": 76.28, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/e080621881f559915bf7ebef0728a061e50884bf.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pietro Astolfi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 76.3, "max_score": 76.28, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Object-centric binding in Contrastive Language-Image Pretraining", "score": 76.28, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/e080621881f559915bf7ebef0728a061e50884bf.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Adriana Romero-Soriano", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 76.3, "max_score": 76.28, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Object-centric binding in Contrastive Language-Image Pretraining", "score": 76.28, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/e080621881f559915bf7ebef0728a061e50884bf.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiyao Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 76.2, "max_score": 76.24, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement", "score": 76.24, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/758205547dc940c338296a99ef384666df7ca2ac.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jihoon Kwon", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 76.1, "max_score": 76.06, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions", "score": 76.06, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/26f0105deb76b3529c3f5d5f168a3e97ff0d230f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kyle Min", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 76.1, "max_score": 76.06, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions", "score": 76.06, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/26f0105deb76b3529c3f5d5f168a3e97ff0d230f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jy-yong Sohn", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 76.1, "max_score": 76.06, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions", "score": 76.06, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/26f0105deb76b3529c3f5d5f168a3e97ff0d230f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wang Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 75.0, "max_score": 75.05, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Vinci: Deep Thinking in Text-to-Image Generation using Unified Model with Reinforcement Learning", "score": 75.05, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/f6be9731a06937642b60f557e3342249472839d1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wentao Hu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 75.0, "max_score": 75.05, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Vinci: Deep Thinking in Text-to-Image Generation using Unified Model with Reinforcement Learning", "score": 75.05, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/f6be9731a06937642b60f557e3342249472839d1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hanwang Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 75.0, "max_score": 75.05, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Vinci: Deep Thinking in Text-to-Image Generation using Unified Model with Reinforcement Learning", "score": 75.05, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/f6be9731a06937642b60f557e3342249472839d1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Guohao Sun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 75.0, "max_score": 74.95, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Latent Chain-of-Thought for Visual Reasoning", "score": 74.95, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/afce18f769b150f82c90b46e4582f2c422ebb699.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hang Hua", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 75.0, "max_score": 74.95, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Latent Chain-of-Thought for Visual Reasoning", "score": 74.95, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/afce18f769b150f82c90b46e4582f2c422ebb699.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhiqiang Tao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 75.0, "max_score": 74.95, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Latent Chain-of-Thought for Visual Reasoning", "score": 74.95, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/afce18f769b150f82c90b46e4582f2c422ebb699.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Weijie Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 74.8, "max_score": 74.84, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments", "score": 74.84, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/89b1018647a9f5681e3ad0ed184e80229e3997ee.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xuantang Xiong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 74.8, "max_score": 74.84, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments", "score": 74.84, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/89b1018647a9f5681e3ad0ed184e80229e3997ee.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jinqiao Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 74.8, "max_score": 74.84, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments", "score": 74.84, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/89b1018647a9f5681e3ad0ed184e80229e3997ee.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuyang Hong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 73.9, "max_score": 73.92, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering", "score": 73.92, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/fc97a2f58898074b2cbe0521459da7f6551f2dcb.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaqi Gu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 73.9, "max_score": 73.92, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering", "score": 73.92, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/fc97a2f58898074b2cbe0521459da7f6551f2dcb.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jieping Ye", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 73.9, "max_score": 73.92, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering", "score": 73.92, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/fc97a2f58898074b2cbe0521459da7f6551f2dcb.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiangyu Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 73.2, "max_score": 73.23, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing", "score": 73.23, "session": "San Diego Oral 2", "pdf_url": "https://arxiv.org/pdf/2504.02826", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Peiyuan Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 73.2, "max_score": 73.23, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing", "score": 73.23, "session": "San Diego Oral 2", "pdf_url": "https://arxiv.org/pdf/2504.02826", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haodong Duan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 73.2, "max_score": 73.23, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing", "score": 73.23, "session": "San Diego Oral 2", "pdf_url": "https://arxiv.org/pdf/2504.02826", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yeongtak Oh", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 73.1, "max_score": 73.06, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models", "score": 73.06, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/f251aa0193aee75b85b81e71d2046d1764951792.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dohyun Chung", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 73.1, "max_score": 73.06, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models", "score": 73.06, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/f251aa0193aee75b85b81e71d2046d1764951792.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sungroh Yoon", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 73.1, "max_score": 73.06, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models", "score": 73.06, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/f251aa0193aee75b85b81e71d2046d1764951792.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bowen Dong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.9, "max_score": 72.89, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM", "score": 72.89, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/5c8396fbd51956694870ead9c44afcff4e5253cb.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lei Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.9, "max_score": 72.89, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM", "score": 72.89, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/5c8396fbd51956694870ead9c44afcff4e5253cb.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kaituo Feng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.7, "max_score": 72.72, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Video-R1: Reinforcing Video Reasoning in MLLMs", "score": 72.72, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/6f889c461f4ed89171060b9ea269b24978cfed1f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kaixiong Gong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.7, "max_score": 72.72, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Video-R1: Reinforcing Video Reasoning in MLLMs", "score": 72.72, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/6f889c461f4ed89171060b9ea269b24978cfed1f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiangyu Yue", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.7, "max_score": 72.72, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Video-R1: Reinforcing Video Reasoning in MLLMs", "score": 72.72, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/6f889c461f4ed89171060b9ea269b24978cfed1f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yixiao Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.7, "max_score": 72.69, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers", "score": 72.69, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/ba09a69b9f3ea21dc345fef9e047ff5eb6a933f0.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hanlin Zhu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.7, "max_score": 72.69, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers", "score": 72.69, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/ba09a69b9f3ea21dc345fef9e047ff5eb6a933f0.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Song Mei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.7, "max_score": 72.69, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers", "score": 72.69, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/ba09a69b9f3ea21dc345fef9e047ff5eb6a933f0.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qi Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.6, "max_score": 72.57, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning", "score": 72.57, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/cfd06225250bc51c74d2efae79070dacdee1e780.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yanrui Yu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.6, "max_score": 72.57, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning", "score": 72.57, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/cfd06225250bc51c74d2efae79070dacdee1e780.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianfei Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.6, "max_score": 72.57, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning", "score": 72.57, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/cfd06225250bc51c74d2efae79070dacdee1e780.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mi Luo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.0, "max_score": 72.02, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "When Thinking Drifts: Evidential Grounding for Robust Video Reasoning", "score": 72.02, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/013556e3887ae4788bffd0dd1845278e3ce6ff47.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zihui Xue", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.0, "max_score": 72.02, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "When Thinking Drifts: Evidential Grounding for Robust Video Reasoning", "score": 72.02, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/013556e3887ae4788bffd0dd1845278e3ce6ff47.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kristen Grauman", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.0, "max_score": 72.02, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "When Thinking Drifts: Evidential Grounding for Robust Video Reasoning", "score": 72.02, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/013556e3887ae4788bffd0dd1845278e3ce6ff47.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tim Genewein", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.0, "max_score": 71.95, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Understanding Prompt Tuning and In-Context Learning via Meta-Learning", "score": 71.95, "session": "Mexico City Poster Session 1", "pdf_url": "https://openreview.net/pdf/e8c73fa77b9026d1f61c4b015c19f5c062b83022.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Li Kevin Wenliang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.0, "max_score": 71.95, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Understanding Prompt Tuning and In-Context Learning via Meta-Learning", "score": 71.95, "session": "Mexico City Poster Session 1", "pdf_url": "https://openreview.net/pdf/e8c73fa77b9026d1f61c4b015c19f5c062b83022.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Marcus Hutter", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 72.0, "max_score": 71.95, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Understanding Prompt Tuning and In-Context Learning via Meta-Learning", "score": 71.95, "session": "Mexico City Poster Session 1", "pdf_url": "https://openreview.net/pdf/e8c73fa77b9026d1f61c4b015c19f5c062b83022.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yinhan He", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 71.6, "max_score": 71.56, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens", "score": 71.56, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/39a70c107ed5819e3435a94da518615c9e3db25c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wendy Zheng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 71.6, "max_score": 71.56, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens", "score": 71.56, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/39a70c107ed5819e3435a94da518615c9e3db25c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jundong Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 71.6, "max_score": 71.56, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens", "score": 71.56, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/39a70c107ed5819e3435a94da518615c9e3db25c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yan Gong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.7, "max_score": 70.68, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "RelationAdapter: Learning and Transferring Visual Relation with Diffusion Transformers", "score": 70.68, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/2c9877508b7505c4c6e730f2b3f8e054edbe33ea.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yin Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.7, "max_score": 70.68, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "RelationAdapter: Learning and Transferring Visual Relation with Diffusion Transformers", "score": 70.68, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/2c9877508b7505c4c6e730f2b3f8e054edbe33ea.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yihe Deng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.7, "max_score": 70.66, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles", "score": 70.66, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/62ceb097c643e0416c764c187ebf4f4d6d1ba9c3.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hritik Bansal", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.7, "max_score": 70.66, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles", "score": 70.66, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/62ceb097c643e0416c764c187ebf4f4d6d1ba9c3.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kai-Wei Chang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.7, "max_score": 70.66, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles", "score": 70.66, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/62ceb097c643e0416c764c187ebf4f4d6d1ba9c3.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pei Peng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.3, "max_score": 70.29, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition", "score": 70.29, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/9cc6a292fd51139e30693138af29bfc25cd09a5c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ming-Kun Xie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.3, "max_score": 70.29, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition", "score": 70.29, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/9cc6a292fd51139e30693138af29bfc25cd09a5c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sheng-Jun Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 70.3, "max_score": 70.29, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition", "score": 70.29, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/9cc6a292fd51139e30693138af29bfc25cd09a5c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shmuel Berman", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.8, "max_score": 69.82, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs", "score": 69.82, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/ca79a743ac55c7fce5e100be3b2d695bf84a71ba.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jia Deng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.8, "max_score": 69.82, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs", "score": 69.82, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/ca79a743ac55c7fce5e100be3b2d695bf84a71ba.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Rongyang Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.5, "max_score": 69.52, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in Open-domain Question Answering", "score": 69.52, "session": "San Diego Poster Session 4", "pdf_url": "https://arxiv.org/pdf/2512.05119", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuqing Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.5, "max_score": 69.52, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in Open-domain Question Answering", "score": 69.52, "session": "San Diego Poster Session 4", "pdf_url": "https://arxiv.org/pdf/2512.05119", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Enhong Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.5, "max_score": 69.52, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "RAG-IGBench: Innovative Evaluation for RAG-based Interleaved Generation in Open-domain Question Answering", "score": 69.52, "session": "San Diego Poster Session 4", "pdf_url": "https://arxiv.org/pdf/2512.05119", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chen Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.3, "max_score": 69.34, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards Single-Source Domain Generalized Object Detection via Causal Visual Prompts", "score": 69.34, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/3086489f466993ba21652ce8a79637805cd873f9.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Huiying Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.3, "max_score": 69.34, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards Single-Source Domain Generalized Object Detection via Causal Visual Prompts", "score": 69.34, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/3086489f466993ba21652ce8a79637805cd873f9.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xinzhong Zhu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 69.3, "max_score": 69.34, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards Single-Source Domain Generalized Object Detection via Causal Visual Prompts", "score": 69.34, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/3086489f466993ba21652ce8a79637805cd873f9.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dongyoung Kim", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.4, "max_score": 68.39, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics", "score": 68.39, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/28f2bd82b713f7bb9565dbe9321b8b6fb4782dc3.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Huiwon Jang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.4, "max_score": 68.39, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics", "score": 68.39, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/28f2bd82b713f7bb9565dbe9321b8b6fb4782dc3.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jinwoo Shin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.4, "max_score": 68.39, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics", "score": 68.39, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/28f2bd82b713f7bb9565dbe9321b8b6fb4782dc3.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kun Xiang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.3, "max_score": 68.27, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SeePhys:  Does Seeing Help Thinking? ‚Äì Benchmarking Vision-Based Physics Reasoning", "score": 68.27, "session": "San Diego Poster Session 4", "pdf_url": "https://arxiv.org/pdf/2505.19099", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Heng Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.3, "max_score": 68.27, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SeePhys:  Does Seeing Help Thinking? ‚Äì Benchmarking Vision-Based Physics Reasoning", "score": 68.27, "session": "San Diego Poster Session 4", "pdf_url": "https://arxiv.org/pdf/2505.19099", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaodan Liang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.3, "max_score": 68.27, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SeePhys:  Does Seeing Help Thinking? ‚Äì Benchmarking Vision-Based Physics Reasoning", "score": 68.27, "session": "San Diego Poster Session 4", "pdf_url": "https://arxiv.org/pdf/2505.19099", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Moritz Miller", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.1, "max_score": 68.14, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Counterfactual reasoning: an analysis of in-context emergence", "score": 68.14, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/94255f9f374c1c8987130333640df5f6b55fd57b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bernhard Sch√∂lkopf", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.1, "max_score": 68.14, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Counterfactual reasoning: an analysis of in-context emergence", "score": 68.14, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/94255f9f374c1c8987130333640df5f6b55fd57b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Siyuan Guo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.1, "max_score": 68.14, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Counterfactual reasoning: an analysis of in-context emergence", "score": 68.14, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/94255f9f374c1c8987130333640df5f6b55fd57b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Constantin Venhoff", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.1, "max_score": 68.07, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval", "score": 68.07, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/fdeb4ce0127121481dc4bc44af9c24a30eb5a6d3.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ashkan Khakzar", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.1, "max_score": 68.07, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval", "score": 68.07, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/fdeb4ce0127121481dc4bc44af9c24a30eb5a6d3.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Neel Nanda", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.1, "max_score": 68.07, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval", "score": 68.07, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/fdeb4ce0127121481dc4bc44af9c24a30eb5a6d3.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Brown Ebouky", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68.01, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Eliciting Reasoning in Language Models with Cognitive Tools", "score": 68.01, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/ea576c6461243d27a8102c9dfa554db950d66757.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Andrea Bartezzaghi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68.01, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Eliciting Reasoning in Language Models with Cognitive Tools", "score": 68.01, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/ea576c6461243d27a8102c9dfa554db950d66757.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mattia Rigotti", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 68.0, "max_score": 68.01, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Eliciting Reasoning in Language Models with Cognitive Tools", "score": 68.01, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/ea576c6461243d27a8102c9dfa554db950d66757.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Patrick Kahardipraja", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.7, "max_score": 67.67, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation", "score": 67.67, "session": "Mexico City Poster Session 3", "pdf_url": "https://openreview.net/pdf/4680606493cfdf0ac8851fe0e353c1878d147253.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Reduan Achtibat", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.7, "max_score": 67.67, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation", "score": 67.67, "session": "Mexico City Poster Session 3", "pdf_url": "https://openreview.net/pdf/4680606493cfdf0ac8851fe0e353c1878d147253.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sebastian Lapuschkin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.7, "max_score": 67.67, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation", "score": 67.67, "session": "Mexico City Poster Session 3", "pdf_url": "https://openreview.net/pdf/4680606493cfdf0ac8851fe0e353c1878d147253.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chengpeng Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.7, "max_score": 67.7, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Teaching Language Models to Reason with Tools", "score": 67.7, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/406d2cc67059647e62684368005578debbc188e9.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhengyang Tang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.7, "max_score": 67.7, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Teaching Language Models to Reason with Tools", "score": 67.7, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/406d2cc67059647e62684368005578debbc188e9.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dayiheng Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.7, "max_score": 67.7, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Teaching Language Models to Reason with Tools", "score": 67.7, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/406d2cc67059647e62684368005578debbc188e9.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Muye Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.4, "max_score": 67.42, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ChartSketcher: Reasoning with Multimodal Feedback and Reflection for Chart Understanding", "score": 67.42, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/e0a6d52ba473df422b4d1442df349437256e50ac.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lingling Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.4, "max_score": 67.42, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ChartSketcher: Reasoning with Multimodal Feedback and Reflection for Chart Understanding", "score": 67.42, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/e0a6d52ba473df422b4d1442df349437256e50ac.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jun Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.4, "max_score": 67.42, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ChartSketcher: Reasoning with Multimodal Feedback and Reflection for Chart Understanding", "score": 67.42, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/e0a6d52ba473df422b4d1442df349437256e50ac.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Miaoyu Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.4, "max_score": 67.36, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Two Causally Related Needles in a Video Haystack", "score": 67.36, "session": "San Diego Poster Session 6", "pdf_url": "https://arxiv.org/pdf/2505.19853", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qin Chao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.4, "max_score": 67.36, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Two Causally Related Needles in a Video Haystack", "score": 67.36, "session": "San Diego Poster Session 6", "pdf_url": "https://arxiv.org/pdf/2505.19853", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Boyang Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.4, "max_score": 67.36, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Two Causally Related Needles in a Video Haystack", "score": 67.36, "session": "San Diego Poster Session 6", "pdf_url": "https://arxiv.org/pdf/2505.19853", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wujian Peng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.2, "max_score": 67.18, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "INST-IT: Boosting Instance Understanding via Explicit Visual Prompt Instruction Tuning", "score": 67.18, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/981b9df93d11c493f23f85073116bf2f42508fe4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lingchen Meng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.2, "max_score": 67.18, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "INST-IT: Boosting Instance Understanding via Explicit Visual Prompt Instruction Tuning", "score": 67.18, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/981b9df93d11c493f23f85073116bf2f42508fe4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yu-Gang Jiang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 67.2, "max_score": 67.18, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "INST-IT: Boosting Instance Understanding via Explicit Visual Prompt Instruction Tuning", "score": 67.18, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/981b9df93d11c493f23f85073116bf2f42508fe4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenxiao Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.7, "max_score": 66.69, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards Reliable and Holistic Visual In-Context Learning Prompt Selection", "score": 66.69, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/fb03fb96ae01e3fd443a1ed7d1588bb263abe314.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jing-Hao Xue", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.7, "max_score": 66.69, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards Reliable and Holistic Visual In-Context Learning Prompt Selection", "score": 66.69, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/fb03fb96ae01e3fd443a1ed7d1588bb263abe314.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yanwei Fu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.7, "max_score": 66.69, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards Reliable and Holistic Visual In-Context Learning Prompt Selection", "score": 66.69, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/fb03fb96ae01e3fd443a1ed7d1588bb263abe314.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Weizhe Yuan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.4, "max_score": 66.38, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions", "score": 66.38, "session": "San Diego Poster Session 2", "pdf_url": "https://arxiv.org/pdf/2502.13124", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jane Yu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.4, "max_score": 66.38, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions", "score": 66.38, "session": "San Diego Poster Session 2", "pdf_url": "https://arxiv.org/pdf/2502.13124", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xian Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.4, "max_score": 66.38, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions", "score": 66.38, "session": "San Diego Poster Session 2", "pdf_url": "https://arxiv.org/pdf/2502.13124", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junhao Shi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.4, "max_score": 66.44, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner", "score": 66.44, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/c4dfdf9e91cc4527c4c879b52d8cc8d352f626ce.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhaoye Fei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.4, "max_score": 66.44, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner", "score": 66.44, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/c4dfdf9e91cc4527c4c879b52d8cc8d352f626ce.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xipeng Qiu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.4, "max_score": 66.44, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner", "score": 66.44, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/c4dfdf9e91cc4527c4c879b52d8cc8d352f626ce.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ruiqi Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.4, "max_score": 66.41, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "RESAnything: Attribute Prompting for Arbitrary Referring Segmentation", "score": 66.41, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/3ec88c013e57b56be455d70c321c6a870176ef99.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hao Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.4, "max_score": 66.41, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "RESAnything: Attribute Prompting for Arbitrary Referring Segmentation", "score": 66.41, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/3ec88c013e57b56be455d70c321c6a870176ef99.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Liqiang Nie", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 66.3, "max_score": 80.77, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Spatial Understanding from Videos: Structured Prompts Meet Simulation Data", "score": 80.77, "session": "Mexico City Poster Session 1", "pdf_url": "https://openreview.net/pdf/ae7873ba9e6cdcfd29717a979c05e0533e2d1f47.pdf", "relevant": 0, "reads": 0}, {"title": "CogVLA: Cognition-Aligned Vision-Language-Action Models via Instruction-Driven Routing & Sparsification", "score": 51.85, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/e67f52c1abdf244a6b56b29468a234e1c2b8d202.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sungho Jeon", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.3, "max_score": 66.34, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning", "score": 66.34, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/6ac620875c43ec32aad37e9348fd7ad1d1c6e2b0.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xinyue Ma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.3, "max_score": 66.34, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning", "score": 66.34, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/6ac620875c43ec32aad37e9348fd7ad1d1c6e2b0.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Myeongjae Jeon", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.3, "max_score": 66.34, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning", "score": 66.34, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/6ac620875c43ec32aad37e9348fd7ad1d1c6e2b0.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Baoqi Pei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.2, "max_score": 66.18, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT", "score": 66.18, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/1665e46ba22f09a2c8f9a73f99a650efa4e800a8.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yifei Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.2, "max_score": 66.18, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT", "score": 66.18, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/1665e46ba22f09a2c8f9a73f99a650efa4e800a8.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yu Qiao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 66.2, "max_score": 66.18, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT", "score": 66.18, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/1665e46ba22f09a2c8f9a73f99a650efa4e800a8.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jin Seong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.9, "max_score": 65.9, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MemEIC: A Step Toward Continual and Compositional Knowledge Editing", "score": 65.9, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/e5209240370185240edeb5bc9a5296481fd5702b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiyun Park", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.9, "max_score": 65.9, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MemEIC: A Step Toward Continual and Compositional Knowledge Editing", "score": 65.9, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/e5209240370185240edeb5bc9a5296481fd5702b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Namhoon Lee", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.9, "max_score": 65.9, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MemEIC: A Step Toward Continual and Compositional Knowledge Editing", "score": 65.9, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/e5209240370185240edeb5bc9a5296481fd5702b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaomin Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.8, "max_score": 65.83, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs", "score": 65.83, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/057301f0efb5252bc107372a0fda3e9e277ad744.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhou Yu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.8, "max_score": 65.83, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs", "score": 65.83, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/057301f0efb5252bc107372a0fda3e9e277ad744.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Anurag Beniwal", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.8, "max_score": 65.83, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs", "score": 65.83, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/057301f0efb5252bc107372a0fda3e9e277ad744.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaoyu Zhan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.7, "max_score": 65.66, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models", "score": 65.66, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/1321dbcd7c95604245189132069ca5ac16d3058c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenxuan Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.7, "max_score": 65.66, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models", "score": 65.66, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/1321dbcd7c95604245189132069ca5ac16d3058c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yanwen Guo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.7, "max_score": 65.66, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models", "score": 65.66, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/1321dbcd7c95604245189132069ca5ac16d3058c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Whie Jung", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.1, "max_score": 65.09, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Bridging the gap to real-world language-grounded visual concept learning", "score": 65.09, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/0bcb1846aed7fb3d788d751b0129765e4861a9ab.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Semin Kim", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.1, "max_score": 65.09, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Bridging the gap to real-world language-grounded visual concept learning", "score": 65.09, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/0bcb1846aed7fb3d788d751b0129765e4861a9ab.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Seunghoon Hong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.1, "max_score": 65.09, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Bridging the gap to real-world language-grounded visual concept learning", "score": 65.09, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/0bcb1846aed7fb3d788d751b0129765e4861a9ab.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenyi Xiao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65.03, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Fast-Slow Thinking GRPO for Large Vision-Language Model Reasoning", "score": 65.03, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/52fe68c14c7abde636af8fd396bd6a026575c7f0.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Leilei Gan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 65.0, "max_score": 65.03, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Fast-Slow Thinking GRPO for Large Vision-Language Model Reasoning", "score": 65.03, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/52fe68c14c7abde636af8fd396bd6a026575c7f0.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuxin Wen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.5, "max_score": 64.46, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Quantifying Cross-Modality Memorization in Vision-Language Models", "score": 64.46, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/e5e5cfc7f6638a6081384cd92bec67f1ceee43ab.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yangsibo Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.5, "max_score": 64.46, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Quantifying Cross-Modality Memorization in Vision-Language Models", "score": 64.46, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/e5e5cfc7f6638a6081384cd92bec67f1ceee43ab.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chiyuan Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.5, "max_score": 64.46, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Quantifying Cross-Modality Memorization in Vision-Language Models", "score": 64.46, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/e5e5cfc7f6638a6081384cd92bec67f1ceee43ab.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jundong Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.4, "max_score": 64.36, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MuSLR: Multimodal Symbolic Logical Reasoning", "score": 64.36, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/c3fc5a8b459eec88aa0b5113f6e1f32a2d2743e4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hao Fei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.4, "max_score": 64.36, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MuSLR: Multimodal Symbolic Logical Reasoning", "score": 64.36, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/c3fc5a8b459eec88aa0b5113f6e1f32a2d2743e4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wynne Hsu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.4, "max_score": 64.36, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MuSLR: Multimodal Symbolic Logical Reasoning", "score": 64.36, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/c3fc5a8b459eec88aa0b5113f6e1f32a2d2743e4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ye Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.2, "max_score": 64.25, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning", "score": 64.25, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/336fb0f358e8f162033e50e7ebc25e1ba700ce2f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zongyang Ma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.2, "max_score": 64.25, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning", "score": 64.25, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/336fb0f358e8f162033e50e7ebc25e1ba700ce2f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chang Wen Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.2, "max_score": 64.25, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning", "score": 64.25, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/336fb0f358e8f162033e50e7ebc25e1ba700ce2f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shuang Zeng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64.03, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "score": 64.03, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/fbba0847160a598188317e0885484ad6c0596097.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xinyuan Chang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64.03, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "score": 64.03, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/fbba0847160a598188317e0885484ad6c0596097.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xing Wei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 64.0, "max_score": 64.03, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving", "score": 64.03, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/fbba0847160a598188317e0885484ad6c0596097.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Liyan Tang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.7, "max_score": 63.72, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "score": 63.72, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2505.13444", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Grace Kim", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.7, "max_score": 63.72, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "score": 63.72, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2505.13444", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Greg Durrett", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.7, "max_score": 63.72, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "score": 63.72, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2505.13444", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuqi Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.2, "max_score": 63.18, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents", "score": 63.18, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/f6767d3d1ddd7b12b85d94abd6d7a313406de8a8.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sunhao Dai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.2, "max_score": 63.18, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents", "score": 63.18, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/f6767d3d1ddd7b12b85d94abd6d7a313406de8a8.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jun Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.2, "max_score": 63.18, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents", "score": 63.18, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/f6767d3d1ddd7b12b85d94abd6d7a313406de8a8.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yiyang Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.1, "max_score": 63.07, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "All You Need is One: Capsule Prompt Tuning with a Single Vector", "score": 63.07, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/66d1effdb19558fa52a14e3d6f9475451d810c9d.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "James Chenhao Liang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.1, "max_score": 63.07, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "All You Need is One: Capsule Prompt Tuning with a Single Vector", "score": 63.07, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/66d1effdb19558fa52a14e3d6f9475451d810c9d.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Cheng Han", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.1, "max_score": 63.07, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "All You Need is One: Capsule Prompt Tuning with a Single Vector", "score": 63.07, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/66d1effdb19558fa52a14e3d6f9475451d810c9d.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhenyu Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.0, "max_score": 63.04, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "score": 63.04, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/311841e491eca81acc49d5288503e9b7819788d6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianyi Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.0, "max_score": 63.04, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "score": 63.04, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/311841e491eca81acc49d5288503e9b7819788d6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaxin Pei", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 63.0, "max_score": 63.04, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents", "score": 63.04, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/311841e491eca81acc49d5288503e9b7819788d6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhihang Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.9, "max_score": 62.91, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CAPability: A Comprehensive Visual Caption Benchmark for Evaluating Both Correctness and Thoroughness", "score": 62.91, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2502.14914", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chen-Wei Xie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.9, "max_score": 62.91, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CAPability: A Comprehensive Visual Caption Benchmark for Evaluating Both Correctness and Thoroughness", "score": 62.91, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2502.14914", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hongtao Xie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.9, "max_score": 62.91, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CAPability: A Comprehensive Visual Caption Benchmark for Evaluating Both Correctness and Thoroughness", "score": 62.91, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2502.14914", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yiren Song", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 62.8, "max_score": 70.68, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "RelationAdapter: Learning and Transferring Visual Relation with Diffusion Transformers", "score": 70.68, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/2c9877508b7505c4c6e730f2b3f8e054edbe33ea.pdf", "relevant": 0, "reads": 0}, {"title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains", "score": 54.99, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/37967ca9eb618eb49ac5f64b75660d4ad6a47109.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hang Yin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.8, "max_score": 62.75, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Stitch and Tell: A Structured Data Augmentation Method for Spatial Understanding", "score": 62.75, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/9de7c7ad156768fbba5018c67e831060150f4fa6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaomin He", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.8, "max_score": 62.75, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Stitch and Tell: A Structured Data Augmentation Method for Spatial Understanding", "score": 62.75, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/9de7c7ad156768fbba5018c67e831060150f4fa6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jialiang Kang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.8, "max_score": 62.76, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding", "score": 62.76, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/5cc1cc1fbfadc72f4c5f96b94d29671e352d2f1f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Han Shu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.8, "max_score": 62.76, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding", "score": 62.76, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/5cc1cc1fbfadc72f4c5f96b94d29671e352d2f1f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xinghao Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.8, "max_score": 62.76, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding", "score": 62.76, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/5cc1cc1fbfadc72f4c5f96b94d29671e352d2f1f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tom Kouwenhoven", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.6, "max_score": 62.6, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Cross-modal Associations in Vision and Language Models: Revisiting the Bouba-Kiki Effect", "score": 62.6, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/58f4768cf595d4f408c01015954cec5e3a3d83c6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kiana Shahrasbi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.6, "max_score": 62.6, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Cross-modal Associations in Vision and Language Models: Revisiting the Bouba-Kiki Effect", "score": 62.6, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/58f4768cf595d4f408c01015954cec5e3a3d83c6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tessa Verhoef", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.6, "max_score": 62.6, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Cross-modal Associations in Vision and Language Models: Revisiting the Bouba-Kiki Effect", "score": 62.6, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/58f4768cf595d4f408c01015954cec5e3a3d83c6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhongyi Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.5, "max_score": 62.48, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ChatVLA-2: Vision-Language-Action Model with Open-World Reasoning", "score": 62.48, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/c88d737915ea445cb600d21cb0c7125912b7053b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yichen Zhu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.5, "max_score": 62.48, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ChatVLA-2: Vision-Language-Action Model with Open-World Reasoning", "score": 62.48, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/c88d737915ea445cb600d21cb0c7125912b7053b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yi Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.5, "max_score": 62.48, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ChatVLA-2: Vision-Language-Action Model with Open-World Reasoning", "score": 62.48, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/c88d737915ea445cb600d21cb0c7125912b7053b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuxuan Luo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.4, "max_score": 62.42, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning", "score": 62.42, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2506.10963", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ryan Yuan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.4, "max_score": 62.42, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning", "score": 62.42, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2506.10963", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhouhui Lian", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.4, "max_score": 62.42, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MMMG: A Massive, Multidisciplinary, Multi-Tier Generation Benchmark for Text-to-Image Reasoning", "score": 62.42, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2506.10963", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chao Qu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.4, "max_score": 62.37, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning", "score": 62.37, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/4caa878eb3dbd25718a328d937d8d6cc4e0d3100.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Siting Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.4, "max_score": 62.42, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval", "score": 62.42, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/8197443737fe19c9a91c16f13d0dcf17e31a87d0.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiang Gao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.4, "max_score": 62.42, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval", "score": 62.42, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/8197443737fe19c9a91c16f13d0dcf17e31a87d0.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Simon Shaolei Du", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 62.4, "max_score": 62.42, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval", "score": 62.42, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/8197443737fe19c9a91c16f13d0dcf17e31a87d0.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jingli Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.9, "max_score": 61.89, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding", "score": 61.89, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2507.07984", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chenming Zhu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.9, "max_score": 61.89, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding", "score": 61.89, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2507.07984", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiangmiao Pang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.9, "max_score": 61.89, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online Spatio-temporal Scene Understanding", "score": 61.89, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2507.07984", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Daiqing Qi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.8, "max_score": 61.8, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Improve Temporal Reasoning in Multimodal Large Language Models via Video Contrastive Decoding", "score": 61.8, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/1494d847179a71c38929ca5d87774036b701f164.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dongliang Guo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.8, "max_score": 61.8, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Improve Temporal Reasoning in Multimodal Large Language Models via Video Contrastive Decoding", "score": 61.8, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/1494d847179a71c38929ca5d87774036b701f164.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sheng Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.8, "max_score": 61.8, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Improve Temporal Reasoning in Multimodal Large Language Models via Video Contrastive Decoding", "score": 61.8, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/1494d847179a71c38929ca5d87774036b701f164.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qiuchen Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.7, "max_score": 61.71, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning", "score": 61.71, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/facbdeba1c5f4027c295d801ab7c24a818c3957f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ruixue Ding", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.7, "max_score": 61.71, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning", "score": 61.71, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/facbdeba1c5f4027c295d801ab7c24a818c3957f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Feng Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.7, "max_score": 61.71, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning", "score": 61.71, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/facbdeba1c5f4027c295d801ab7c24a818c3957f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Guy Davidson", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.2, "max_score": 61.24, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Do different prompting methods yield a common task representation in language models?", "score": 61.24, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/3b10744b26832eee4512c27477a8af62d2dea11f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Todd M. Gureckis", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.2, "max_score": 61.24, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Do different prompting methods yield a common task representation in language models?", "score": 61.24, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/3b10744b26832eee4512c27477a8af62d2dea11f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Adina Williams", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.2, "max_score": 61.24, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Do different prompting methods yield a common task representation in language models?", "score": 61.24, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/3b10744b26832eee4512c27477a8af62d2dea11f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Taeyoun Kim", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.1, "max_score": 61.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Reasoning as an Adaptive Defense for Safety", "score": 61.12, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/c036ebd40ea82ab94b9badc0028faf7e94456ca1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Fahim Tajwar", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.1, "max_score": 61.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Reasoning as an Adaptive Defense for Safety", "score": 61.12, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/c036ebd40ea82ab94b9badc0028faf7e94456ca1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Aviral Kumar", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 61.1, "max_score": 61.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Reasoning as an Adaptive Defense for Safety", "score": 61.12, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/c036ebd40ea82ab94b9badc0028faf7e94456ca1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Renpu Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 60.8, "max_score": 60.77, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Unlabeled Data Can Provably Enhance In-Context Learning of Transformers", "score": 60.77, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/fbbc09553e40eaaefc993ce079b6a800e2f94653.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jing Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 60.8, "max_score": 60.77, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Unlabeled Data Can Provably Enhance In-Context Learning of Transformers", "score": 60.77, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/fbbc09553e40eaaefc993ce079b6a800e2f94653.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kaixiang Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 60.6, "max_score": 60.62, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Jury-and-Judge Chain-of-Thought for Uncovering Toxic Data in 3D Visual Grounding", "score": 60.62, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/25a92931070b5b801961cd0767a855a579110111.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qifeng Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 60.6, "max_score": 60.62, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Jury-and-Judge Chain-of-Thought for Uncovering Toxic Data in 3D Visual Grounding", "score": 60.62, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/25a92931070b5b801961cd0767a855a579110111.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shengfeng He", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 60.6, "max_score": 60.62, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Jury-and-Judge Chain-of-Thought for Uncovering Toxic Data in 3D Visual Grounding", "score": 60.62, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/25a92931070b5b801961cd0767a855a579110111.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaxin Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 60.0, "max_score": 60.01, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Surprise3D: A Dataset for Spatial Understanding and Reasoning in Complex 3D Scenes", "score": 60.01, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2507.07781", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ziwen Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 60.0, "max_score": 60.01, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Surprise3D: A Dataset for Spatial Understanding and Reasoning in Complex 3D Scenes", "score": 60.01, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2507.07781", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mingming Gong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 60.0, "max_score": 60.01, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Surprise3D: A Dataset for Spatial Understanding and Reasoning in Complex 3D Scenes", "score": 60.01, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2507.07781", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haibo Jin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 60.0, "max_score": 59.96, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Evaluating the Inductive Abilities of Large Language Models: Why Chain-of-Thought Reasoning Sometimes Hurts More Than Helps", "score": 59.96, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/df72ac5f5d065191323bb112c1280014c74a4ab1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Peiyan Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 60.0, "max_score": 59.96, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Evaluating the Inductive Abilities of Large Language Models: Why Chain-of-Thought Reasoning Sometimes Hurts More Than Helps", "score": 59.96, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/df72ac5f5d065191323bb112c1280014c74a4ab1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haohan Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 60.0, "max_score": 59.96, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Evaluating the Inductive Abilities of Large Language Models: Why Chain-of-Thought Reasoning Sometimes Hurts More Than Helps", "score": 59.96, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/df72ac5f5d065191323bb112c1280014c74a4ab1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Longtian Qiu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.9, "max_score": 59.92, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation", "score": 59.92, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/96f8ac8e6e7af57f7d1898ceced0a9d165735c5e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shan Ning", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.9, "max_score": 59.92, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation", "score": 59.92, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/96f8ac8e6e7af57f7d1898ceced0a9d165735c5e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xuming He", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.9, "max_score": 59.92, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation", "score": 59.92, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/96f8ac8e6e7af57f7d1898ceced0a9d165735c5e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kan Li", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 59.5, "max_score": 62.75, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Stitch and Tell: A Structured Data Augmentation Method for Spatial Understanding", "score": 62.75, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/9de7c7ad156768fbba5018c67e831060150f4fa6.pdf", "relevant": 0, "reads": 0}, {"title": "Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules", "score": 56.3, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/ccc57d1f272b13a4853fce78582f6e5b42ba120e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Senqiao Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.1, "max_score": 59.08, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning", "score": 59.08, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/a1ce03f1786df2e67c61dd99ba4f40d2d92f913b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junyi Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.1, "max_score": 59.08, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning", "score": 59.08, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/a1ce03f1786df2e67c61dd99ba4f40d2d92f913b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaya Jia", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.1, "max_score": 59.08, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning", "score": 59.08, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/a1ce03f1786df2e67c61dd99ba4f40d2d92f913b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yue Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.0, "max_score": 59.03, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning", "score": 59.03, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/3c2def3de954b67f399af3bd84c4e317c6d3b893.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shengfang Zhai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.0, "max_score": 59.03, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning", "score": 59.03, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/3c2def3de954b67f399af3bd84c4e317c6d3b893.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bryan Hooi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 59.0, "max_score": 59.03, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning", "score": 59.03, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/3c2def3de954b67f399af3bd84c4e317c6d3b893.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Minghe Gao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.8, "max_score": 58.77, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Counterfactual Evolution of Multimodal Datasets via Visual Programming", "score": 58.77, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/1c64e34216940d5fb89d4399163b323815db7475.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhongqi Yue", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.8, "max_score": 58.77, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Counterfactual Evolution of Multimodal Datasets via Visual Programming", "score": 58.77, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/1c64e34216940d5fb89d4399163b323815db7475.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Juncheng Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.8, "max_score": 58.77, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Counterfactual Evolution of Multimodal Datasets via Visual Programming", "score": 58.77, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/1c64e34216940d5fb89d4399163b323815db7475.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ming Nie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.8, "max_score": 58.83, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards Unified Multimodal Interleaved Generation via Group Relative Policy Optimization", "score": 58.83, "session": "Mexico City Poster Session 2", "pdf_url": "https://openreview.net/pdf/be81012c4b8d2224b305eb1f109bd4f030fe2ac2.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chunwei Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.8, "max_score": 58.83, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards Unified Multimodal Interleaved Generation via Group Relative Policy Optimization", "score": 58.83, "session": "Mexico City Poster Session 2", "pdf_url": "https://openreview.net/pdf/be81012c4b8d2224b305eb1f109bd4f030fe2ac2.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kuicai Dong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.3, "max_score": 58.33, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering", "score": 58.33, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2505.16470", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "CHANG YUJING", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.3, "max_score": 58.33, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering", "score": 58.33, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2505.16470", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yong Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.3, "max_score": 58.33, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering", "score": 58.33, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2505.16470", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhaowei Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.3, "max_score": 58.32, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly", "score": 58.32, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2505.10610", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenhao Yu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.3, "max_score": 58.32, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly", "score": 58.32, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2505.10610", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mark Steedman", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.3, "max_score": 58.32, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly", "score": 58.32, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2505.10610", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhangyun Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.1, "max_score": 58.14, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Learning Robust Vision-Language Models from Natural Latent Spaces", "score": 58.14, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/e8595a31cf77a217d9396e59495cf5cba4c01d8e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ni Ding", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.1, "max_score": 58.14, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Learning Robust Vision-Language Models from Natural Latent Spaces", "score": 58.14, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/e8595a31cf77a217d9396e59495cf5cba4c01d8e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Aniket Mahanti", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.1, "max_score": 58.14, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Learning Robust Vision-Language Models from Natural Latent Spaces", "score": 58.14, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/e8595a31cf77a217d9396e59495cf5cba4c01d8e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jun-Peng Jiang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.0, "max_score": 58.01, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Multimodal Tabular Reasoning with Privileged Structured Information", "score": 58.01, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/c140cd5b9ba2e6981d9024c35dbfc29dc881e63b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yu Xia", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.0, "max_score": 58.01, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Multimodal Tabular Reasoning with Privileged Structured Information", "score": 58.01, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/c140cd5b9ba2e6981d9024c35dbfc29dc881e63b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Han-Jia Ye", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 58.0, "max_score": 58.01, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Multimodal Tabular Reasoning with Privileged Structured Information", "score": 58.01, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/c140cd5b9ba2e6981d9024c35dbfc29dc881e63b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mingyang Fu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.9, "max_score": 57.87, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Seeking and Updating with Live Visual Knowledge", "score": 57.87, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2504.05288", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuyang Peng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.9, "max_score": 57.87, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Seeking and Updating with Live Visual Knowledge", "score": 57.87, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2504.05288", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ranjay Krishna", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.9, "max_score": 57.87, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Seeking and Updating with Live Visual Knowledge", "score": 57.87, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2504.05288", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yizhi LI", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.8, "max_score": 57.81, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "OmniBench: Towards The Future of  Universal Omni-Language Models", "score": 57.81, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/c52922a7cb62ce2d159c41efd473976ed5a5693f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ge Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.8, "max_score": 57.81, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "OmniBench: Towards The Future of  Universal Omni-Language Models", "score": 57.81, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/c52922a7cb62ce2d159c41efd473976ed5a5693f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chenghua Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.8, "max_score": 57.81, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "OmniBench: Towards The Future of  Universal Omni-Language Models", "score": 57.81, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/c52922a7cb62ce2d159c41efd473976ed5a5693f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiarui Yao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.4, "max_score": 57.4, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL", "score": 57.4, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/c7e078578b86348d9d5cc90c5db280404242f16b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yifan HAO", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.4, "max_score": 57.4, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL", "score": 57.4, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/c7e078578b86348d9d5cc90c5db280404242f16b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tong Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.4, "max_score": 57.4, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL", "score": 57.4, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/c7e078578b86348d9d5cc90c5db280404242f16b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shengming Yuan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.1, "max_score": 57.06, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models", "score": 57.06, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/21b2b82696dbfdaf174b6c8e52ac2477c419be55.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xinyu Lyu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.1, "max_score": 57.06, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models", "score": 57.06, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/21b2b82696dbfdaf174b6c8e52ac2477c419be55.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lianli Gao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.1, "max_score": 57.06, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models", "score": 57.06, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/21b2b82696dbfdaf174b6c8e52ac2477c419be55.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sen Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.1, "max_score": 57.06, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SAMPO: Scale-wise Autoregression with Motion Prompt for Generative World Models", "score": 57.06, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/829c7820e5b754498ac9f30bbe8588d4d77b2ddf.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jingyi Tian", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.1, "max_score": 57.06, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SAMPO: Scale-wise Autoregression with Motion Prompt for Generative World Models", "score": 57.06, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/829c7820e5b754498ac9f30bbe8588d4d77b2ddf.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Gang Hua", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.1, "max_score": 57.06, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SAMPO: Scale-wise Autoregression with Motion Prompt for Generative World Models", "score": 57.06, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/829c7820e5b754498ac9f30bbe8588d4d77b2ddf.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhifang Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.1, "max_score": 57.14, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning", "score": 57.14, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/ca844585c6a13050f54ca0b88e52a39da65aacfd.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shuo He", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.1, "max_score": 57.14, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning", "score": 57.14, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/ca844585c6a13050f54ca0b88e52a39da65aacfd.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lei Feng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.1, "max_score": 57.14, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning", "score": 57.14, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/ca844585c6a13050f54ca0b88e52a39da65aacfd.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Man Ho LAM", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.1, "max_score": 57.13, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning", "score": 57.13, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/9b9883a392dca607cfb2b26eccc24136b0e5cf90.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chaozheng Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.1, "max_score": 57.13, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning", "score": 57.13, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/9b9883a392dca607cfb2b26eccc24136b0e5cf90.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Michael Lyu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 57.1, "max_score": 57.13, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning", "score": 57.13, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/9b9883a392dca607cfb2b26eccc24136b0e5cf90.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sarthak Chakraborty", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.5, "max_score": 56.52, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Generative Caching for Structurally Similar Prompts and Responses", "score": 56.52, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/df29ee5226893fdb3de7942c122869fbc8f4ced9.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Suman Nath", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.5, "max_score": 56.52, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Generative Caching for Structurally Similar Prompts and Responses", "score": 56.52, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/df29ee5226893fdb3de7942c122869fbc8f4ced9.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Indranil Gupta", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.5, "max_score": 56.52, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Generative Caching for Structurally Similar Prompts and Responses", "score": 56.52, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/df29ee5226893fdb3de7942c122869fbc8f4ced9.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xudong Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.5, "max_score": 56.48, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs", "score": 56.48, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/bec6a67c764747509298fcd29045d23332b42142.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mengdan Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.5, "max_score": 56.48, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs", "score": 56.48, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/bec6a67c764747509298fcd29045d23332b42142.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Rongrong Ji", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.5, "max_score": 56.48, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs", "score": 56.48, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/bec6a67c764747509298fcd29045d23332b42142.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Haolei Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.5, "max_score": 56.47, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "score": 56.47, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/b63a80f12eba96f7a1d80c464c6cd302c8b2c6b6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yuchen Yan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.5, "max_score": 56.47, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "score": 56.47, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/b63a80f12eba96f7a1d80c464c6cd302c8b2c6b6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ling Fu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.5, "max_score": 56.45, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "OCRBench v2: An Improved Benchmark for Evaluating Large Multimodal Models on Visual Text Localization and Reasoning", "score": 56.45, "session": "Mexico City Poster Session 6", "pdf_url": "https://arxiv.org/pdf/2501.00321", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhebin Kuang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.5, "max_score": 56.45, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "OCRBench v2: An Improved Benchmark for Evaluating Large Multimodal Models on Visual Text Localization and Reasoning", "score": 56.45, "session": "Mexico City Poster Session 6", "pdf_url": "https://arxiv.org/pdf/2501.00321", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiang Bai", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.5, "max_score": 56.45, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "OCRBench v2: An Improved Benchmark for Evaluating Large Multimodal Models on Visual Text Localization and Reasoning", "score": 56.45, "session": "Mexico City Poster Session 6", "pdf_url": "https://arxiv.org/pdf/2501.00321", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hoyoon Byun", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.4, "max_score": 56.35, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CCL: Causal-aware In-context Learning for Out-of-Distribution Generalization", "score": 56.35, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/6d184a907967073156ac2ab81fa7d47b72c88e03.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Gyeongdeok Seo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.4, "max_score": 56.35, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CCL: Causal-aware In-context Learning for Out-of-Distribution Generalization", "score": 56.35, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/6d184a907967073156ac2ab81fa7d47b72c88e03.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kyungwoo Song", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.4, "max_score": 56.35, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CCL: Causal-aware In-context Learning for Out-of-Distribution Generalization", "score": 56.35, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/6d184a907967073156ac2ab81fa7d47b72c88e03.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yang Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.4, "max_score": 56.36, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning", "score": 56.36, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/4dbb7f099e0ae9ed52f26be78bbd5f18b4adc8a6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhuolin Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.4, "max_score": 56.36, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning", "score": 56.36, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/4dbb7f099e0ae9ed52f26be78bbd5f18b4adc8a6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wei Ping", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.4, "max_score": 56.36, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning", "score": 56.36, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/4dbb7f099e0ae9ed52f26be78bbd5f18b4adc8a6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yueqi Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.3, "max_score": 56.3, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules", "score": 56.3, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/ccc57d1f272b13a4853fce78582f6e5b42ba120e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Peiwen Yuan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.3, "max_score": 56.3, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules", "score": 56.3, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/ccc57d1f272b13a4853fce78582f6e5b42ba120e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Guanghao Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.2, "max_score": 56.21, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought", "score": 56.21, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/e31dcc29db400a0d8f2b23e4dca55cc1663442a1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenhao Jiang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.2, "max_score": 56.21, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought", "score": 56.21, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/e31dcc29db400a0d8f2b23e4dca55cc1663442a1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chun Yuan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 56.2, "max_score": 56.21, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought", "score": 56.21, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/e31dcc29db400a0d8f2b23e4dca55cc1663442a1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yanglin Feng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.9, "max_score": 55.92, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Robust Cross-modal Alignment Learning for Cross-Scene Spatial Reasoning and Grounding", "score": 55.92, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/17c45f7182d9da6b92fcf8467d83199936ca20aa.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hongyuan Zhu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.9, "max_score": 55.92, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Robust Cross-modal Alignment Learning for Cross-Scene Spatial Reasoning and Grounding", "score": 55.92, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/17c45f7182d9da6b92fcf8467d83199936ca20aa.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Peng Hu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.9, "max_score": 55.92, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Robust Cross-modal Alignment Learning for Cross-Scene Spatial Reasoning and Grounding", "score": 55.92, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/17c45f7182d9da6b92fcf8467d83199936ca20aa.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yueting Zhuang", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 55.6, "max_score": 56.47, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning", "score": 56.47, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/b63a80f12eba96f7a1d80c464c6cd302c8b2c6b6.pdf", "relevant": 0, "reads": 0}, {"title": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and Generation via Reinforcement Learning", "score": 54.7, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/4aad34acb1fe54ce3d709604ec7d81ff8ce11b11.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yiju Guo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.6, "max_score": 55.56, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Learning to Focus: Causal Attention Distillation via Gradient‚ÄêGuided Token Pruning", "score": 55.56, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/ccfb52a7538dd17ba250574015c53a7ab5a47315.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenkai Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.6, "max_score": 55.56, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Learning to Focus: Causal Attention Distillation via Gradient‚ÄêGuided Token Pruning", "score": 55.56, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/ccfb52a7538dd17ba250574015c53a7ab5a47315.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yankai Lin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.6, "max_score": 55.56, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Learning to Focus: Causal Attention Distillation via Gradient‚ÄêGuided Token Pruning", "score": 55.56, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/ccfb52a7538dd17ba250574015c53a7ab5a47315.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhongwei Wan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.5, "max_score": 55.55, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning", "score": 55.55, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/514851b938f3c4ab888ed4499926b37fdbb1b89c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhihao Dou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.5, "max_score": 55.55, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning", "score": 55.55, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/514851b938f3c4ab888ed4499926b37fdbb1b89c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shen Yan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.5, "max_score": 55.55, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning", "score": 55.55, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/514851b938f3c4ab888ed4499926b37fdbb1b89c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Li Zhang", "paper_count": 2, "highly_relevant_count": 0, "avg_score": 55.5, "max_score": 58.83, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards Unified Multimodal Interleaved Generation via Group Relative Policy Optimization", "score": 58.83, "session": "Mexico City Poster Session 2", "pdf_url": "https://openreview.net/pdf/be81012c4b8d2224b305eb1f109bd4f030fe2ac2.pdf", "relevant": 0, "reads": 0}, {"title": "From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D", "score": 52.25, "session": "San Diego Poster Session 6", "pdf_url": "https://arxiv.org/pdf/2503.22976", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zimeng Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.3, "max_score": 55.26, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models", "score": 55.26, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2510.26937", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jinxin Ke", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.3, "max_score": 55.26, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models", "score": 55.26, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2510.26937", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ziliang Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.3, "max_score": 55.26, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models", "score": 55.26, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2510.26937", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hongqiong Zhong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.3, "max_score": 55.3, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards Visualization-of-Thought Jailbreak Attack against Large Visual Language Models", "score": 55.3, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/98bc54bf489ff48cae9cd4c5cff483480722ea9c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qingyang Teng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.3, "max_score": 55.3, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards Visualization-of-Thought Jailbreak Attack against Large Visual Language Models", "score": 55.3, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/98bc54bf489ff48cae9cd4c5cff483480722ea9c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kaifu Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.3, "max_score": 55.3, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards Visualization-of-Thought Jailbreak Attack against Large Visual Language Models", "score": 55.3, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/98bc54bf489ff48cae9cd4c5cff483480722ea9c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenhao Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.2, "max_score": 55.25, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning", "score": 55.25, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/521c2d2f24d89aa54641a2bb0276e4fd1a5cebf4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qiangchang Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.2, "max_score": 55.25, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning", "score": 55.25, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/521c2d2f24d89aa54641a2bb0276e4fd1a5cebf4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yilong Yin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.2, "max_score": 55.25, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning", "score": 55.25, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/521c2d2f24d89aa54641a2bb0276e4fd1a5cebf4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yijun Liang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.1, "max_score": 55.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness", "score": 55.12, "session": "San Diego Poster Session 3", "pdf_url": "https://arxiv.org/pdf/2504.10514", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ming Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.1, "max_score": 55.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness", "score": 55.12, "session": "San Diego Poster Session 3", "pdf_url": "https://arxiv.org/pdf/2504.10514", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianyi Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.1, "max_score": 55.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness", "score": 55.12, "session": "San Diego Poster Session 3", "pdf_url": "https://arxiv.org/pdf/2504.10514", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yunlong Deng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 54.95, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards Self-Refinement of Vision-Language Models with Triangular Consistency", "score": 54.95, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/3acba55c4efdef3ee37a854031bdd972e9e31008.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Guangyi Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 54.95, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards Self-Refinement of Vision-Language Models with Triangular Consistency", "score": 54.95, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/3acba55c4efdef3ee37a854031bdd972e9e31008.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kun Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 54.95, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Towards Self-Refinement of Vision-Language Models with Triangular Consistency", "score": 54.95, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/3acba55c4efdef3ee37a854031bdd972e9e31008.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chun Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 54.99, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains", "score": 54.99, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/37967ca9eb618eb49ac5f64b75660d4ad6a47109.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaojun Ye", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 54.99, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains", "score": 54.99, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/37967ca9eb618eb49ac5f64b75660d4ad6a47109.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Chao Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55.0, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought", "score": 55.0, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/374b81cce984cf48963806b2557f39ee9972a826.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Benfeng Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55.0, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought", "score": 55.0, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/374b81cce984cf48963806b2557f39ee9972a826.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiaochun Cao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 55.0, "max_score": 55.0, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought", "score": 55.0, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/374b81cce984cf48963806b2557f39ee9972a826.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kaihang Pan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.7, "max_score": 54.7, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and Generation via Reinforcement Learning", "score": 54.7, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/4aad34acb1fe54ce3d709604ec7d81ff8ce11b11.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yang Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.7, "max_score": 54.7, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and Generation via Reinforcement Learning", "score": 54.7, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/4aad34acb1fe54ce3d709604ec7d81ff8ce11b11.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qiang Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.6, "max_score": 54.65, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Reliable Lifelong Multimodal Editing: Conflict-Aware Retrieval Meets Multi-Level Guidance", "score": 54.65, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/e9c3d5a5e890cfc52d58bb0f8c853abe590119fe.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Fanrui Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.6, "max_score": 54.65, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Reliable Lifelong Multimodal Editing: Conflict-Aware Retrieval Meets Multi-Level Guidance", "score": 54.65, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/e9c3d5a5e890cfc52d58bb0f8c853abe590119fe.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zheng-Jun Zha", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.6, "max_score": 54.65, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Reliable Lifelong Multimodal Editing: Conflict-Aware Retrieval Meets Multi-Level Guidance", "score": 54.65, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/e9c3d5a5e890cfc52d58bb0f8c853abe590119fe.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zekai Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.4, "max_score": 54.39, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models", "score": 54.39, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/2b62626f959a636707dfb61efe094e4dcd7bd32f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Qi Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.4, "max_score": 54.39, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models", "score": 54.39, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/2b62626f959a636707dfb61efe094e4dcd7bd32f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xudong Yan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.3, "max_score": 54.28, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning", "score": 54.28, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/9ca0e483d5c15956c3f1d6258e602d9ca3d8975f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Songhe Feng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.3, "max_score": 54.28, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning", "score": 54.28, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/9ca0e483d5c15956c3f1d6258e602d9ca3d8975f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiani Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.2, "max_score": 54.25, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation", "score": 54.25, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/32e384d1b7bff0817b05af22b3999df7789e5b8d.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Amish Sethi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.2, "max_score": 54.25, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation", "score": 54.25, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/32e384d1b7bff0817b05af22b3999df7789e5b8d.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mayur Naik", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.2, "max_score": 54.25, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation", "score": 54.25, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/32e384d1b7bff0817b05af22b3999df7789e5b8d.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wufei Ma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.2, "max_score": 54.25, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning", "score": 54.25, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/b2e9fe6cf3d6e72577b4271ec01aa5d2a50e35f9.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yu-Cheng Chou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.2, "max_score": 54.25, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning", "score": 54.25, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/b2e9fe6cf3d6e72577b4271ec01aa5d2a50e35f9.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Alan Yuille", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.2, "max_score": 54.25, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning", "score": 54.25, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/b2e9fe6cf3d6e72577b4271ec01aa5d2a50e35f9.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Fangrui Zhu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.2, "max_score": 54.2, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Struct2D: A Perception-Guided Framework for Spatial Reasoning in MLLMs", "score": 54.2, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/45604c251617139c1947693fb938f69e20844fb1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hanhui Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.2, "max_score": 54.2, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Struct2D: A Perception-Guided Framework for Spatial Reasoning in MLLMs", "score": 54.2, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/45604c251617139c1947693fb938f69e20844fb1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Huaizu Jiang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.2, "max_score": 54.2, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Struct2D: A Perception-Guided Framework for Spatial Reasoning in MLLMs", "score": 54.2, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/45604c251617139c1947693fb938f69e20844fb1.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaming Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.0, "max_score": 54.01, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Exploring the Limits of Vision-Language-Action Manipulation in Cross-task Generalization", "score": 54.01, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/ba782206825d0f1f5219718d27497e7e2d001e66.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ke Ye", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.0, "max_score": 54.01, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Exploring the Limits of Vision-Language-Action Manipulation in Cross-task Generalization", "score": 54.01, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/ba782206825d0f1f5219718d27497e7e2d001e66.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Junwei Liang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 54.0, "max_score": 54.01, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Exploring the Limits of Vision-Language-Action Manipulation in Cross-task Generalization", "score": 54.01, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/ba782206825d0f1f5219718d27497e7e2d001e66.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yingjie Gao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.7, "max_score": 53.74, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Test-Time Adaptive Object Detection with Foundation Model", "score": 53.74, "session": "Mexico City Poster Session 4", "pdf_url": "https://openreview.net/pdf/b306a99a829c5ac70afc15277fe7ba935e41fc66.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yanan Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.7, "max_score": 53.74, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Test-Time Adaptive Object Detection with Foundation Model", "score": 53.74, "session": "Mexico City Poster Session 4", "pdf_url": "https://openreview.net/pdf/b306a99a829c5ac70afc15277fe7ba935e41fc66.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Di Huang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.7, "max_score": 53.74, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Test-Time Adaptive Object Detection with Foundation Model", "score": 53.74, "session": "Mexico City Poster Session 4", "pdf_url": "https://openreview.net/pdf/b306a99a829c5ac70afc15277fe7ba935e41fc66.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhiwei Hao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.6, "max_score": 53.57, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MixPrompt: Efficient Mixed Prompting for Multimodal Semantic Segmentation", "score": 53.57, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/5c3bb6a639ece979e6fcba7d959066150516cae4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhongyu Xiao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.6, "max_score": 53.57, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MixPrompt: Efficient Mixed Prompting for Multimodal Semantic Segmentation", "score": 53.57, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/5c3bb6a639ece979e6fcba7d959066150516cae4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Dan Zeng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.6, "max_score": 53.57, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MixPrompt: Efficient Mixed Prompting for Multimodal Semantic Segmentation", "score": 53.57, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/5c3bb6a639ece979e6fcba7d959066150516cae4.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xueliang Zhao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.5, "max_score": 53.45, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "DynaAct: Large Language Model Reasoning with Dynamic Action Spaces", "score": 53.45, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/45820087f0186edb228a8b251bb2af4a3cca38b5.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wei Wu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.5, "max_score": 53.45, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "DynaAct: Large Language Model Reasoning with Dynamic Action Spaces", "score": 53.45, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/45820087f0186edb228a8b251bb2af4a3cca38b5.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Lingpeng Kong", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.5, "max_score": 53.45, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "DynaAct: Large Language Model Reasoning with Dynamic Action Spaces", "score": 53.45, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/45820087f0186edb228a8b251bb2af4a3cca38b5.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jaekyun Park", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.4, "max_score": 53.37, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion", "score": 53.37, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/e8964927372da08d17937b20a3df7acddc069b7f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Hye Won Chung", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.4, "max_score": 53.37, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion", "score": 53.37, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/e8964927372da08d17937b20a3df7acddc069b7f.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ankan Deria", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.3, "max_score": 53.34, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning", "score": 53.34, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/8d9df6d290dbe7ca260a923b4f2357a4855ede26.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Adinath Madhavrao Dukre", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.3, "max_score": 53.34, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning", "score": 53.34, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/8d9df6d290dbe7ca260a923b4f2357a4855ede26.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Imran Razzak", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.3, "max_score": 53.34, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning", "score": 53.34, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/8d9df6d290dbe7ca260a923b4f2357a4855ede26.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jorge (Zhoujun) Cheng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.2, "max_score": 53.16, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective", "score": 53.16, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2506.14965", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shibo Hao", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.2, "max_score": 53.16, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective", "score": 53.16, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2506.14965", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhiting Hu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.2, "max_score": 53.16, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective", "score": 53.16, "session": "San Diego Poster Session 1", "pdf_url": "https://arxiv.org/pdf/2506.14965", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kaiyuan Eric Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.2, "max_score": 53.17, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Robo2VLM: Improving Visual Question Answering using Large-Scale Robot Manipulation Data", "score": 53.17, "session": "San Diego Poster Session 2", "pdf_url": null, "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shuangyu Xie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.2, "max_score": 53.17, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Robo2VLM: Improving Visual Question Answering using Large-Scale Robot Manipulation Data", "score": 53.17, "session": "San Diego Poster Session 2", "pdf_url": null, "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ken Goldberg", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.2, "max_score": 53.17, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Robo2VLM: Improving Visual Question Answering using Large-Scale Robot Manipulation Data", "score": 53.17, "session": "San Diego Poster Session 2", "pdf_url": null, "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Melanie Rieff", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.1, "max_score": 53.14, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SMMILE: An expert-driven benchmark for multimodal medical in-context learning", "score": 53.14, "session": "San Diego Poster Session 2", "pdf_url": "https://arxiv.org/pdf/2506.21355", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Maya Varma", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.1, "max_score": 53.14, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SMMILE: An expert-driven benchmark for multimodal medical in-context learning", "score": 53.14, "session": "San Diego Poster Session 2", "pdf_url": "https://arxiv.org/pdf/2506.21355", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Michael Moor", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.1, "max_score": 53.14, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "SMMILE: An expert-driven benchmark for multimodal medical in-context learning", "score": 53.14, "session": "San Diego Poster Session 2", "pdf_url": "https://arxiv.org/pdf/2506.21355", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Muquan Yu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.1, "max_score": 53.1, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex", "score": 53.1, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/940e87e2f014c1f3d9c0b85364c63c2d437a1d9e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Mu Nan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.1, "max_score": 53.1, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex", "score": 53.1, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/940e87e2f014c1f3d9c0b85364c63c2d437a1d9e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Andrew Luo", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 53.1, "max_score": 53.1, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex", "score": 53.1, "session": "San Diego Poster Session 4", "pdf_url": "https://openreview.net/pdf/940e87e2f014c1f3d9c0b85364c63c2d437a1d9e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shohei Enomoto", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.5, "max_score": 52.53, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Enhancing Visual Prompting through Expanded Transformation Space and Overfitting Mitigation", "score": 52.53, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/4d3dececab4908f91a81a58eb3b1b31959a0e01e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Stephen Chung", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.5, "max_score": 52.55, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Thinker: Learning to Think Fast and Slow", "score": 52.55, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/f23194e92e9932bef9aa81c91ed4d6aa1d835472.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenyu Du", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.5, "max_score": 52.55, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Thinker: Learning to Think Fast and Slow", "score": 52.55, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/f23194e92e9932bef9aa81c91ed4d6aa1d835472.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jie Fu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.5, "max_score": 52.55, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Thinker: Learning to Think Fast and Slow", "score": 52.55, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/f23194e92e9932bef9aa81c91ed4d6aa1d835472.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiahui Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.2, "max_score": 52.25, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D", "score": 52.25, "session": "San Diego Poster Session 6", "pdf_url": "https://arxiv.org/pdf/2503.22976", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yurui Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.2, "max_score": 52.25, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D", "score": 52.25, "session": "San Diego Poster Session 6", "pdf_url": "https://arxiv.org/pdf/2503.22976", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yi Ding", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.1, "max_score": 52.1, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models", "score": 52.1, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/2cca74fc9d7ace7039b552ae0c9713648f96865a.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Ruqi Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.1, "max_score": 52.1, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models", "score": 52.1, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/2cca74fc9d7ace7039b552ae0c9713648f96865a.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kai Yan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.1, "max_score": 52.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MIR-Bench: Can Your LLM Recognize Complicated Patterns via Many-Shot In-Context Reasoning?", "score": 52.12, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2502.09933", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhan Ling", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.1, "max_score": 52.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MIR-Bench: Can Your LLM Recognize Complicated Patterns via Many-Shot In-Context Reasoning?", "score": 52.12, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2502.09933", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiecao Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.1, "max_score": 52.12, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "MIR-Bench: Can Your LLM Recognize Complicated Patterns via Many-Shot In-Context Reasoning?", "score": 52.12, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2502.09933", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenhui Tan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 52.02, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains", "score": 52.02, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/9a1c1a85e08c73fd0ae736b069afe387fbe0fb59.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiaze Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 52.02, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains", "score": 52.02, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/9a1c1a85e08c73fd0ae736b069afe387fbe0fb59.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jian Luan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 52.02, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains", "score": 52.02, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/9a1c1a85e08c73fd0ae736b069afe387fbe0fb59.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Bill Psomas", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 51.98, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Instance-Level Composed Image Retrieval", "score": 51.98, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/adc14ffb898a463ae55a846a3e9f69bd1d5584c0.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "George Retsinas", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 51.98, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Instance-Level Composed Image Retrieval", "score": 51.98, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/adc14ffb898a463ae55a846a3e9f69bd1d5584c0.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Giorgos Tolias", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 52.0, "max_score": 51.98, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Instance-Level Composed Image Retrieval", "score": 51.98, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/adc14ffb898a463ae55a846a3e9f69bd1d5584c0.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wei Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.9, "max_score": 51.85, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CogVLA: Cognition-Aligned Vision-Language-Action Models via Instruction-Driven Routing & Sparsification", "score": 51.85, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/e67f52c1abdf244a6b56b29468a234e1c2b8d202.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Renshan Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.9, "max_score": 51.85, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "CogVLA: Cognition-Aligned Vision-Language-Action Models via Instruction-Driven Routing & Sparsification", "score": 51.85, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/e67f52c1abdf244a6b56b29468a234e1c2b8d202.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Xiangyan Liu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.8, "max_score": 51.75, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation", "score": 51.75, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/533c2b23b105c6b1687de467a26c6eb3d0dfc213.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jinjie Ni", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.8, "max_score": 51.75, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation", "score": 51.75, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/533c2b23b105c6b1687de467a26c6eb3d0dfc213.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Michael Qizhe Shieh", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.8, "max_score": 51.75, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation", "score": 51.75, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/533c2b23b105c6b1687de467a26c6eb3d0dfc213.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "lixiong Qin", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.4, "max_score": 51.44, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Face-Human-Bench: A Comprehensive Benchmark of Face and Human Understanding for Multi-modal Assistants", "score": 51.44, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/ea956a6c039f0b611dcf0d08e7fd7196fd22b665.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shilong Ou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.4, "max_score": 51.44, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Face-Human-Bench: A Comprehensive Benchmark of Face and Human Understanding for Multi-modal Assistants", "score": 51.44, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/ea956a6c039f0b611dcf0d08e7fd7196fd22b665.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Weiran Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.4, "max_score": 51.44, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Face-Human-Bench: A Comprehensive Benchmark of Face and Human Understanding for Multi-modal Assistants", "score": 51.44, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/ea956a6c039f0b611dcf0d08e7fd7196fd22b665.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Gunshi Gupta", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.4, "max_score": 51.44, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning", "score": 51.44, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/4877c8c9446b72d733a5677610304c04fd55f44d.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Karmesh Yadav", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.4, "max_score": 51.44, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning", "score": 51.44, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/4877c8c9446b72d733a5677610304c04fd55f44d.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Rahaf Aljundi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.4, "max_score": 51.44, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning", "score": 51.44, "session": "San Diego Poster Session 6", "pdf_url": "https://openreview.net/pdf/4877c8c9446b72d733a5677610304c04fd55f44d.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zizhao Chen", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.3, "max_score": 51.29, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Knot So Simple: A Minimalistic Environment for Spatial Reasoning", "score": 51.29, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2505.18028", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Yoav Artzi", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.3, "max_score": 51.29, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Knot So Simple: A Minimalistic Environment for Spatial Reasoning", "score": 51.29, "session": "San Diego Poster Session 5", "pdf_url": "https://arxiv.org/pdf/2505.18028", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Frank R√∂der", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51.0, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization", "score": 51.0, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/7c307a9a6f2b1e42e901aea2422257dc706fe28c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jan Benad", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51.0, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization", "score": 51.0, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/7c307a9a6f2b1e42e901aea2422257dc706fe28c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pradeep Kr. Banerjee", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 51.0, "max_score": 51.0, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization", "score": 51.0, "session": "San Diego Poster Session 3", "pdf_url": "https://openreview.net/pdf/7c307a9a6f2b1e42e901aea2422257dc706fe28c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kangrui Wang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.8, "max_score": 50.84, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "score": 50.84, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/ed11882b04656406635941c99bb0b5dbf475a088.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Pingyue Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.8, "max_score": 50.84, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "score": 50.84, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/ed11882b04656406635941c99bb0b5dbf475a088.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Manling Li", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.8, "max_score": 50.84, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents", "score": 50.84, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/ed11882b04656406635941c99bb0b5dbf475a088.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Fayi Le", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.5, "max_score": 50.54, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "DualCnst: Enhancing Zero-Shot Out-of-Distribution Detection via Text-Image Consistency in Vision-Language Models", "score": 50.54, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/76ec8720e26ef14ba02c4eee1e192dc5c75c2ed3.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenwu He", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.5, "max_score": 50.54, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "DualCnst: Enhancing Zero-Shot Out-of-Distribution Detection via Text-Image Consistency in Vision-Language Models", "score": 50.54, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/76ec8720e26ef14ba02c4eee1e192dc5c75c2ed3.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Zhuo-Xu Cui", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.5, "max_score": 50.54, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "DualCnst: Enhancing Zero-Shot Out-of-Distribution Detection via Text-Image Consistency in Vision-Language Models", "score": 50.54, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/76ec8720e26ef14ba02c4eee1e192dc5c75c2ed3.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tianle Zhang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.5, "max_score": 50.53, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning", "score": 50.53, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/01fbf16fb741c1fe74011db4bbab3fdfea94f97c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wanlong Fang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.5, "max_score": 50.53, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning", "score": 50.53, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/01fbf16fb741c1fe74011db4bbab3fdfea94f97c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Alvin Chan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.5, "max_score": 50.53, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning", "score": 50.53, "session": "San Diego Poster Session 1", "pdf_url": "https://openreview.net/pdf/01fbf16fb741c1fe74011db4bbab3fdfea94f97c.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kevin Hayes", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.5, "max_score": 50.55, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges", "score": 50.55, "session": "San Diego Poster Session 3", "pdf_url": "https://arxiv.org/pdf/2512.02161", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Micah Goldblum", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.5, "max_score": 50.55, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges", "score": 50.55, "session": "San Diego Poster Session 3", "pdf_url": "https://arxiv.org/pdf/2512.02161", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Tom Goldstein", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.5, "max_score": 50.55, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges", "score": 50.55, "session": "San Diego Poster Session 3", "pdf_url": "https://arxiv.org/pdf/2512.02161", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Wenbin An", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.5, "max_score": 50.5, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation", "score": 50.5, "session": "San Diego Oral 5", "pdf_url": "https://openreview.net/pdf/40d94836204a19bf22a4813c820925434476760b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiahao Nie", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.5, "max_score": 50.5, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation", "score": 50.5, "session": "San Diego Oral 5", "pdf_url": "https://openreview.net/pdf/40d94836204a19bf22a4813c820925434476760b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shijian Lu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.5, "max_score": 50.5, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation", "score": 50.5, "session": "San Diego Oral 5", "pdf_url": "https://openreview.net/pdf/40d94836204a19bf22a4813c820925434476760b.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Kunlun Xu", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.4, "max_score": 50.37, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "C$^2$Prompt: Class-aware Client Knowledge Interaction for Federated Continual Learning", "score": 50.37, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/f8b45261958643144e6e7e3f1a61d4363528d66e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "yibo feng", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.4, "max_score": 50.37, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "C$^2$Prompt: Class-aware Client Knowledge Interaction for Federated Continual Learning", "score": 50.37, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/f8b45261958643144e6e7e3f1a61d4363528d66e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiahuan Zhou", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.4, "max_score": 50.37, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "C$^2$Prompt: Class-aware Client Knowledge Interaction for Federated Continual Learning", "score": 50.37, "session": "San Diego Poster Session 5", "pdf_url": "https://openreview.net/pdf/f8b45261958643144e6e7e3f1a61d4363528d66e.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Sunqi Fan", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.1, "max_score": 50.06, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task", "score": 50.06, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/c3fe7507152ba55f4cdc2d606716d226689509d6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Jiashuo Cui", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.1, "max_score": 50.06, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task", "score": 50.06, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/c3fe7507152ba55f4cdc2d606716d226689509d6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}, {"name": "Shuojin Yang", "paper_count": 1, "highly_relevant_count": 0, "avg_score": 50.1, "max_score": 50.06, "total_relevant": 0, "total_reads": 0, "papers": [{"title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task", "score": 50.06, "session": "San Diego Poster Session 2", "pdf_url": "https://openreview.net/pdf/c3fe7507152ba55f4cdc2d606716d226689509d6.pdf", "relevant": 0, "reads": 0}], "affiliation": "Unknown", "role": "Unknown", "photo_url": null, "profile_url": null}];

        // Available categories
        const allCategories = ["Vision-Language Reasoning and Chain-of-Thought", "Reinforcement Learning for Multimodal Models", "Visual Grounding and Spatial Reasoning", "Prompt Learning and Optimization", "Multimodal Benchmarks and Evaluation", "Embodied AI and Vision-Language-Action Models", "Hallucination Detection and Model Robustness", "In-Context Learning and Few-Shot Adaptation", "Video Understanding and Temporal Reasoning", "Compositional and Counterfactual Reasoning"];

        // Pagination state
        let currentAuthorsPage = 1;
        const authorsPerPage = 10;
        let currentPapersPage = 1;
        const papersPerPage = 20;

        // Search state
        let searchQuery = '';

        function displayStats() {
            const scores = papers.map(p => parseInt(p.score));
            const totalPapers = papers.length;
            const avgScore = (scores.reduce((a, b) => a + b, 0) / totalPapers).toFixed(1);
            const topScore = Math.max(...scores);

            document.getElementById('totalPapers').textContent = totalPapers;
            document.getElementById('avgScore').textContent = avgScore;
            document.getElementById('topScore').textContent = topScore;
        }

        function displayChart() {
            const scores = papers.map(p => parseInt(p.score));

            // Create histogram bins focused on the 50-100 range (5-point resolution)
            const bins = {};
            const binSize = 5;
            const minBound = 50;
            const maxBound = 100;
            for (let i = minBound; i <= maxBound; i += binSize) {
                bins[i] = 0;
            }

            scores.forEach(score => {
                const clamped = Math.max(minBound, Math.min(maxBound, score));
                const bin = Math.floor((clamped - minBound) / binSize) * binSize + minBound;
                bins[bin] = (bins[bin] || 0) + 1;
            });

            const ctx = document.getElementById('scoreChart').getContext('2d');
            new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: Object.keys(bins).map(b => `${b}-${Math.min(parseInt(b) + binSize - 1, maxBound)}`),
                    datasets: [{
                        label: 'Number of Papers',
                        data: Object.values(bins),
                        backgroundColor: 'rgba(102, 126, 234, 0.8)',
                        borderColor: 'rgba(102, 126, 234, 1)',
                        borderWidth: 2,
                        borderRadius: 8
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: {
                        legend: {
                            display: false
                        }
                    },
                    scales: {
                        y: {
                            beginAtZero: true,
                            ticks: {
                                precision: 0
                            }
                        }
                    }
                }
            });
        }

        let selectedCategories = new Set();

        function renderCategoryFilters() {
            const filtersDiv = document.getElementById('categoryFilters');
            if (!allCategories || allCategories.length === 0) {
                filtersDiv.style.display = 'none';
                return;
            }

            let html = '<div style="margin-bottom: 10px;"><strong>Filter by Category:</strong></div><div>';
            allCategories.forEach(category => {
                html += `<div class="category-pill" onclick="toggleCategory('${category.replace(/'/g, "\'")}')">${category}</div>`;
            });
            html += '</div>';
            filtersDiv.innerHTML = html;
        }

        function toggleCategory(category) {
            if (selectedCategories.has(category)) {
                selectedCategories.delete(category);
            } else {
                selectedCategories.add(category);
            }

            // Update UI
            document.querySelectorAll('.category-pill').forEach(pill => {
                if (pill.textContent === category) {
                    pill.classList.toggle('selected');
                }
            });

            // Re-display papers with filter
            displayPapers(document.getElementById('sortBy').value, 1);
        }

        function togglePaperCard(index) {
            const card = document.querySelectorAll('.paper-card')[index];
            card.classList.toggle('expanded');
        }

        function openPaperModal(paperIndex, currentPapers) {
            const paper = currentPapers[paperIndex];
            const modal = document.getElementById('paperModal');
            const modalContent = document.getElementById('modalContent');

            let html = `
                <h2 style="margin-top: 0; color: #667eea;">${paper.title || 'Untitled'}</h2>
                ${paper.ai_categories && paper.ai_categories.length > 0 ? `
                    <div style="margin-bottom: 20px;">
                        ${paper.ai_categories.map(cat => `<span class="paper-category-badge">${cat}</span>`).join('')}
                    </div>
                ` : ''}
                ${paper.authors ? `<p><strong>Authors:</strong> ${paper.authors}</p>` : ''}
                <p><strong>Relevance Score:</strong> <span style="font-size: 1.2em; color: #667eea; font-weight: bold;">${paper.score}</span></p>
                ${paper.session_type ? `<p><strong>Session:</strong> ${paper.session_type}</p>` : ''}
                ${paper.session_location ? `<p><strong>Location:</strong> ${paper.session_location}</p>` : ''}

                ${paper.description ? `
                    <div style="margin-top: 25px;">
                        <h3 style="color: #667eea; margin-bottom: 10px;">üìñ What is this paper about?</h3>
                        <p style="line-height: 1.6;">${paper.description}</p>
                    </div>
                ` : ''}

                ${paper.novelty ? `
                    <div style="margin-top: 25px; padding: 20px; background: linear-gradient(135deg, #fff5e6 0%, #ffe6f0 100%); border-radius: 10px; border-left: 4px solid #f59e0b;">
                        <h3 style="color: #d97706; margin-top: 0; margin-bottom: 10px;">üí° What Makes This Novel?</h3>
                        <p style="line-height: 1.6; margin-bottom: 0;">${paper.novelty}</p>
                    </div>
                ` : ''}

                ${paper.key_contribution ? `
                    <div style="margin-top: 25px;">
                        <h3 style="color: #667eea; margin-bottom: 10px;">üéØ Key Contribution</h3>
                        <p style="line-height: 1.6;">${paper.key_contribution}</p>
                    </div>
                ` : ''}

                ${paper.key_findings ? `
                    <div style="margin-top: 25px;">
                        <h3 style="color: #667eea; margin-bottom: 10px;">üîç Key Findings</h3>
                        <p style="line-height: 1.6;">${paper.key_findings}</p>
                    </div>
                ` : ''}

                ${paper.pdf_url ? `
                    <div style="margin-top: 25px;">
                        <a href="${paper.pdf_url}" target="_blank" style="display: inline-block; padding: 12px 24px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; text-decoration: none; border-radius: 8px; font-weight: 600;">
                            üìÑ View Full PDF
                        </a>
                    </div>
                ` : ''}
            `;

            modalContent.innerHTML = html;
            modal.classList.add('active');
        }

        function closePaperModal() {
            document.getElementById('paperModal').classList.remove('active');
        }

        function displayPapers(sortBy = 'score', page = 1) {
            currentPapersPage = page;
            let sortedPapers = [...papers];

            // Filter by search query
            if (searchQuery.trim()) {
                const query = searchQuery.toLowerCase().trim();
                const queryTerms = query.split(/\s+/).filter(t => t.length > 0);

                sortedPapers = sortedPapers.filter(paper => {
                    const title = (paper.title || '').toLowerCase();
                    const authors = (paper.authors || '').toLowerCase();
                    const description = (paper.description || '').toLowerCase();
                    const keyFindings = (paper.key_findings || '').toLowerCase();
                    const novelty = (paper.novelty || '').toLowerCase();
                    const categories = (paper.ai_categories || []).join(' ').toLowerCase();
                    const session = (paper.session_name || paper.session_type || '').toLowerCase();

                    const searchableText = `${title} ${authors} ${description} ${keyFindings} ${novelty} ${categories} ${session}`;

                    // All query terms must match somewhere
                    return queryTerms.every(term => searchableText.includes(term));
                });
            }

            // Filter by selected categories
            if (selectedCategories.size > 0) {
                sortedPapers = sortedPapers.filter(paper => {
                    if (!paper.ai_categories || paper.ai_categories.length === 0) {
                        return false;
                    }
                    return paper.ai_categories.some(cat => selectedCategories.has(cat));
                });
            }

            // Update search results info
            const searchResultsInfo = document.getElementById('searchResultsInfo');
            if (searchQuery.trim()) {
                searchResultsInfo.style.display = 'block';
                searchResultsInfo.innerHTML = `Found <strong>${sortedPapers.length}</strong> paper${sortedPapers.length !== 1 ? 's' : ''} matching "<em>${searchQuery}</em>"`;
            } else {
                searchResultsInfo.style.display = 'none';
            }

            switch(sortBy) {
                case 'score':
                    sortedPapers.sort((a, b) => parseInt(b.score) - parseInt(a.score));
                    break;
                case 'title':
                    sortedPapers.sort((a, b) => a.title.localeCompare(b.title));
                    break;
            }

            const totalPapers = sortedPapers.length;
            const totalPages = Math.ceil(totalPapers / papersPerPage);
            const startIdx = (page - 1) * papersPerPage;
            const endIdx = startIdx + papersPerPage;
            const pagePapers = sortedPapers.slice(startIdx, endIdx);

            const papersList = document.getElementById('papersList');
            papersList.innerHTML = pagePapers.map((paper, idx) => `
                <div class="paper-card" onclick="togglePaperCard(${idx})">
                    <div class="paper-header">
                        <div class="paper-title">${paper.title || 'Untitled'}</div>
                        <div class="paper-score">${paper.score}</div>
                    </div>
                    ${paper.ai_categories && paper.ai_categories.length > 0 ? `
                        <div style="margin: 10px 0;">
                            ${paper.ai_categories.map(cat => `<span class="paper-category-badge">${cat}</span>`).join('')}
                        </div>
                    ` : ''}
                    ${paper.authors ? `<div class="paper-authors">üë• ${paper.authors}</div>` : ''}
                    <div class="paper-details">
                        ${paper.session_type ? `<div class="detail-item"><strong>Session:</strong> ${paper.session_type}</div>` : ''}
                        ${paper.session_location ? `<div class="detail-item"><strong>Location:</strong> ${paper.session_location}</div>` : ''}
                    </div>
                    <div class="paper-expandable">
                        ${paper.novelty ? `
                            <div class="paper-key-info" style="background: linear-gradient(135deg, #fff5e6 0%, #ffe6f0 100%); border-left: 3px solid #f59e0b;">
                                <strong style="color: #d97706;">üí° What's Novel:</strong>
                                <p style="margin: 8px 0; line-height: 1.5;">${paper.novelty.substring(0, 200)}${paper.novelty.length > 200 ? '...' : ''}</p>
                            </div>
                        ` : ''}
                        ${paper.key_contribution ? `
                            <div class="paper-key-info">
                                <strong style="color: #667eea;">üéØ Key Contribution:</strong>
                                <p style="margin: 8px 0; line-height: 1.5;">${paper.key_contribution.substring(0, 150)}${paper.key_contribution.length > 150 ? '...' : ''}</p>
                            </div>
                        ` : ''}
                        ${paper.key_findings ? `
                            <div class="paper-key-info">
                                <strong style="color: #667eea;">üîç Key Findings:</strong>
                                <p style="margin: 8px 0; line-height: 1.5;">${paper.key_findings.substring(0, 150)}${paper.key_findings.length > 150 ? '...' : ''}</p>
                            </div>
                        ` : ''}
                        <button onclick="event.stopPropagation(); openPaperModal(${idx}, ${JSON.stringify(pagePapers).replace(/"/g, '&quot;')})"
                                style="margin-top: 15px; padding: 8px 16px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; border: none; border-radius: 6px; cursor: pointer; font-weight: 600;">
                            View Full Details
                        </button>
                    </div>

                    ${paper.pdf_url ? `<div class="paper-link"><a href="${paper.pdf_url}" target="_blank" onclick="event.stopPropagation()">üìÑ View PDF ‚Üí</a></div>` : ''}
                </div>
            `).join('');

            // Render pagination controls
            const paginationDiv = document.getElementById('papersPagination');
            if (totalPages > 1) {
                let paginationHTML = '';

                // Previous button
                paginationHTML += `<button class="pagination-btn" onclick="displayPapers('${sortBy}', ${page - 1})" ${page === 1 ? 'disabled' : ''}>‚Üê Previous</button>`;

                // Page info
                paginationHTML += `<span class="pagination-info">Page ${page} of ${totalPages} (${totalPapers} papers)</span>`;

                // Next button
                paginationHTML += `<button class="pagination-btn" onclick="displayPapers('${sortBy}', ${page + 1})" ${page === totalPages ? 'disabled' : ''}>Next ‚Üí</button>`;

                paginationDiv.innerHTML = paginationHTML;
            } else {
                paginationDiv.innerHTML = '';
            }

            // Scroll to top of papers list
            if (page > 1) {
                document.getElementById('papersList').scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }

        // Search box helper functions
        function clearSearchBox() {
            const searchInput = document.getElementById('paperSearch');
            searchInput.value = '';
            searchQuery = '';
            document.getElementById('clearSearch').style.display = 'none';
            displayPapers(document.getElementById('sortBy').value, 1);
        }

        let searchDebounceTimer = null;
        function handleSearchInput(e) {
            const clearBtn = document.getElementById('clearSearch');
            clearBtn.style.display = e.target.value ? 'block' : 'none';

            // Debounce search to avoid too many re-renders
            clearTimeout(searchDebounceTimer);
            searchDebounceTimer = setTimeout(() => {
                searchQuery = e.target.value;
                displayPapers(document.getElementById('sortBy').value, 1);
            }, 200);
        }

        // Initialize on load
        document.addEventListener('DOMContentLoaded', () => {
            displayStats();
            displayChart();
            renderCategoryFilters();
            displayPapers();

            document.getElementById('sortBy').addEventListener('change', (e) => {
                displayPapers(e.target.value);
            });

            // Initialize search box
            const searchInput = document.getElementById('paperSearch');
            searchInput.addEventListener('input', handleSearchInput);
            searchInput.addEventListener('keydown', (e) => {
                if (e.key === 'Escape') {
                    clearSearchBox();
                    searchInput.blur();
                }
            });

            // Initialize authors display
            displayAffiliationChart();
            displayAuthors();

            // Close modal when clicking outside
            document.getElementById('paperModal').addEventListener('click', (e) => {
                if (e.target.id === 'paperModal') {
                    closePaperModal();
                }
            });
        });

        function switchTab(tabName) {
            // Hide all tabs
            document.getElementById('papersTab').classList.remove('active');
            document.getElementById('authorsTab').classList.remove('active');
            document.getElementById('synthesisTab').classList.remove('active');

            // Remove active from all tab buttons
            document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));

            // Show selected tab
            if (tabName === 'papers') {
                document.getElementById('papersTab').classList.add('active');
                document.querySelectorAll('.tab')[0].classList.add('active');
            } else if (tabName === 'authors') {
                document.getElementById('authorsTab').classList.add('active');
                document.querySelectorAll('.tab')[1].classList.add('active');
            } else if (tabName === 'synthesis') {
                document.getElementById('synthesisTab').classList.add('active');
                document.querySelectorAll('.tab')[2].classList.add('active');
            }
        }

        function displayAffiliationChart() {
            // Get authors with highly relevant papers and known affiliations
            const qualifyingAuthors = authors.filter(a =>
                a.highly_relevant_count >= 1 &&
                a.affiliation &&
                a.affiliation !== 'Unknown'
            );

            // Count affiliations
            const affiliationCounts = {};
            qualifyingAuthors.forEach(author => {
                const affiliation = author.affiliation;
                affiliationCounts[affiliation] = (affiliationCounts[affiliation] || 0) + 1;
            });

            // Sort by count and take top 15
            const sortedAffiliations = Object.entries(affiliationCounts)
                .sort((a, b) => b[1] - a[1])
                .slice(0, 15);

            const labels = sortedAffiliations.map(a => a[0]);
            const data = sortedAffiliations.map(a => a[1]);

            const ctx = document.getElementById('affiliationChart').getContext('2d');
            new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: labels,
                    datasets: [{
                        label: 'Number of Researchers',
                        data: data,
                        backgroundColor: 'rgba(102, 126, 234, 0.8)',
                        borderColor: 'rgba(102, 126, 234, 1)',
                        borderWidth: 1
                    }]
                },
                options: {
                    indexAxis: 'y',
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: {
                        legend: {
                            display: false
                        },
                        title: {
                            display: true,
                            text: `Top Institutions (${qualifyingAuthors.length} of ${authors.filter(a => a.highly_relevant_count >= 1).length} authors have known affiliations)`,
                            font: {
                                size: 13
                            },
                            color: '#666'
                        }
                    },
                    scales: {
                        x: {
                            beginAtZero: true,
                            ticks: {
                                stepSize: 1
                            },
                            title: {
                                display: true,
                                text: 'Number of Researchers'
                            }
                        }
                    }
                }
            });
        }

        function displayAuthors(page = 1) {
            currentAuthorsPage = page;

            // Filter to authors with at least 1 highly relevant paper (score >= 85)
            let sortedAuthors = authors
                .filter(a => a.highly_relevant_count >= 1)
                .map(a => ({
                    ...a,
                    papers: (a.papers || []).slice().sort((p1, p2) => {
                        const s1 = parseFloat(p1.score || 0);
                        const s2 = parseFloat(p2.score || 0);
                        if (s1 !== s2) return s2 - s1;
                        return (p1.title || '').localeCompare(p2.title || '');
                    })
                }))
                .sort((a, b) => {
                    if (b.highly_relevant_count !== a.highly_relevant_count) {
                        return b.highly_relevant_count - a.highly_relevant_count;
                    }
                    const avgA = parseFloat(a.avg_score || 0);
                    const avgB = parseFloat(b.avg_score || 0);
                    if (avgB !== avgA) {
                        return avgB - avgA;
                    }
                    return (a.name || '').localeCompare(b.name || '');
                });

            const totalAuthors = sortedAuthors.length;
            const totalPages = Math.ceil(totalAuthors / authorsPerPage);
            const startIdx = (page - 1) * authorsPerPage;
            const endIdx = startIdx + authorsPerPage;
            const pageAuthors = sortedAuthors.slice(startIdx, endIdx);

            const authorsList = document.getElementById('authorsList');
            authorsList.innerHTML = pageAuthors.map(author => `
                <div class="author-card">
                    <div class="author-header">
                        ${author.photo_url ? `
                            <img src="${author.photo_url}" alt="${author.name}" class="author-photo" onerror="this.style.display='none'; this.nextElementSibling.style.display='flex';">
                            <div class="author-photo-placeholder" style="display: none;">${author.name.charAt(0)}</div>
                        ` : `
                            <div class="author-photo-placeholder">${author.name.charAt(0)}</div>
                        `}
                        <div class="author-info">
                            <div class="author-name">
                                ${author.name}
                                ${author.profile_url ? `<a href="${author.profile_url}" target="_blank" class="author-profile-link" onclick="event.stopPropagation()">üîó Profile</a>` : ''}
                            </div>
                            ${author.affiliation && author.affiliation !== 'Unknown' ? `
                                <div class="author-affiliation">
                                    <span class="affiliation-badge" title="Current affiliation">${author.affiliation}</span>
                                    ${author.role && author.role !== 'Unknown' ? `<span class="role-badge" title="Academic/professional role">${author.role}</span>` : ''}
                                </div>
                            ` : ''}
                            <div class="author-stats-badges">
                                <div class="author-badge" title="Total number of papers by this author that align with your interests">
                                    üìÑ <strong>${author.paper_count}</strong> total
                                </div>
                                <div class="author-badge" title="Average relevance score across all their papers (higher = better alignment with your interests)">
                                    üìä <strong>${author.avg_score}</strong> avg
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="author-papers-list">
                        ${author.papers.map(paper => `
                            <div class="author-paper-item">
                                <div class="author-paper-score" title="Relevance score: how well this paper aligns with your research interests (higher = stronger alignment)">${paper.score}</div>
                                <div class="author-paper-title">${paper.title}</div>
                            </div>
                        `).join('')}
                    </div>
                </div>
            `).join('');

            // Render pagination controls
            const paginationDiv = document.getElementById('authorsPagination');
            if (totalPages > 1) {
                let paginationHTML = '';

                // Previous button
                paginationHTML += `<button class="pagination-btn" onclick="displayAuthors(${page - 1})" ${page === 1 ? 'disabled' : ''}>‚Üê Previous</button>`;

                // Page info
                paginationHTML += `<span class="pagination-info">Page ${page} of ${totalPages} (${totalAuthors} authors)</span>`;

                // Next button
                paginationHTML += `<button class="pagination-btn" onclick="displayAuthors(${page + 1})" ${page === totalPages ? 'disabled' : ''}>Next ‚Üí</button>`;

                paginationDiv.innerHTML = paginationHTML;
            } else {
                paginationDiv.innerHTML = '';
            }

            // Scroll to top of authors list
            if (page > 1) {
                document.getElementById('authorsList').scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        }
    </script>
</body>
</html>
